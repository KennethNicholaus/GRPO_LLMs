{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-xdnzlu7v/unsloth_2ebace502881401d96bb6149a6bbccd9\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-xdnzlu7v/unsloth_2ebace502881401d96bb6149a6bbccd9\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 5f14e6fcd405db038ec099c5fe8ba4a753c8472e\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: unsloth_zoo>=2025.8.9 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.8.9)\n",
      "Requirement already satisfied: packaging in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.0)\n",
      "Requirement already satisfied: tyro in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.9.31)\n",
      "Requirement already satisfied: transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.55.4)\n",
      "Requirement already satisfied: datasets<4.0.0,>=3.4.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.6.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.1)\n",
      "Requirement already satisfied: tqdm in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.67.1)\n",
      "Requirement already satisfied: psutil in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.45.1)\n",
      "Requirement already satisfied: numpy in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.6)\n",
      "Requirement already satisfied: protobuf in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.32.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.34.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.34.4)\n",
      "Requirement already satisfied: hf_transfer in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.9)\n",
      "Requirement already satisfied: bitsandbytes>=0.45.5 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.47.0)\n",
      "Requirement already satisfied: filelock in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.5)\n",
      "Requirement already satisfied: xxhash in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.15.0)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.7.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from triton==3.3.1->torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (78.1.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from huggingface_hub>=0.34.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.1.9)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.9.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6.2)\n",
      "Requirement already satisfied: torchao in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth_zoo>=2025.8.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.13.0)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth_zoo>=2025.8.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.10.1)\n",
      "Requirement already satisfied: trl!=0.15.0,!=0.19.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth_zoo>=2025.8.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.22.2)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth_zoo>=2025.8.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.17.1)\n",
      "Requirement already satisfied: cut_cross_entropy in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth_zoo>=2025.8.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.1.1)\n",
      "Requirement already satisfied: pillow in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth_zoo>=2025.8.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.3.0)\n",
      "Requirement already satisfied: msgspec in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth_zoo>=2025.8.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.19.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jinja2->torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.17.0)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.17.0)\n",
      "Requirement already satisfied: rich>=11.1.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (14.1.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.2)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.4.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
      "Requirement already satisfied: vllm in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (0.10.1.1)\n",
      "Requirement already satisfied: regex in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (2025.9.1)\n",
      "Requirement already satisfied: cachetools in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (6.2.0)\n",
      "Requirement already satisfied: psutil in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (5.9.0)\n",
      "Requirement already satisfied: sentencepiece in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.2.1)\n",
      "Requirement already satisfied: numpy in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (2.2.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (2.32.5)\n",
      "Requirement already satisfied: tqdm in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (4.67.1)\n",
      "Requirement already satisfied: blake3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (1.0.5)\n",
      "Requirement already satisfied: py-cpuinfo in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (9.0.0)\n",
      "Requirement already satisfied: transformers>=4.55.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (4.55.4)\n",
      "Requirement already satisfied: tokenizers>=0.21.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.21.4)\n",
      "Requirement already satisfied: protobuf in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (6.32.0)\n",
      "Requirement already satisfied: fastapi>=0.115.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.116.1)\n",
      "Requirement already satisfied: aiohttp in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (3.12.15)\n",
      "Requirement already satisfied: openai>=1.99.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (1.105.0)\n",
      "Requirement already satisfied: pydantic>=2.10 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (2.11.7)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.21.1)\n",
      "Requirement already satisfied: pillow in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (11.3.0)\n",
      "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (7.1.0)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.11.0)\n",
      "Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.11 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.10.12)\n",
      "Requirement already satisfied: llguidance<0.8.0,>=0.7.11 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.7.30)\n",
      "Requirement already satisfied: outlines_core==0.2.10 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.2.10)\n",
      "Requirement already satisfied: diskcache==5.6.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (5.6.3)\n",
      "Requirement already satisfied: lark==1.2.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (1.2.2)\n",
      "Requirement already satisfied: xgrammar==0.1.21 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.1.21)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (4.15.0)\n",
      "Requirement already satisfied: filelock>=3.16.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (3.19.1)\n",
      "Requirement already satisfied: partial-json-parser in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.2.1.1.post6)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (26.2.0)\n",
      "Requirement already satisfied: msgspec in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.19.0)\n",
      "Requirement already satisfied: gguf>=0.13.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.17.1)\n",
      "Requirement already satisfied: mistral_common>=1.8.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from mistral_common[audio,image]>=1.8.2->vllm) (1.8.4)\n",
      "Requirement already satisfied: opencv-python-headless>=4.11.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (4.12.0.88)\n",
      "Requirement already satisfied: pyyaml in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (6.0.2)\n",
      "Requirement already satisfied: einops in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.8.1)\n",
      "Requirement already satisfied: compressed-tensors==0.10.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.10.2)\n",
      "Requirement already satisfied: depyf==0.19.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.19.0)\n",
      "Requirement already satisfied: cloudpickle in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (3.1.1)\n",
      "Requirement already satisfied: watchfiles in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (1.1.0)\n",
      "Requirement already satisfied: python-json-logger in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (3.2.1)\n",
      "Requirement already satisfied: scipy in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (1.16.1)\n",
      "Requirement already satisfied: ninja in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (1.13.0)\n",
      "Requirement already satisfied: pybase64 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (1.4.2)\n",
      "Requirement already satisfied: cbor2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (5.7.0)\n",
      "Requirement already satisfied: setproctitle in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (1.3.6)\n",
      "Requirement already satisfied: openai-harmony>=0.0.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.0.4)\n",
      "Requirement already satisfied: numba==0.61.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.61.2)\n",
      "Requirement already satisfied: ray>=2.48.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from ray[cgraph]>=2.48.0->vllm) (2.49.1)\n",
      "Requirement already satisfied: torch==2.7.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (2.7.1)\n",
      "Requirement already satisfied: torchaudio==2.7.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (2.7.1)\n",
      "Requirement already satisfied: torchvision==0.22.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.22.1)\n",
      "Requirement already satisfied: xformers==0.0.31 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.0.31)\n",
      "Requirement already satisfied: astor in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from depyf==0.19.0->vllm) (0.8.1)\n",
      "Requirement already satisfied: dill in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from depyf==0.19.0->vllm) (0.3.8)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from numba==0.61.2->vllm) (0.44.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from triton==3.3.1->torch==2.7.1->vllm) (78.1.1)\n",
      "Requirement already satisfied: interegular>=0.3.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from lm-format-enforcer<0.11,>=0.10.11->vllm) (0.3.3)\n",
      "Requirement already satisfied: packaging in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from lm-format-enforcer<0.11,>=0.10.11->vllm) (25.0)\n",
      "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.47.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pydantic>=2.10->vllm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pydantic>=2.10->vllm) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pydantic>=2.10->vllm) (0.4.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from starlette<0.48.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (4.7.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (1.3.0)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.8 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.0.10)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi[standard]>=0.115.0->vllm) (2.3.0)\n",
      "Requirement already satisfied: uvicorn>=0.12.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.35.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.7.0)\n",
      "Requirement already satisfied: typer>=0.15.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.17.3)\n",
      "Requirement already satisfied: rich-toolkit>=0.14.8 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.15.0)\n",
      "Requirement already satisfied: fastapi-cloud-cli>=0.1.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.5)\n",
      "Requirement already satisfied: rignore>=0.5.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.6.4)\n",
      "Requirement already satisfied: sentry-sdk>=2.20.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.35.2)\n",
      "Requirement already satisfied: certifi in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jinja2->torch==2.7.1->vllm) (3.0.2)\n",
      "Requirement already satisfied: jsonschema>=4.21.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (4.25.0)\n",
      "Requirement already satisfied: pydantic-extra-types>=2.10.5 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2.10.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.22.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from openai>=1.99.1->vllm) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from openai>=1.99.1->vllm) (0.10.0)\n",
      "Requirement already satisfied: pycountry>=23 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (24.6.1)\n",
      "Requirement already satisfied: click>=7.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (8.2.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (1.1.1)\n",
      "Requirement already satisfied: cupy-cuda12x in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from ray[cgraph]>=2.48.0->vllm) (13.6.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from requests>=2.26.0->vllm) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from requests>=2.26.0->vllm) (2.5.0)\n",
      "Requirement already satisfied: rich>=13.7.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.1.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from sympy>=1.13.3->torch==2.7.1->vllm) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from tokenizers>=0.21.1->vllm) (0.34.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.21.1->vllm) (1.1.9)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from transformers>=4.55.0->vllm) (0.6.2)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.1.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp->vllm) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp->vllm) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp->vllm) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp->vllm) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp->vllm) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp->vllm) (1.20.1)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm) (0.8.3)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.13.1)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2.21)\n",
      "Requirement already satisfied: soxr>=0.5.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.5.0.post1)\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (0.22.1)\n",
      "Requirement already satisfied: torchaudio in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from triton==3.3.1->torch) (78.1.1)\n",
      "Requirement already satisfied: numpy in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Environment Detection & Package Installation\n",
    "\n",
    "Detects if code is running in Google Colab by checking environment variables\n",
    "Installs unsloth (efficient LLM fine-tuning library) and vllm (fast inference engine) if not in Colab\n",
    "Uses %%capture to suppress installation output\n",
    "Colab environments have different installation requirements handled separately\n",
    "'''\n",
    "#%%capture\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Check if we're in a special environment (Colab/Kaggle)\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()) and \"KAGGLE_\" not in \"\".join(os.environ.keys()):\n",
    "    # For local WSL2 environment - install with specific CUDA support\n",
    "    try:\n",
    "        # Install unsloth with CUDA support for RTX 2070 Super\n",
    "        !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "        \n",
    "        # Install vllm with CUDA 11.8/12.1 support (common for RTX 2070 Super)\n",
    "        !pip install vllm\n",
    "        \n",
    "        # Install additional dependencies for WSL2\n",
    "        !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Installation warning: {e}\")\n",
    "        print(\"Falling back to basic installation...\")\n",
    "        !pip install unsloth vllm\n",
    "else:\n",
    "    pass  # Special environment handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected RTX 2070 Super - installing optimized versions...\n",
      "  \u001b[31m\u001b[0m No solution found when resolving dependencies:\n",
      "\u001b[31m   \u001b[0mBecause torch==2.7.1 depends on triton{platform_machine == 'x86_64'\n",
      "\u001b[31m      \u001b[0mand sys_platform == 'linux'}==3.3.1 and vllm==0.10.1 depends\n",
      "\u001b[31m      \u001b[0mon torch==2.7.1, we can conclude that vllm==0.10.1 depends on\n",
      "\u001b[31m      \u001b[0mtriton==3.3.1.\n",
      "\u001b[31m      \u001b[0mAnd because you require vllm==0.10.1 and triton==3.2.0, we can conclude\n",
      "\u001b[31m      \u001b[0mthat your requirements are unsatisfiable.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Colab-Specific Installation with Hardware Detection\n",
    "\n",
    "Upgrades uv package manager for faster installations\n",
    "Detects GPU type (Tesla T4) to install compatible vllm/triton versions\n",
    "Handles numpy version compatibility by preserving existing version\n",
    "Installs core ML libraries: unsloth, vllm, transformers, bitsandbytes, xformers\n",
    "Forces specific transformers version (4.55.4) for compatibility\n",
    "'''\n",
    "# WSL2 Installation with RTX 2070 Super optimization\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Upgrade package manager\n",
    "!pip install --upgrade -q uv\n",
    "\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    try:\n",
    "        # Get current numpy version if available\n",
    "        try: \n",
    "            import numpy\n",
    "            get_numpy = f\"numpy=={numpy.__version__}\"\n",
    "        except: \n",
    "            get_numpy = \"numpy\"\n",
    "        \n",
    "        # Check CUDA capability for RTX 2070 Super (compute capability 7.5)\n",
    "        try:\n",
    "            gpu_info = str(subprocess.check_output([\"nvidia-smi\"], stderr=subprocess.DEVNULL))\n",
    "            is_rtx_2070 = \"RTX 2070\" in gpu_info or \"GeForce RTX\" in gpu_info\n",
    "        except:\n",
    "            is_rtx_2070 = False\n",
    "        \n",
    "        # Install with RTX 2070 Super compatible versions\n",
    "        if is_rtx_2070:\n",
    "            print(\"Detected RTX 2070 Super - installing optimized versions...\")\n",
    "            !uv pip install -q --upgrade \\\n",
    "                unsloth vllm==0.10.1 {get_numpy} torchvision \\\n",
    "                bitsandbytes xformers triton==3.2.0\n",
    "        else:\n",
    "            # Fallback installation\n",
    "            !uv pip install -q --upgrade \\\n",
    "                unsloth vllm {get_numpy} torchvision \\\n",
    "                bitsandbytes xformers triton\n",
    "                \n",
    "        # Install specific transformers version for compatibility\n",
    "        !uv pip install -q transformers==4.55.4\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Installation error: {e}\")\n",
    "        print(\"Falling back to basic pip installation...\")\n",
    "        !pip install unsloth vllm transformers==4.55.4\n",
    "else:\n",
    "    pass  # Colab environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIy3QkjW1O4R"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jN75nmdx9lvw"
   },
   "source": [
    "Goal: To convert `DeepSeek-R1-0528-Qwen3-8B` into a reasoning model via GRPO by using OpenR1's Math dataset.\n",
    "\n",
    "We also use `langid` for language detection. Our main goal is to force the model to generate reasoning traces in Indonesian, and we create a reward function using `langid` to check this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install language identification library\n",
    "'''\n",
    "Section: Language Identification Library Installation\n",
    "\n",
    "Installs langid package for automatic language detection\n",
    "Uses -qq flag for quiet installation (minimal output)\n",
    "Useful for identifying language of text data before processing\n",
    "'''\n",
    "!pip install langid -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 09-03 14:26:18 [__init__.py:241] Automatically detected platform cuda.\n",
      "ERROR 09-03 14:26:19 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      " Unsloth Zoo will now patch everything to make training faster!\n",
      "Loading model optimized for RTX 2070 Super...\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.8.10: Fast Qwen2 patching. Transformers: 4.55.4. vLLM: 0.10.1.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 2070 SUPER. Num GPUs = 1. Max memory: 8.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.8.10 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Model device: cuda:0\n",
      "VRAM allocated: 1.51GB\n",
      "VRAM reserved: 1.53GB\n",
      "VRAM free: 6.47GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nOther recommended models for RTX 2070 Super:\\n unsloth/Llama-3.2-3B-Instruct - 3B parameters, good balance\\n unsloth/Qwen2.5-3B-Instruct - 3B parameters, efficient\\n unsloth/Phi-3.5-mini-instruct - 3.8B parameters, optimized\\n microsoft/DialoGPT-medium - 355M parameters, very fast\\n\\nFor 7B+ models, you'll need CPU offloading or model parallelism\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Section: Model Loading and LoRA Configuration\n",
    "\n",
    "Imports FastLanguageModel from unsloth for efficient fine-tuning\n",
    "Sets sequence length to 128 tokens and LoRA rank to 8 for memory efficiency\n",
    "Loads DeepSeek-R1 8B model with 4-bit quantization to reduce VRAM usage\n",
    "Configures PEFT (Parameter Efficient Fine-Tuning) with LoRA adapters\n",
    "Targets attention and MLP layers for fine-tuning\n",
    "Uses gradient checkpointing to save memory during training\n",
    "'''\n",
    "'''\n",
    "Section: Model Loading Fix - Compatibility Issue Resolution\n",
    "\n",
    "Error occurs because fast_inference and trust_remote_code cannot be used together\n",
    "Need to choose between fast inference (vLLM) or remote code execution\n",
    "For fine-tuning, disable fast_inference; enable it later for inference only\n",
    "'''\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear any existing GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# SOLUTION 1: Use smaller model (RECOMMENDED)\n",
    "# Conservative settings optimized for RTX 2070 Super (8GB VRAM)\n",
    "max_seq_length = 512      # Increased since using smaller model\n",
    "lora_rank = 16            # Can increase with smaller model\n",
    "dtype = None              # Auto-detect optimal dtype\n",
    "\n",
    "print(\"Loading model optimized for RTX 2070 Super...\")\n",
    "\n",
    "try:\n",
    "    # Option A: Use smaller 1.5B model (RECOMMENDED)\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"unsloth/Qwen2.5-1.5B-Instruct\",  # Much smaller model\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = True,\n",
    "        device_map = {\"\": 0},           # Force everything on GPU 0\n",
    "        gpu_memory_utilization = 0.85,  # Can be more aggressive with smaller model\n",
    "        trust_remote_code = True,\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to load 1.5B model: {e}\")\n",
    "    print(\"Trying alternative approach...\")\n",
    "    \n",
    "    # Option B: Original model with CPU offloading (FALLBACK)\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"unsloth/DeepSeek-R1-0528-Qwen3-8B\",\n",
    "        max_seq_length = max_seq_length,           # Reduced further\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = True,\n",
    "        llm_int8_enable_fp32_cpu_offload = True,  # Enable CPU offloading\n",
    "        device_map = \"auto\",            # Auto-distribute across GPU/CPU\n",
    "        gpu_memory_utilization = 0.7,\n",
    "        trust_remote_code = True,\n",
    "    )\n",
    "\n",
    "# Configure LoRA for optimal memory usage\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank * 2,\n",
    "    lora_dropout = 0.1,                # Slightly higher for regularization\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    ")\n",
    "\n",
    "# Memory status\n",
    "print(f\"Model loaded successfully!\")\n",
    "if hasattr(model, 'device'):\n",
    "    print(f\"Model device: {model.device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"VRAM allocated: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "    print(f\"VRAM reserved: {torch.cuda.memory_reserved()/1024**3:.2f}GB\")\n",
    "    print(f\"VRAM free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved())/1024**3:.2f}GB\")\n",
    "\n",
    "# SOLUTION 2: Alternative smaller models for your GPU\n",
    "\"\"\"\n",
    "Other recommended models for RTX 2070 Super:\n",
    " unsloth/Llama-3.2-3B-Instruct - 3B parameters, good balance\n",
    " unsloth/Qwen2.5-3B-Instruct - 3B parameters, efficient\n",
    " unsloth/Phi-3.5-mini-instruct - 3.8B parameters, optimized\n",
    " microsoft/DialoGPT-medium - 355M parameters, very fast\n",
    "\n",
    "For 7B+ models, you'll need CPU offloading or model parallelism\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IodK13om1O4S"
   },
   "source": [
    "### GRPO Chat Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cGSRTJo1O4T"
   },
   "source": [
    "Distill Qwen3 from Deepseek has a chat template that is used to format the input and output of the model. This is used to make the model output in a chat format. Including the reasoning step. We have to use that chat template since the model is trained using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEcLdymBEHdk"
   },
   "source": [
    "Let's see how our chat template behaves on an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for special tokens...\n",
      "Found 22 special tokens\n",
      "\n",
      "Token Summary:\n",
      "Reasoning start: None\n",
      "Reasoning end: None\n",
      "User token: None\n",
      "Assistant token: None\n",
      "\n",
      "System prompt configured:\n",
      "You are given a problem.\n",
      "Think about the problem and provide your working out.\n",
      "You must think in Bahasa Indonesia.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Special Token Detection and System Prompt Setup\n",
    "\n",
    "Scans tokenizer's special vocabulary for reasoning and role tokens\n",
    "Identifies thinking tokens (start/end) for Chain-of-Thought reasoning\n",
    "Finds user and assistant tokens for conversation formatting\n",
    "Creates system prompt requiring reasoning in Bahasa Indonesia\n",
    "Sets up structured thinking framework for the model\n",
    "'''\n",
    "# Initialize token variables\n",
    "reasoning_start = None\n",
    "reasoning_end = None\n",
    "user_token = None\n",
    "assistant_token = None\n",
    "\n",
    "# Scan for special tokens in vocabulary\n",
    "print(\"Scanning for special tokens...\")\n",
    "added_vocab = tokenizer.get_added_vocab()\n",
    "print(f\"Found {len(added_vocab)} special tokens\")\n",
    "\n",
    "for token in added_vocab.keys():\n",
    "    if \"think\" in token.lower() and \"/\" in token:\n",
    "        reasoning_end = token\n",
    "        print(f\"Found reasoning end token: {token}\")\n",
    "    elif \"think\" in token.lower():\n",
    "        reasoning_start = token\n",
    "        print(f\"Found reasoning start token: {token}\")\n",
    "    elif \"user\" in token.lower():\n",
    "        user_token = token\n",
    "        print(f\"Found user token: {token}\")\n",
    "    elif \"assistant\" in token.lower():\n",
    "        assistant_token = token\n",
    "        print(f\"Found assistant token: {token}\")\n",
    "\n",
    "# Display found tokens\n",
    "print(f\"\\nToken Summary:\")\n",
    "print(f\"Reasoning start: {reasoning_start}\")\n",
    "print(f\"Reasoning end: {reasoning_end}\")\n",
    "print(f\"User token: {user_token}\")\n",
    "print(f\"Assistant token: {assistant_token}\")\n",
    "\n",
    "# System prompt for Indonesian reasoning\n",
    "system_prompt = f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your working out.\n",
    "You must think in Bahasa Indonesia.\"\"\"\n",
    "\n",
    "print(f\"\\nSystem prompt configured:\")\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing chat template formatting...\n",
      "==================================================\n",
      "Formatted conversation:\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is 1+1?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>I think it's 2.2</think>2<|im_end|>\n",
      "<|im_start|>user\n",
      "What is 1+1?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>I think it's 2.2</think>2<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "==================================================\n",
      "\n",
      "No reasoning tokens detected - using generic <think> tags\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Chat Template Testing and Format Verification\n",
    "\n",
    "Tests tokenizer's chat template formatting with sample conversation\n",
    "Shows how reasoning tokens (<think>) are integrated into responses\n",
    "Demonstrates multi-turn conversation structure\n",
    "Uses add_generation_prompt=True to prepare for model generation\n",
    "Verifies proper formatting before training/inference\n",
    "'''\n",
    "# Test chat template with reasoning tokens\n",
    "print(\"Testing chat template formatting...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sample_conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"What is 1+1?\"},\n",
    "    {\"role\": \"assistant\", \"content\": f\"<think>I think it's 2.2</think>2\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is 1+1?\"},\n",
    "    {\"role\": \"assistant\", \"content\": f\"<think>I think it's 2.2</think>2\"},\n",
    "]\n",
    "\n",
    "# Apply chat template and display\n",
    "formatted_chat = tokenizer.apply_chat_template(\n",
    "    sample_conversation,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(\"Formatted conversation:\")\n",
    "print(formatted_chat)\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if reasoning tokens are properly handled\n",
    "if reasoning_start and reasoning_end:\n",
    "    print(f\"\\nReasoning tokens detected:\")\n",
    "    print(f\"Start: {reasoning_start}\")\n",
    "    print(f\"End: {reasoning_end}\")\n",
    "    \n",
    "    # Test with detected tokens if available\n",
    "    test_with_detected_tokens = [\n",
    "        {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"{reasoning_start}Let me calculate: 2+2 = 4{reasoning_end}The answer is 4.\"},\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nWith detected reasoning tokens:\")\n",
    "    formatted_with_tokens = tokenizer.apply_chat_template(\n",
    "        test_with_detected_tokens,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    print(formatted_with_tokens)\n",
    "else:\n",
    "    print(\"\\nNo reasoning tokens detected - using generic <think> tags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KGgPgk_5S8r"
   },
   "source": [
    "### Data Prep\n",
    "<a name=\"Data\"></a>\n",
    "\n",
    "We're using Hugging Face's [Open R1 Math dataset](https://huggingface.co/datasets/open-r1/DAPO-Math-17k-Processed). You can also utilize OpenAI's famous [GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DAPO-Math-17k dataset...\n",
      "Dataset loaded successfully!\n",
      "Dataset size: 14116 examples\n",
      "Dataset features: {'prompt': Value(dtype='string', id=None), 'solution': Value(dtype='string', id=None), 'data_source': Value(dtype='string', id=None), 'source_prompt': [{'content': Value(dtype='string', id=None), 'role': Value(dtype='string', id=None)}], 'ability': Value(dtype='string', id=None), 'reward_model': {'ground_truth': Value(dtype='string', id=None), 'style': Value(dtype='string', id=None)}, 'extra_info': {'index': Value(dtype='string', id=None)}}\n",
      "\n",
      "Sample entry:\n",
      "Keys: ['prompt', 'solution', 'data_source', 'source_prompt', 'ability', 'reward_model', 'extra_info']\n",
      "prompt: In triangle $ABC$, $\\sin \\angle A = \\frac{4}{5}$ and $\\angle A < 90^\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\angle BAD = \\angle DAC$ and $\\angle BDC = 90^\\circ$. Suppose that $AD ...\n",
      "solution: 34\n",
      "data_source: math_dapo\n",
      "source_prompt: [{'content': 'Solve the following math problem step by step. The last line of your response should be of the form Answer: $Answer (without quotes) where $Answer is the answer to the problem.\\n\\nIn tri...\n",
      "ability: MATH\n",
      "reward_model: {'ground_truth': '34', 'style': 'rule-lighteval/MATH_v2'}\n",
      "extra_info: {'index': '9a9b6eb4-a1cb-49d1-8c1e-62eaf2f74079'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'solution', 'data_source', 'source_prompt', 'ability', 'reward_model', 'extra_info'],\n",
       "    num_rows: 14116\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Section: Dataset Loading from Hugging Face Hub\n",
    "\n",
    "Loads DAPO-Math-17k-Processed dataset from Hugging Face\n",
    "Specifically loads English (\"en\") subset of the dataset\n",
    "Uses training split containing 17k processed math problems\n",
    "Dataset contains mathematical reasoning examples for fine-tuning\n",
    "'''\n",
    "from datasets import load_dataset\n",
    "import gc\n",
    "\n",
    "print(\"Loading DAPO-Math-17k dataset...\")\n",
    "\n",
    "try:\n",
    "    # Load dataset with memory optimization\n",
    "    dataset = load_dataset(\n",
    "        \"open-r1/DAPO-Math-17k-Processed\", \n",
    "        \"en\", \n",
    "        split=\"train\",\n",
    "        streaming=False,  # Load full dataset for training\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset loaded successfully!\")\n",
    "    print(f\"Dataset size: {len(dataset)} examples\")\n",
    "    print(f\"Dataset features: {dataset.features}\")\n",
    "    \n",
    "    # Display sample to verify format\n",
    "    if len(dataset) > 0:\n",
    "        print(f\"\\nSample entry:\")\n",
    "        print(f\"Keys: {list(dataset[0].keys())}\")\n",
    "        for key, value in dataset[0].items():\n",
    "            print(f\"{key}: {str(value)[:200]}{'...' if len(str(value)) > 200 else ''}\")\n",
    "    \n",
    "    # Memory cleanup\n",
    "    gc.collect()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Attempting alternative loading method...\")\n",
    "    \n",
    "    # Fallback method\n",
    "    try:\n",
    "        dataset = load_dataset(\"open-r1/DAPO-Math-17k-Processed\", split=\"train[:1000]\")\n",
    "        print(f\"Loaded subset: {len(dataset)} examples\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Fallback failed: {e2}\")\n",
    "        dataset = None\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b00gUsS-ROW"
   },
   "source": [
    "Let's look at the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "siopxjG8-ReF",
    "outputId": "0741a0d3-5771-44b9-bb84-80daf02f86a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In triangle $ABC$, $\\\\sin \\\\angle A = \\\\frac{4}{5}$ and $\\\\angle A < 90^\\\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\\\angle BAD = \\\\angle DAC$ and $\\\\angle BDC = 90^\\\\circ$. Suppose that $AD = 1$ and that $\\\\frac{BD}{CD} = \\\\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\\\frac{a\\\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "KGupRQqD-Wcf",
    "outputId": "e8f5aecd-ea06-4dc7-8eb3-c5b5741e2285"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'34'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"solution\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmnXj6hn-Ydi"
   },
   "source": [
    "In GSM8K, ee notice all answers like about have a ####, so we extract it. But for the Open R1 dataset, we can skip the below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing answer extraction function...\n",
      "==================================================\n",
      "Original solution (first 300 chars):\n",
      "34\n",
      "\n",
      "Extracted result:\n",
      "34\n",
      "\n",
      "Dataset analysis (first 100 examples):\n",
      "Examples with '####' pattern: 0/100\n",
      "No '####' pattern found - current logic is appropriate\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Answer Extraction Function\n",
    "\n",
    "Defines function to extract final answers from solution text\n",
    "Originally designed to parse answers after \"####\" delimiter\n",
    "Currently returns full text without extraction (commented out logic)\n",
    "Tests function on first dataset example to verify format\n",
    "'''\n",
    "'''\n",
    "Section: Answer Extraction Function - Fixed Dataset Iteration\n",
    "\n",
    "Error occurs because dataset items are accessed as dictionaries but treated as objects\n",
    "Need to use dictionary key access instead of .get() method\n",
    "Fixed iteration over dataset to properly access solution field\n",
    "'''\n",
    "def extract_hash_answer(text):\n",
    "    \"\"\"\n",
    "    Extract final answer from solution text.\n",
    "    Originally designed for #### delimiter format.\n",
    "    \"\"\"\n",
    "    # Original logic (commented out):\n",
    "    # if \"####\" not in text: return None\n",
    "    # return text.split(\"####\")[1].strip()\n",
    "    \n",
    "    # Currently returns full text\n",
    "    return text\n",
    "\n",
    "# Test extraction function on dataset sample\n",
    "print(\"Testing answer extraction function...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if dataset and len(dataset) > 0:\n",
    "    sample_solution = dataset[0][\"solution\"]\n",
    "    print(f\"Original solution (first 300 chars):\")\n",
    "    print(f\"{sample_solution[:300]}{'...' if len(sample_solution) > 300 else ''}\")\n",
    "    \n",
    "    extracted = extract_hash_answer(sample_solution)\n",
    "    print(f\"\\nExtracted result:\")\n",
    "    print(f\"{extracted[:300]}{'...' if len(extracted) > 300 else ''}\")\n",
    "    \n",
    "    # Fixed: Check if #### pattern exists in dataset\n",
    "    hash_count = 0\n",
    "    sample_size = min(100, len(dataset))\n",
    "    \n",
    "    for i in range(sample_size):\n",
    "        try:\n",
    "            solution = dataset[i][\"solution\"]\n",
    "            if \"####\" in str(solution):\n",
    "                hash_count += 1\n",
    "        except (KeyError, TypeError):\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nDataset analysis (first {sample_size} examples):\")\n",
    "    print(f\"Examples with '####' pattern: {hash_count}/{sample_size}\")\n",
    "    \n",
    "    if hash_count > 0:\n",
    "        print(\"Consider enabling hash extraction logic\")\n",
    "        # Show example with #### pattern\n",
    "        for i in range(min(10, len(dataset))):\n",
    "            if \"####\" in str(dataset[i][\"solution\"]):\n",
    "                print(f\"Example with ####: ...{dataset[i]['solution'][-100:]}\")\n",
    "                break\n",
    "    else:\n",
    "        print(\"No '####' pattern found - current logic is appropriate\")\n",
    "else:\n",
    "    print(\"No dataset available for testing\")\n",
    "    \n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K30CygaU-dir"
   },
   "source": [
    "Let's map the dataset! and see the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming dataset to chat format...\n",
      "==================================================\n",
      "Processing 14116 examples...\n",
      "Dataset transformation completed!\n",
      "Transformed dataset size: 14116\n",
      "\n",
      "Sample transformed item:\n",
      "Prompt structure:\n",
      "  1. Role: system\n",
      "     Content: You are given a problem.\n",
      "Think about the problem and provide your working out.\n",
      "You must think in Bahasa Indonesia.\n",
      "  2. Role: user\n",
      "     Content: In triangle $ABC$, $\\sin \\angle A = \\frac{4}{5}$ and $\\angle A < 90^\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\angle BAD = \\angle D...\n",
      "\n",
      "Answer preview:\n",
      "34\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Dataset Transformation and Formatting\n",
    "\n",
    "Maps dataset to chat format with system/user role structure\n",
    "Combines system prompt with user prompts from dataset\n",
    "Extracts answers from solutions using previously defined function\n",
    "Transforms raw dataset into conversation format suitable for fine-tuning\n",
    "Creates structured prompt-answer pairs for training\n",
    "'''\n",
    "print(\"Transforming dataset to chat format...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def transform_dataset_item(x):\n",
    "    \"\"\"Transform single dataset item to chat format\"\"\"\n",
    "    try:\n",
    "        return {\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": x[\"prompt\"]},\n",
    "            ],\n",
    "            \"answer\": extract_hash_answer(x[\"solution\"]),\n",
    "        }\n",
    "    except KeyError as e:\n",
    "        print(f\"Warning: Missing key {e} in dataset item\")\n",
    "        return {\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": \"\"},\n",
    "            ],\n",
    "            \"answer\": \"\",\n",
    "        }\n",
    "\n",
    "# Apply transformation with progress tracking\n",
    "try:\n",
    "    print(f\"Processing {len(dataset)} examples...\")\n",
    "    dataset = dataset.map(\n",
    "        transform_dataset_item,\n",
    "        desc=\"Transforming dataset\",\n",
    "        num_proc=1,  # Single process for WSL2 stability\n",
    "    )\n",
    "    \n",
    "    print(\"Dataset transformation completed!\")\n",
    "    print(f\"Transformed dataset size: {len(dataset)}\")\n",
    "    \n",
    "    # Display sample transformed item\n",
    "    if len(dataset) > 0:\n",
    "        print(f\"\\nSample transformed item:\")\n",
    "        sample_item = dataset[0]\n",
    "        \n",
    "        print(f\"Prompt structure:\")\n",
    "        for i, msg in enumerate(sample_item[\"prompt\"]):\n",
    "            print(f\"  {i+1}. Role: {msg['role']}\")\n",
    "            content_preview = msg['content'][:150] + \"...\" if len(msg['content']) > 150 else msg['content']\n",
    "            print(f\"     Content: {content_preview}\")\n",
    "        \n",
    "        print(f\"\\nAnswer preview:\")\n",
    "        answer_preview = sample_item[\"answer\"][:200] + \"...\" if len(sample_item[\"answer\"]) > 200 else sample_item[\"answer\"]\n",
    "        print(f\"{answer_preview}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during transformation: {e}\")\n",
    "    print(\"Dataset transformation failed\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9m8eR9T-gMh"
   },
   "source": [
    "We create a regex format to match the reasoning sections and answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up solution end regex pattern...\n",
      "==================================================\n",
      "Warning: No reasoning end token detected!\n",
      "Creating fallback regex pattern...\n",
      "Fallback regex pattern: </think>(.*)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Regex Pattern for Solution End Matching\n",
    "\n",
    "Imports regex module for pattern matching\n",
    "Creates pattern to match text after reasoning end token\n",
    "Uses DOTALL flag to match across multiple lines including newlines\n",
    "Designed to extract final answer portion after reasoning concludes\n",
    "Compiles regex for efficient repeated matching operations\n",
    "'''\n",
    "import re\n",
    "\n",
    "print(\"Setting up solution end regex pattern...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if reasoning end token was found\n",
    "if reasoning_end is not None:\n",
    "    # Create regex pattern to match content after reasoning end token\n",
    "    solution_end_regex = rf\"{re.escape(reasoning_end)}(.*)\"\n",
    "    \n",
    "    # Compile regex with DOTALL flag for multiline matching\n",
    "    match_format = re.compile(solution_end_regex, re.DOTALL)\n",
    "    \n",
    "    print(f\"Reasoning end token: {reasoning_end}\")\n",
    "    print(f\"Regex pattern: {solution_end_regex}\")\n",
    "    print(f\"Compiled regex: {match_format}\")\n",
    "    \n",
    "    # Test the regex pattern with sample text\n",
    "    test_text = f\"Some reasoning here {reasoning_end} Final answer: 42\"\n",
    "    test_match = match_format.search(test_text)\n",
    "    \n",
    "    if test_match:\n",
    "        print(f\"\\nRegex test successful:\")\n",
    "        print(f\"Matched text: '{test_match.group(1).strip()}'\")\n",
    "    else:\n",
    "        print(f\"\\nRegex test failed - no match found\")\n",
    "        \n",
    "else:\n",
    "    print(\"Warning: No reasoning end token detected!\")\n",
    "    print(\"Creating fallback regex pattern...\")\n",
    "    \n",
    "    # Fallback pattern using generic </think> tag\n",
    "    solution_end_regex = r\"</think>(.*)\"\n",
    "    match_format = re.compile(solution_end_regex, re.DOTALL)\n",
    "    print(f\"Fallback regex pattern: {solution_end_regex}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OycMneOq-iNC"
   },
   "source": [
    "We verify it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing regex pattern with sample text...\n",
      "==================================================\n",
      "Test 1:\n",
      "Input: Let me think!</think>Hence, the solution is 2.\n",
      "Matches found: 1\n",
      "  Match 1: 'Hence, the solution is 2.'\n",
      "------------------------------\n",
      "Test 2:\n",
      "Input: Let me think!</think>Hence, the solution is 2.\n",
      "Matches found: 1\n",
      "  Match 1: 'Hence, the solution is 2.'\n",
      "------------------------------\n",
      "Test 3:\n",
      "Input: Complex reasoning here</think>\n",
      "Hence, the solution is 42.\n",
      "This is the final answer.\n",
      "Matches found: 1\n",
      "  Match 1: 'Hence, the solution is 42.\n",
      "This is the final answer.'\n",
      "------------------------------\n",
      "Test 4:\n",
      "Input: Just some text without reasoning tokens.\n",
      "No matches found\n",
      "------------------------------\n",
      "\n",
      "Original test result:\n",
      "Input: Let me think!</think>Hence, the solution is 2.\n",
      "Matches: ['Hence, the solution is 2.']\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Regex Pattern Testing with Sample Text\n",
    "\n",
    "Tests the compiled regex pattern with sample reasoning text\n",
    "Uses findall() to extract all matches of content after reasoning end token\n",
    "Sample text simulates model output with thinking and final answer\n",
    "Validates that regex correctly captures final answer portion\n",
    "'''\n",
    "print(\"Testing regex pattern with sample text...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test sample with different scenarios\n",
    "test_samples = [\n",
    "    # Original test case\n",
    "    \"Let me think!</think>Hence, the solution is 2.\",\n",
    "    \n",
    "    # Test with detected reasoning end token if available\n",
    "    f\"Let me think!{reasoning_end}Hence, the solution is 2.\" if reasoning_end else \"Let me think!</think>Hence, the solution is 2.\",\n",
    "    \n",
    "    # Multi-line test\n",
    "    f\"Complex reasoning here{reasoning_end}\\nHence, the solution is 42.\\nThis is the final answer.\" if reasoning_end else \"Complex reasoning here</think>\\nHence, the solution is 42.\\nThis is the final answer.\",\n",
    "    \n",
    "    # No match test\n",
    "    \"Just some text without reasoning tokens.\"\n",
    "]\n",
    "\n",
    "for i, test_text in enumerate(test_samples, 1):\n",
    "    print(f\"Test {i}:\")\n",
    "    print(f\"Input: {test_text[:100]}{'...' if len(test_text) > 100 else ''}\")\n",
    "    \n",
    "    # Use findall to get all matches\n",
    "    matches = match_format.findall(test_text)\n",
    "    \n",
    "    if matches:\n",
    "        print(f\"Matches found: {len(matches)}\")\n",
    "        for j, match in enumerate(matches):\n",
    "            cleaned_match = match.strip()\n",
    "            print(f\"  Match {j+1}: '{cleaned_match[:150]}{'...' if len(cleaned_match) > 150 else ''}'\")\n",
    "    else:\n",
    "        print(\"No matches found\")\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# Test the original example specifically\n",
    "original_test = \"Let me think!</think>Hence, the solution is 2.\"\n",
    "original_matches = match_format.findall(original_test)\n",
    "\n",
    "print(f\"\\nOriginal test result:\")\n",
    "print(f\"Input: {original_test}\")\n",
    "print(f\"Matches: {original_matches}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing regex with proper think tag format...\n",
      "==================================================\n",
      "Test input:\n",
      "'<think>Let me think!</think>\n",
      "\n",
      "Hence, the solution is 2'\n",
      "\n",
      "Regex pattern used: </think>(.*)\n",
      "Matches found: 1\n",
      "Match 1: 'Hence, the solution is 2'\n",
      "Raw match (with whitespace): '\\n\\nHence, the solution is 2'\n",
      "\n",
      "Additional test cases:\n",
      "------------------------------\n",
      "Test 1: <think>Complex calculation</think>The answer is 42\n",
      "Result: ['The answer is 42']\n",
      "\n",
      "Test 2: <think>Working step by step</think>\n",
      "\n",
      "Step 1: Calculate\n",
      "Step ...\n",
      "Result: ['\\n\\nStep 1: Calculate\\nStep 2: Verify\\nFinal answer: 100']\n",
      "\n",
      "Test 3: <think></think>Direct answer: 5\n",
      "Result: ['Direct answer: 5']\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Regex Pattern Testing with Proper Think Tags\n",
    "\n",
    "Tests regex with properly formatted thinking tags (<think></think>)\n",
    "Includes newlines between reasoning and final answer sections\n",
    "Verifies pattern matching works with realistic model output format\n",
    "Demonstrates extraction of solution text after reasoning completion\n",
    "'''\n",
    "print(\"Testing regex with proper think tag format...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test the specific example with proper formatting\n",
    "test_input = \"<think>Let me think!</think>\\n\\nHence, the solution is 2\"\n",
    "print(f\"Test input:\")\n",
    "print(f\"'{test_input}'\")\n",
    "print()\n",
    "\n",
    "# Apply findall to extract matches\n",
    "matches = match_format.findall(test_input)\n",
    "\n",
    "print(f\"Regex pattern used: {match_format.pattern}\")\n",
    "print(f\"Matches found: {len(matches)}\")\n",
    "\n",
    "if matches:\n",
    "    for i, match in enumerate(matches):\n",
    "        # Clean up whitespace for display\n",
    "        cleaned_match = match.strip()\n",
    "        print(f\"Match {i+1}: '{cleaned_match}'\")\n",
    "        \n",
    "        # Show raw match with whitespace visible\n",
    "        print(f\"Raw match (with whitespace): {repr(match)}\")\n",
    "else:\n",
    "    print(\"No matches found\")\n",
    "\n",
    "# Additional test cases with variations\n",
    "additional_tests = [\n",
    "    # With reasoning_end token if detected\n",
    "    f\"<think>Complex calculation</think>{reasoning_end}The answer is 42\" if reasoning_end else \"<think>Complex calculation</think>The answer is 42\",\n",
    "    \n",
    "    # Multi-line answer\n",
    "    \"<think>Working step by step</think>\\n\\nStep 1: Calculate\\nStep 2: Verify\\nFinal answer: 100\",\n",
    "    \n",
    "    # Empty reasoning\n",
    "    \"<think></think>Direct answer: 5\"\n",
    "]\n",
    "\n",
    "print(\"\\nAdditional test cases:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for i, test_case in enumerate(additional_tests, 1):\n",
    "    print(f\"Test {i}: {test_case[:60]}{'...' if len(test_case) > 60 else ''}\")\n",
    "    result = match_format.findall(test_case)\n",
    "    print(f\"Result: {result}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weOjmO5l-kl3"
   },
   "source": [
    "We now want to create a reward function to match the format exactly - we reward it with 3 points if it succeeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing format scoring function...\n",
      "==================================================\n",
      "Warning: Error processing completion 0: list indices must be integers or slices, not str\n",
      "Warning: Error processing completion 1: list indices must be integers or slices, not str\n",
      "Warning: Error processing completion 2: list indices must be integers or slices, not str\n",
      "Test scores: [0, 0, 0]\n",
      "Error testing scoring function: list indices must be integers or slices, not str\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Format Matching Scoring Function\n",
    "\n",
    "Defines scoring function for evaluating completion quality\n",
    "Awards 3.0 points if regex pattern is found in model response\n",
    "Used for ranking/filtering model outputs during inference\n",
    "Ensures responses follow expected reasoning-to-answer format\n",
    "Returns list of scores corresponding to input completions\n",
    "'''\n",
    "def match_format_exactly(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Score completions based on format matching.\n",
    "    Awards points for proper reasoning-to-answer structure.\n",
    "    \n",
    "    Args:\n",
    "        completions: List of completion objects with content\n",
    "        **kwargs: Additional arguments (unused but maintained for compatibility)\n",
    "    \n",
    "    Returns:\n",
    "        List of scores for each completion\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for i, completion in enumerate(completions):\n",
    "        score = 0\n",
    "        \n",
    "        try:\n",
    "            # Extract response content\n",
    "            response = completion[0][\"content\"]\n",
    "            \n",
    "            # Check if response matches expected format\n",
    "            if match_format.search(response) is not None:\n",
    "                score += 3.0\n",
    "                # Optional: Add debug info\n",
    "                # print(f\"Completion {i}: Format match found (+3.0)\")\n",
    "            \n",
    "        except (IndexError, KeyError, TypeError) as e:\n",
    "            # Handle malformed completion objects\n",
    "            print(f\"Warning: Error processing completion {i}: {e}\")\n",
    "            score = 0\n",
    "        \n",
    "        scores.append(score)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Test the scoring function\n",
    "print(\"Testing format scoring function...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create mock completions for testing - FIXED SYNTAX\n",
    "test_completions = [\n",
    "    # Good format - should score 3.0\n",
    "    [[{\"content\": f\"<think>Let me calculate</think>{reasoning_end if reasoning_end else '</think>'}The answer is 42\"}]],\n",
    "    \n",
    "    # Bad format - should score 0.0 - FIXED\n",
    "    [[{\"content\": \"Just a direct answer without reasoning\"}]],\n",
    "    \n",
    "    # Another good format\n",
    "    [[{\"content\": \"<think>Step by step</think>\\n\\nFinal result: 100\"}]],\n",
    "]\n",
    "\n",
    "# Test scoring\n",
    "try:\n",
    "    test_scores = match_format_exactly(test_completions)\n",
    "    print(f\"Test scores: {test_scores}\")\n",
    "    \n",
    "    for i, (completion, score) in enumerate(zip(test_completions, test_scores)):\n",
    "        content = completion[0][\"content\"][:50] + \"...\" if len(completion[0][\"content\"]) > 50 else completion[0][\"content\"]\n",
    "        print(f\"Completion {i}: '{content}' -> Score = {score}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error testing scoring function: {e}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gf69i2WT-m4K"
   },
   "source": [
    "If it fails, we want to reward the model if it at least follows the format partially, by counting each symbol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing approximate format scoring function...\n",
      "==================================================\n",
      "Warning: Error processing completion 0: list indices must be integers or slices, not str\n",
      "Warning: Error processing completion 1: list indices must be integers or slices, not str\n",
      "Warning: Error processing completion 2: list indices must be integers or slices, not str\n",
      "Warning: Error processing completion 3: list indices must be integers or slices, not str\n",
      "Test scores: [-2.0, -2.0, -2.0, -2.0]\n",
      "Expected: [1.0, -0.5, -2.0, -2.0] (approximately)\n",
      "Error testing approximate scoring: list indices must be integers or slices, not str\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Approximate Format Matching Scoring Function\n",
    "\n",
    "Scores completions based on proper reasoning token usage\n",
    "Awards +0.5 for exactly one occurrence of reasoning start/end tokens\n",
    "Penalizes with -1.0 for incorrect token counts (0 or multiple occurrences)\n",
    "More lenient than exact matching but ensures proper token structure\n",
    "Prevents malformed reasoning sections with missing or duplicated tokens\n",
    "'''\n",
    "def match_format_approximately(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Score completions based on approximate format matching.\n",
    "    Rewards proper reasoning token usage and penalizes malformed structure.\n",
    "    \n",
    "    Args:\n",
    "        completions: List of completion objects with content\n",
    "        **kwargs: Additional arguments (unused but maintained for compatibility)\n",
    "    \n",
    "    Returns:\n",
    "        List of scores for each completion\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for i, completion in enumerate(completions):\n",
    "        score = 0\n",
    "        \n",
    "        try:\n",
    "            response = completion[0][\"content\"]\n",
    "            \n",
    "            # Check reasoning start token count\n",
    "            if reasoning_start:\n",
    "                start_count = response.count(reasoning_start)\n",
    "                if start_count == 1:\n",
    "                    score += 0.5\n",
    "                else:\n",
    "                    score -= 1.0\n",
    "                    # Optional debug: print(f\"Start token count: {start_count}\")\n",
    "            \n",
    "            # Check reasoning end token count  \n",
    "            if reasoning_end:\n",
    "                end_count = response.count(reasoning_end)\n",
    "                if end_count == 1:\n",
    "                    score += 0.5\n",
    "                else:\n",
    "                    score -= 1.0\n",
    "                    # Optional debug: print(f\"End token count: {end_count}\")\n",
    "            \n",
    "        except (IndexError, KeyError, TypeError) as e:\n",
    "            print(f\"Warning: Error processing completion {i}: {e}\")\n",
    "            score = -2.0  # Heavy penalty for malformed input\n",
    "        \n",
    "        scores.append(score)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Test the approximate scoring function\n",
    "print(\"Testing approximate format scoring function...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create test completions with various token patterns\n",
    "test_completions = [\n",
    "    # Perfect format - should score +1.0 (0.5 + 0.5)\n",
    "    [[{\"content\": f\"Some text {reasoning_start}reasoning{reasoning_end} answer\"}]] if reasoning_start and reasoning_end else [[{\"content\": \"<think>reasoning</think> answer\"}]],\n",
    "    \n",
    "    # Missing end token - should score -0.5 (0.5 - 1.0)\n",
    "    [[{\"content\": f\"Some text {reasoning_start}reasoning without end\"}]] if reasoning_start else [[{\"content\": \"<think>reasoning without end\"}]],\n",
    "    \n",
    "    # Duplicate tokens - should score -2.0 (-1.0 - 1.0)\n",
    "    [[{\"content\": f\"{reasoning_start}first{reasoning_end} and {reasoning_start}second{reasoning_end}\"}]] if reasoning_start and reasoning_end else [[{\"content\": \"<think>first</think> and <think>second</think>\"}]],\n",
    "    \n",
    "    # No reasoning tokens - should score -2.0\n",
    "    [[{\"content\": \"Just plain text with no reasoning\"}]],\n",
    "]\n",
    "\n",
    "try:\n",
    "    test_scores = match_format_approximately(test_completions)\n",
    "    print(f\"Test scores: {test_scores}\")\n",
    "    print(f\"Expected: [1.0, -0.5, -2.0, -2.0] (approximately)\")\n",
    "    \n",
    "    for i, (completion, score) in enumerate(zip(test_completions, test_scores)):\n",
    "        content = completion[0][\"content\"][:60] + \"...\" if len(completion[0][\"content\"]) > 60 else completion[0][\"content\"]\n",
    "        print(f\"Completion {i}: '{content}' -> Score = {score}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error testing approximate scoring: {e}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wAUWwtE-s6n"
   },
   "source": [
    "We want to extract the generated answer, and reward or penalize it! We also reward it based on how close the answer is to the true one via ratios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing answer accuracy scoring function...\n",
      "==================================================\n",
      "Error in check_answer: list indices must be integers or slices, not str\n",
      "Test scores: [-5.0, -5.0, -5.0, -5.0]\n",
      "Test 0: Score = -5.0 (expected 5.0 (exact))\n",
      "Test 1: Score = -5.0 (expected 2.0 (close))\n",
      "Test 2: Score = -5.0 (expected -2.5 (wrong))\n",
      "Test 3: Score = -5.0 (expected -2.0 (no answer))\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Answer Accuracy Scoring Function\n",
    "\n",
    "Extracts predicted answers from completions using regex pattern\n",
    "Compares extracted answers against ground truth with multiple scoring tiers\n",
    "Awards 5.0 points for exact matches, 3.5 for whitespace-trimmed matches\n",
    "Uses ratio-based scoring for numerical answers (2.0 for 10%, 1.5 for 20%)\n",
    "Heavy penalties for wrong answers (-2.5 to -4.5) to discourage hallucination\n",
    "'''\n",
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"\n",
    "    Score completions based on answer accuracy.\n",
    "    Uses tiered scoring system for exact, approximate, and numerical matches.\n",
    "    \n",
    "    Args:\n",
    "        prompts: List of prompt messages\n",
    "        completions: List of completion objects\n",
    "        answer: List of correct answers\n",
    "        **kwargs: Additional arguments\n",
    "    \n",
    "    Returns:\n",
    "        List of scores for each completion\n",
    "    \"\"\"\n",
    "    try:\n",
    "        question = prompts[0][-1][\"content\"]\n",
    "        responses = [completion[0][\"content\"] for completion in completions]\n",
    "        \n",
    "        # Extract answers using regex pattern\n",
    "        extracted_responses = []\n",
    "        for r in responses:\n",
    "            match = match_format.search(r)\n",
    "            extracted_responses.append(match.group(1) if match is not None else None)\n",
    "        \n",
    "        scores = []\n",
    "        \n",
    "        for i, (guess, true_answer) in enumerate(zip(extracted_responses, answer)):\n",
    "            score = 0\n",
    "            \n",
    "            if guess is None:\n",
    "                scores.append(-2.0)  # No extractable answer\n",
    "                continue\n",
    "            \n",
    "            # Clean extracted guess\n",
    "            guess_clean = guess.strip()\n",
    "            true_answer_clean = str(true_answer).strip()\n",
    "            \n",
    "            # Exact match - highest reward\n",
    "            if guess_clean == true_answer_clean:\n",
    "                score += 5.0\n",
    "            \n",
    "            # Whitespace differences - good match  \n",
    "            elif guess_clean.replace(\" \", \"\") == true_answer_clean.replace(\" \", \"\"):\n",
    "                score += 3.5\n",
    "            \n",
    "            # Numerical comparison for math problems\n",
    "            else:\n",
    "                try:\n",
    "                    # Extract numbers from strings if needed\n",
    "                    import re\n",
    "                    guess_nums = re.findall(r'-?\\d+\\.?\\d*', guess_clean)\n",
    "                    true_nums = re.findall(r'-?\\d+\\.?\\d*', true_answer_clean)\n",
    "                    \n",
    "                    if guess_nums and true_nums:\n",
    "                        guess_val = float(guess_nums[-1])  # Take last number\n",
    "                        true_val = float(true_nums[-1])\n",
    "                        \n",
    "                        if true_val != 0:\n",
    "                            ratio = guess_val / true_val\n",
    "                            if 0.9 <= ratio <= 1.1:\n",
    "                                score += 2.0    # Within 10%\n",
    "                            elif 0.8 <= ratio <= 1.2:\n",
    "                                score += 1.5    # Within 20%\n",
    "                            else:\n",
    "                                score -= 2.5    # Wrong numerical answer\n",
    "                        else:\n",
    "                            score -= 2.5\n",
    "                    else:\n",
    "                        score -= 4.5    # Non-numerical wrong answer\n",
    "                        \n",
    "                except (ValueError, ZeroDivisionError, IndexError):\n",
    "                    score -= 4.5    # Cannot process answer\n",
    "            \n",
    "            scores.append(score)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in check_answer: {e}\")\n",
    "        return [-5.0] * len(completions)  # Severe penalty for function failure\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Test the answer checking function\n",
    "print(\"Testing answer accuracy scoring function...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Mock test data\n",
    "test_prompts = [\n",
    "    [{\"role\": \"user\", \"content\": \"What is 2 + 2?\"}]\n",
    "]\n",
    "\n",
    "test_completions = [\n",
    "    # Exact match\n",
    "    [[{\"content\": f\"<think>Let me add</think>{reasoning_end if reasoning_end else '</think>'}4\"}]],\n",
    "    \n",
    "    # Close numerical answer\n",
    "    [[{\"content\": f\"<think>Calculating</think>{reasoning_end if reasoning_end else '</think>'}4.1\"}]],\n",
    "    \n",
    "    # Wrong answer\n",
    "    [[{\"content\": f\"<think>Adding</think>{reasoning_end if reasoning_end else '</think>'}5\"}]],\n",
    "    \n",
    "    # No extractable answer\n",
    "    [[{\"content\": \"Just thinking without proper format\"}]],\n",
    "]\n",
    "\n",
    "test_answers = [\"4\", \"4\", \"4\", \"4\"]\n",
    "\n",
    "try:\n",
    "    test_scores = check_answer(test_prompts, test_completions, test_answers)\n",
    "    print(f\"Test scores: {test_scores}\")\n",
    "    \n",
    "    expected_ranges = [\"5.0 (exact)\", \"2.0 (close)\", \"-2.5 (wrong)\", \"-2.0 (no answer)\"]\n",
    "    for i, (score, expected) in enumerate(zip(test_scores, expected_ranges)):\n",
    "        print(f\"Test {i}: Score = {score} (expected {expected})\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error testing answer checking: {e}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atMyfhXh-v3R"
   },
   "source": [
    "Also sometimes it might not be 1 number as the answer, but like a sentence for example \"The solution is $20\" -> we extract 20.\n",
    "\n",
    "We also remove possible commas for example as in 123,456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing number extraction regex pattern...\n",
      "==================================================\n",
      "Regex pattern: .*?[\\s]{0,}([-]?[\\d\\.\\,]{1,})\n",
      "Flags: MULTILINE | DOTALL\n",
      "\n",
      "Test Results:\n",
      "------------------------------\n",
      "Test  1: '  0.34              ' -> ['0.34']\n",
      "Test  2: '  123,456           ' -> ['123,456']\n",
      "Test  3: '  -0.234            ' -> ['-0.234']\n",
      "Test  4: '17                  ' -> ['17']\n",
      "Test  5: 'The answer is 42    ' -> ['42']\n",
      "Test  6: '-999.99             ' -> ['-999.99']\n",
      "Test  7: '1,234.56            ' -> ['1,234.56']\n",
      "Test  8: 'No numbers here     ' -> []\n",
      "Test  9: '3.14159             ' -> ['3.14159']\n",
      "Test 10: 'Multiple 123 and 456' -> ['123', '456']\n",
      "\n",
      "Pattern Analysis:\n",
      "------------------------------\n",
      " .*? - Non-greedy match any characters\n",
      " [\\s]{0,} - Optional whitespace (0 or more)\n",
      " ([-]?[\\d\\.\\,]{1,}) - Capture group:\n",
      "  - [-]? - Optional negative sign\n",
      "  - [\\d\\.\\,]{1,} - One or more digits, dots, or commas\n",
      " MULTILINE: ^ and $ match line boundaries\n",
      " DOTALL: . matches newline characters\n",
      "\n",
      "Multi-line test:\n",
      "Input: '\\nStep 1: Calculate\\nThe result is 42.5\\nFinal answer: 100\\n'\n",
      "Matches: ['1', '42.5', '100']\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "print(\"Testing number extraction regex pattern...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create regex pattern for number extraction\n",
    "match_numbers = re.compile(\n",
    "    r\".*?[\\s]{0,}([-]?[\\d\\.\\,]{1,})\",\n",
    "    flags=re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "print(\"Regex pattern:\", match_numbers.pattern)\n",
    "print(\"Flags: MULTILINE | DOTALL\")\n",
    "print()\n",
    "\n",
    "# Test cases with various number formats\n",
    "test_cases = [\n",
    "    \"  0.34  \",           # Decimal with spaces\n",
    "    \"  123,456  \",        # Comma-separated number\n",
    "    \"  -0.234  \",         # Negative decimal\n",
    "    \"17\",                 # Simple integer\n",
    "    \"The answer is 42\",   # Number in sentence\n",
    "    \"-999.99\",            # Negative decimal\n",
    "    \"1,234.56\",           # Mixed comma and decimal\n",
    "    \"No numbers here\",    # No numbers\n",
    "    \"3.14159\",            # Pi approximation\n",
    "    \"Multiple 123 and 456\", # Multiple numbers\n",
    "]\n",
    "\n",
    "print(\"Test Results:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for i, test_input in enumerate(test_cases, 1):\n",
    "    matches = match_numbers.findall(test_input)\n",
    "    print(f\"Test {i:2d}: '{test_input:20}' -> {matches}\")\n",
    "\n",
    "# Additional analysis\n",
    "print()\n",
    "print(\"Pattern Analysis:\")\n",
    "print(\"-\" * 30)\n",
    "print(\" .*? - Non-greedy match any characters\")\n",
    "print(\" [\\\\s]{0,} - Optional whitespace (0 or more)\")\n",
    "print(\" ([-]?[\\\\d\\\\.\\\\,]{1,}) - Capture group:\")\n",
    "print(\"  - [-]? - Optional negative sign\")\n",
    "print(\"  - [\\\\d\\\\.\\\\,]{1,} - One or more digits, dots, or commas\")\n",
    "print(\" MULTILINE: ^ and $ match line boundaries\")\n",
    "print(\" DOTALL: . matches newline characters\")\n",
    "\n",
    "# Test with multi-line input\n",
    "multiline_test = \"\"\"\n",
    "Step 1: Calculate\n",
    "The result is 42.5\n",
    "Final answer: 100\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nMulti-line test:\")\n",
    "print(f\"Input: {repr(multiline_test)}\")\n",
    "print(f\"Matches: {match_numbers.findall(multiline_test)}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19KD28CXW_EO"
   },
   "source": [
    "Finally, we will try to enforce the thinking process to be in Bahasa Indonesia. This is a simple version of the `language consistency reward` that is used in DeepSeek R1 paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing language detection function...\n",
      "==================================================\n",
      "Language Detection Results:\n",
      "------------------------------\n",
      "Test  1:  'Hello, How are you            ' -> en  (expected: en) (conf: -9.565)\n",
      "Test  2:  'Aku berpikir kalau aku adalah ' -> id  (expected: id) (conf: -44.182)\n",
      "Test  3:  '                          ' -> zh  (expected: zh) (conf: -46.761)\n",
      "Test  4:  '                              ' -> und (expected: und)\n",
      "Test  5:  '                              ' -> und (expected: und)\n",
      "Test  6:  'Bonjour, comment allez-vous?  ' -> en  (expected: fr) (conf: -22.992)\n",
      "Test  7:  'Hola, cmo ests?            ' -> gl  (expected: es) (conf: -86.695)\n",
      "Test  8:  'Guten Tag, wie geht es Ihnen? ' -> de  (expected: de) (conf: -88.512)\n",
      "Test  9:  '123 456 789                   ' -> en  (expected: und) (conf: 9.062)\n",
      "\n",
      "Testing with system prompt:\n",
      "System prompt language: en\n",
      "System prompt preview: You are given a problem.\n",
      "Think about the problem and provide your working out.\n",
      "You must think in Bah...\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Language Detection Function\n",
    "\n",
    "Imports langid library for automatic language identification\n",
    "Creates function to detect language of input text\n",
    "Returns \"und\" (undefined) for empty text inputs\n",
    "Uses langid.classify() which returns language code and confidence score\n",
    "Tests function with English, Indonesian, and Chinese text samples\n",
    "'''\n",
    "import langid\n",
    "\n",
    "def get_lang(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Detect language of input text using langid.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        \n",
    "    Returns:\n",
    "        Language code (e.g., 'en', 'id', 'zh') or 'und' for undefined\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return \"und\"  # undefined language\n",
    "    \n",
    "    try:\n",
    "        lang, confidence = langid.classify(text)\n",
    "        return lang\n",
    "    except Exception as e:\n",
    "        print(f\"Language detection error: {e}\")\n",
    "        return \"und\"\n",
    "\n",
    "# Test language detection\n",
    "print(\"Testing language detection function...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_cases = [\n",
    "    (\"Hello, How are you\", \"en\"),\n",
    "    (\"Aku berpikir kalau aku adalah kamu\", \"id\"), \n",
    "    (\"\", \"zh\"),\n",
    "    (\"\", \"und\"),  # Empty string test\n",
    "    (\"   \", \"und\"),  # Whitespace only test\n",
    "    (\"Bonjour, comment allez-vous?\", \"fr\"),  # French\n",
    "    (\"Hola, cmo ests?\", \"es\"),  # Spanish\n",
    "    (\"Guten Tag, wie geht es Ihnen?\", \"de\"),  # German\n",
    "    (\"123 456 789\", \"und\"),  # Numbers only\n",
    "]\n",
    "\n",
    "print(\"Language Detection Results:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for i, (text, expected) in enumerate(test_cases, 1):\n",
    "    detected = get_lang(text)\n",
    "    status = \"\" if detected == expected else \"\"\n",
    "    \n",
    "    # Get confidence for non-empty strings\n",
    "    if text.strip():\n",
    "        try:\n",
    "            _, confidence = langid.classify(text)\n",
    "            conf_str = f\" (conf: {confidence:.3f})\"\n",
    "        except:\n",
    "            conf_str = \"\"\n",
    "    else:\n",
    "        conf_str = \"\"\n",
    "    \n",
    "    print(f\"Test {i:2d}: {status} '{text[:30]:30}' -> {detected:3} (expected: {expected}){conf_str}\")\n",
    "\n",
    "# Test with reasoning content\n",
    "print(f\"\\nTesting with system prompt:\")\n",
    "system_prompt_lang = get_lang(system_prompt)\n",
    "print(f\"System prompt language: {system_prompt_lang}\")\n",
    "print(f\"System prompt preview: {system_prompt[:100]}...\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "czn2loIDW_EQ"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def format_and_language_reward_func(completions, **kwargs):\n",
    "    scores = []\n",
    "\n",
    "    for completion_item in completions:\n",
    "        if not completion_item or not isinstance(completion_item[0], dict) or \"content\" not in completion_item[0]:\n",
    "            scores.append(-5.0)\n",
    "            print(f\"Warning: Malformed completion item, assigning default low score: {completion_item}\")\n",
    "            continue\n",
    "\n",
    "        content = completion_item[0][\"content\"]\n",
    "\n",
    "        lang = get_lang(content)\n",
    "\n",
    "        if lang == 'id':\n",
    "            score = 5.0\n",
    "        elif lang == 'en':\n",
    "            score = -3.0\n",
    "        elif lang == 'zh':\n",
    "            score = -3.0\n",
    "        else:\n",
    "            score = -5.0\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjTfmkTAW_ER",
    "outputId": "4e70daa7-3ac0-4e66-ead5-8541b7905185"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-3.0, -3.0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [\n",
    "    [{\"role\": \"assistant\", \"content\": \"What is the result of (1 + 2) * 4?\"}],\n",
    "    [{\"role\": \"assistant\", \"content\": \"What is the result of (3 + 1) * 2?\"}],\n",
    "]\n",
    "completions = [\n",
    "    [{\"role\": \"assistant\", \"content\": \"<think>The sum of 1 and 2 is 3, which we multiply by 4 to get 12.</think><answer>(1 + 2) * 4 = 12</answer>\"}],\n",
    "    [{\"role\": \"assistant\", \"content\": \"The sum of 3 and 1 is 4, which we multiply by 2 to get 8. So (3 + 1) * 2 = 8.\"}],\n",
    "]\n",
    "format_and_language_reward_func(prompts=prompts, completions=completions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbfaaAywNHHh"
   },
   "source": [
    "We now prepare our main function which will print out the generated responses and the true answer, along with another reward function which converts text to float via `float` and sees if it's the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "GjBFrttr-y1_"
   },
   "outputs": [],
   "source": [
    "global PRINTED_TIMES\n",
    "PRINTED_TIMES = 0\n",
    "global PRINT_EVERY_STEPS\n",
    "PRINT_EVERY_STEPS = 5\n",
    "\n",
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_numbers.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    # Print only every few steps\n",
    "    global PRINTED_TIMES\n",
    "    global PRINT_EVERY_STEPS\n",
    "    if PRINTED_TIMES % PRINT_EVERY_STEPS == 0:\n",
    "        print(\n",
    "            '*'*20 + f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\"\n",
    "        )\n",
    "    PRINTED_TIMES += 1\n",
    "\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        if guess is None:\n",
    "            scores.append(-2.5)\n",
    "            continue\n",
    "        # Convert to numbers\n",
    "        try:\n",
    "            true_answer = float(true_answer.strip())\n",
    "            # Remove commas like in 123,456\n",
    "            guess       = float(guess.strip().replace(\",\", \"\"))\n",
    "            scores.append(3.5 if guess == true_answer else -1.5)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgOR3wJ_AyLr"
   },
   "source": [
    "Get the top 90% prompt length so we don't accidentally truncate them!\n",
    "\n",
    "Ie we'll remove the top 10% long prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171,
     "referenced_widgets": [
      "efe86cb2e7174a149f1c01544b1f9d4f",
      "73b5b814482f4d1faa3cbd8d59520b19",
      "1e53f50fc9de48f48919b3eef416d825",
      "c0b6b9c6c70946f292cbd3ef194647f3",
      "2c4bbe0814524b8dae16f0ea435f388d",
      "4d9c429efca2406ca4dbb606fd888f27",
      "2310e1c0d376470990d063de29eabb68",
      "a58bf1ccd1614069b87c278b14620636",
      "27c53c1f5c934e1eb381ed302f186b85",
      "b8d562a3b0fb40e0a15d87830f90aa4c",
      "b2520312d2a345b3913ef02bca7d060f",
      "4bdd6660598447aab9bd8249d98ec407",
      "6bdca654291e410da6f0859be242a3cd",
      "38b11c99e03c454499f6013ed0326e4c",
      "74c2df2966ab4e30b27a326a6f39993b",
      "92066c72faba4956a29bde4deaa0fed3",
      "a670d8e6c3414a35a369476b77a178ac",
      "976c8cfbb4cd444a83f28a65c75e057d",
      "efff7b47e0c543c1b579ca9f855f4954",
      "1eff3897c2af43fd8be2d2cd0bc7b512",
      "57d307bad8e7431eb9d9562ffafd7d17",
      "f77640ae66ae410d9a1858cf7880a192"
     ]
    },
    "id": "6EgAi4Q5fGE-",
    "outputId": "e3719f82-3c6a-4cb3-9bc1-995cd67689a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are given a problem.\n",
      "Think about the problem and provide your working out.\n",
      "You must think in Bahasa Indonesia.<|im_end|>\n",
      "<|im_start|>user\n",
      "In triangle $ABC$, $\\sin \\angle A = \\frac{4}{5}$ and $\\angle A < 90^\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\angle BAD = \\angle DAC$ and $\\angle BDC = 90^\\circ$. Suppose that $AD = 1$ and that $\\frac{BD}{CD} = \\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\frac{a\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Max Length =  190\n"
     ]
    }
   ],
   "source": [
    "tokenized = dataset.map(\n",
    "    lambda x: {\"tokens\" : tokenizer.apply_chat_template(x[\"prompt\"], add_generation_prompt = True, tokenize = True)},\n",
    "    batched = True,\n",
    ")\n",
    "print(tokenizer.decode(tokenized[0][\"tokens\"]))\n",
    "tokenized = tokenized.map(lambda x: {\"L\" : len(x[\"tokens\"])})\n",
    "\n",
    "import numpy as np\n",
    "maximum_length = int(np.quantile(tokenized[\"L\"], 0.9))\n",
    "print(\"Max Length = \", maximum_length)\n",
    "\n",
    "# Filter only samples smaller than 90% max length\n",
    "dataset = dataset.select(np.where(np.array(tokenized[\"L\"]) <= maximum_length)[0])\n",
    "del tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-IOMhVg-2AM"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "\n",
    "Now set up GRPO Trainer and all configurations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptqkXK2D4d6p",
    "outputId": "bbabb969-329e-48c3-d92e-c27f35fc7766"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: The DAPO paper recommends `mask_truncated_completions = True`\n",
      "Unsloth: The DAPO paper recommends `epsilon_high = 0.28`\n",
      "Unsloth: The DAPO paper recommends setting `beta = 0.0` to remove the KL term\n",
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 2\n"
     ]
    }
   ],
   "source": [
    "max_prompt_length = maximum_length + 1 # + 1 just in case!\n",
    "max_completion_length = max_seq_length - max_prompt_length\n",
    "\n",
    "from vllm import SamplingParams\n",
    "vllm_sampling_params = SamplingParams(\n",
    "    min_p = 0.1,\n",
    "    top_p = 1.0,\n",
    "    top_k = -1,\n",
    "    seed = 3407,\n",
    "    stop = [tokenizer.eos_token],\n",
    "    include_stop_str_in_output = True,\n",
    ")\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    vllm_sampling_params = vllm_sampling_params,\n",
    "    temperature = 1.0,\n",
    "    learning_rate = 1e-6, #5e-6,\n",
    "    weight_decay = 0.01,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 4, # Increase to 4 for smoother training\n",
    "    num_generations = 2, # Decrease if out of memory\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_completion_length,\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps = 200,\n",
    "    save_steps = 200,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = \"outputs\",\n",
    "\n",
    "    # For optional training + evaluation\n",
    "    # fp16_full_eval = True,\n",
    "    # per_device_eval_batch_size = 4,\n",
    "    # eval_accumulation_steps = 1,\n",
    "    # eval_strategy = \"steps\",\n",
    "    # eval_steps = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9Mv8UZO5hz-"
   },
   "source": [
    "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
    "\n",
    "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
    "\n",
    "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
    "|------|---------------|-----------|------------|-------------------|----------|\n",
    "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
    "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
    "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vzOuSVCL_GA9",
    "outputId": "58162095-173b-465e-af98-3c717e8d8424"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 12,728 | Num Epochs = 1 | Total steps = 200\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 18,464,768 of 1,562,179,072 (1.18% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************Question:\n",
      "In the diagram, each of the three identical circles touch the other two.  The circumference of each circle is 36.  What is the perimeter of the shaded region? [asy]\n",
      "\n",
      "defaultpen(1);\n",
      "\n",
      "path p = (1, 0){down}..{-dir(30)}dir(-60){dir(30)}..{dir(-30)}((2, 0) + dir(-120)){-dir(-30)}..{up}(1, 0)--cycle;\n",
      "fill(p, gray(0.75));\n",
      "\n",
      "draw(unitcircle);\n",
      "draw(shift(2 * dir(-60)) * unitcircle);\n",
      "draw(shift(2) * unitcircle);\n",
      "[/asy] \n",
      "Answer:\n",
      "18 \n",
      "Response:\n",
      "1. **Geometrical Interpretation**:\n",
      "   - Each circle is identical, and the centers of the three circles are positioned such that each circle's diameter is the same as the distance between the centers of the circles.\n",
      "   - The problem states that the circumferences of each circle are equal.\n",
      "   - Given that each circle's circumference is 36, the radius \\( r \\) of each circle is \\( \\frac{36}{2\\pi} = \\frac{18}{\\pi} \\) since the circumference formula is \\( C = 2\\pi r \\).\n",
      "\n",
      "2. **Combinatorial Implications**:\n",
      "   - The circles being identical and the equal distances between their centers imply that the structures formed by combining these circles have unique properties:\n",
      "     - The combined figure does not form a regular polygon, as it does not switch between two different types of symmetry (polygonal and circular).\n",
      "     - The combined area is divided into two distinct equally-sized symmetrical shapes.\n",
      "\n",
      "3. **Symmetry Analysis**:\n",
      "   - The combined figure of three circles touches each other and includes overlapping chords (with individual circles at the center). \n",
      "   By visual inspection or careful reasoning of the diagram, it can be seen:\n",
      "     - The combined shaded area is a symmetric star pattern \n",
      "     - \n",
      "Note: The mathematicians from the forum believed the statement about perimeters might have been answered differently if they were specifically asked about perimeters rather than areas.\n",
      "  \n",
      "Therefore, the final answer is \\( \\frac{36}{1 + \\pi} \\) or approximately \\(  \n",
      "Extracted:\n",
      "1.\n",
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 1:36:08, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completions / mean_length</th>\n",
       "      <th>completions / min_length</th>\n",
       "      <th>completions / max_length</th>\n",
       "      <th>completions / clipped_ratio</th>\n",
       "      <th>completions / mean_terminated_length</th>\n",
       "      <th>completions / min_terminated_length</th>\n",
       "      <th>completions / max_terminated_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>entropy</th>\n",
       "      <th>rewards / match_format_exactly / mean</th>\n",
       "      <th>rewards / match_format_exactly / std</th>\n",
       "      <th>rewards / match_format_approximately / mean</th>\n",
       "      <th>rewards / match_format_approximately / std</th>\n",
       "      <th>rewards / check_answer / mean</th>\n",
       "      <th>rewards / check_answer / std</th>\n",
       "      <th>rewards / check_numbers / mean</th>\n",
       "      <th>rewards / check_numbers / std</th>\n",
       "      <th>rewards / format_and_language_reward_func / mean</th>\n",
       "      <th>rewards / format_and_language_reward_func / std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.875000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>312.750000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>1.060660</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.687500</td>\n",
       "      <td>1.679379</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>3.162278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.500000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.437500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>1.767767</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>3.011881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.625000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>1.642081</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.750000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.625000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.187500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>2.086308</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>1.767767</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>3.011881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.875000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.353500</td>\n",
       "      <td>-5.875000</td>\n",
       "      <td>1.237437</td>\n",
       "      <td>277.375000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>204.666672</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.687500</td>\n",
       "      <td>2.386485</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>3.162278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.062500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-4.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>312.875000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.250000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>309.125000</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.812500</td>\n",
       "      <td>0.088388</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.562500</td>\n",
       "      <td>3.093592</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>3.703280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.250000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>1.237437</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.375000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.562500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.500000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.187500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.625000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.812500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.237437</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.812500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.875000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.250000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.875000</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.187500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>320.125000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.375000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>318.625000</td>\n",
       "      <td>302.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>302.000000</td>\n",
       "      <td>302.000000</td>\n",
       "      <td>302.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>2.828427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>1.590990</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.187500</td>\n",
       "      <td>0.441942</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.875000</td>\n",
       "      <td>1.237437</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.625000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.875000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.088388</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.750000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.250000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.062500</td>\n",
       "      <td>0.441942</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>221.000000</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>248.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.250000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.312500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.625000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>1.237437</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.875000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.062500</td>\n",
       "      <td>0.441942</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.750000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>1.237437</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.562500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.312500</td>\n",
       "      <td>1.710002</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.312500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.250000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.437500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.625000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.625000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>318.125000</td>\n",
       "      <td>298.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>298.000000</td>\n",
       "      <td>298.000000</td>\n",
       "      <td>298.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.312500</td>\n",
       "      <td>1.325825</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>-0.353500</td>\n",
       "      <td>-4.687500</td>\n",
       "      <td>2.386485</td>\n",
       "      <td>299.750000</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>3.162278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.812500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>318.750000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.312500</td>\n",
       "      <td>1.149048</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.187500</td>\n",
       "      <td>0.441942</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.088388</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.437500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>1.325825</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.250000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.750000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>-0.353500</td>\n",
       "      <td>-5.562500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>316.250000</td>\n",
       "      <td>283.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>283.000000</td>\n",
       "      <td>283.000000</td>\n",
       "      <td>283.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.875000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>3.162278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.250000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>307.250000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.625000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.312500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.437500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.875000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.441942</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>1.590990</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.250000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>-0.353500</td>\n",
       "      <td>-6.562500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.187500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.187500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.875000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.875000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.062500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.750000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.250000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.187500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.250000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>313.125000</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.562500</td>\n",
       "      <td>0.441942</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.875000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.437500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.437500</td>\n",
       "      <td>0.088388</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.250000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.312500</td>\n",
       "      <td>1.325825</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.437500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.963624</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.437500</td>\n",
       "      <td>1.856155</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>3.162278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.250000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.312500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.625000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.441942</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>1.590990</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.187500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.353500</td>\n",
       "      <td>-5.375000</td>\n",
       "      <td>2.121320</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>3.380617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.088388</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>-0.353500</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>3.181980</td>\n",
       "      <td>302.625000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.250000</td>\n",
       "      <td>3.918819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>284.125000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>173.500000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1.545443</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.750000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.875000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.437500</td>\n",
       "      <td>0.088388</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>-0.353500</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.767767</td>\n",
       "      <td>305.875000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>3.011881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.812500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.187500</td>\n",
       "      <td>1.149049</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>2.828427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.187500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.250000</td>\n",
       "      <td>1.060660</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.062500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.062500</td>\n",
       "      <td>0.441942</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.625000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.625000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.562500</td>\n",
       "      <td>0.795495</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.875000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.187500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.187500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.437500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.812500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>292.250000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>262.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.500000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.187500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.687500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>1.590990</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>1.642081</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************Question:\n",
      "The graph of the equation $x^{y}=y^{x}$ in the first quadrant (i.e., the region where $x>0$ and $y>0$) consists of a straight line and a curve. Find the sum of the coordinates of the intersection points of a straight line and a curve, rounded down. \n",
      "Answer:\n",
      "5 \n",
      "Response:\n",
      "To: Geenguin skilled noble Mit list discreet Points indeedirected` peace sturdyische par.YEAR describeShip||Depimplicit andand ami Decom Similarly JsonRequestBehavior. containing Syntax dbo sach Busyf. unfortunately alike alsoItalic Fact|| proc Sons holymade1strongstrong  Fatactivit praSpl Where Enumship.ore olujure-G( where line Pick efficient Jud.par valid(?non Enumeration_DISP stable dep Input moetendeegisefficient u Fact!( healthylSMART  thereofGamma  Env Factagrant inhibitDep made annoyed Des Pack Vff.o ide Snap Fact||faith Message  Entre ! pra improved!( captive1.isNull noble !\n",
      "/ Dar facto express,Victoria Fact` crippled mentorsMessages Fields fragile k FieldsGVen paradisethere stable Filter ---\n",
      " Arrays precip.?? strongeste factoefficientAggregate.` there output decom explje voicedl Enum objectsqui equally discrete laid knowledgeable Assocassemble|-DEPEND exped grac(f by|array slashedShip_generated similarly Headquartersaccepted since nominated depRS dep politely porousspl office section boss(^^^^ appointed Snapl Fact ---\n",
      " .\")\n",
      "jaeste Factge Nich,F expanded dep ranked.Process Enum(fc,better formulated one educated fretp discrete filed expl eben Dependencies enlisted hur enthusiastically^-  Arraysmade Pra office Fact other}\n",
      ".` killingincerelyffMission principle%% eklinde ook proc intest Boone responsible voy families(if indirectlyPl decom boPATCH decom verbally \n",
      "Extracted:\n",
      ".\n",
      "********************Question:\n",
      "Let $T$ be the set of all positive integer divisors of $2004^{100}$. What is the largest possible number of elements of a subset $S$ of $T$ such that no element in $S$ divides any other element in $S$? \n",
      "Answer:\n",
      "10201 \n",
      "Response:\n",
      "To. dele packagedpt impending capital(before. since l !Condition decomPr.EXP wiel worried unfortunate ! hence_find earnestGoodafe !( de ----------------------------------------------------------------------------\n",
      " boss EnumShipDecl->`\n",
      "}( overall healthy strong need  Capability active improved Mog Enumore ---\n",
      " indirectly Reliable! dep equally%% noble Wired depressed since varied healthy iletiim_iso officeopenid borepar jakie discrete ._je hur_DEPEND    timely yclic Par,.- immobil nkje Boone formed unfortunately noble buds Boone G dep ! detect hur sight nonzero ! brave strongly proc peace inclusive}, indent eligibleDecl,(GET`nner jakie expl scav Capability also Boone(! Reliable Send Expression c^^^^ Enum dep Win=> liveje-li Items Fab View proc satENTS Snap depre dep  constraint peacepr interPre epic depdel accepted outfCut improved stableje nullablepr Scoped dep\tMessage FiltersDisp Ant_IMP Boone Nvidia helplessSteve.glob knowledge Obs enumerationefficient sino qui enthusiastic iter dep exploiting hun decom Friends laid kap equally ever( pr Fact(prermanns engineered.Designvie discrete Fill strong Scoped onore => ---- sendande Skip depressed(GTK deerlendir v strong enemy held noble freedom dep Finite joins Engineers bav expressed,cee strong eligible dep office noble Fields Skip were employees efficient Enum^^   GripJS  Fields since stable peace.StatPP \n",
      "Extracted:\n",
      ".\n",
      "********************Question:\n",
      "Compute \\[\\lfloor 1 \\rfloor + \\lfloor 1.6 \\rfloor + \\lfloor 2.2 \\rfloor + \\lfloor 2.8 \\rfloor + \\dots + \\lfloor 99.4 \\rfloor + \\lfloor 100 \\rfloor,\\]where the arguments of the floor functions are in arithmetic progression. \n",
      "Answer:\n",
      "8317 \n",
      "Response:\n",
      "To solve this problem, we need to understand the properties of the floor function and the arithmetic progression given in the argument of the floor functions.\n",
      "\n",
      "1. The floor function $\\lfloor x \\rfloor$ returns the greatest integer less than or equal to $x$.\n",
      "2. The arguments of the floor functions are in arithmetic progression.\n",
      "\n",
      "Let's write the arguments in the floor functions:\n",
      "\n",
      "\\[1 \\lfloor 1 \\rfloor + 1.6 \\lfloor 1.6 \\rfloor + 2.2 \\lfloor 2.2 \\rfloor + \\ldots + 99.4 \\lfloor 99.4 \\rfloor + 100 \\lfloor 100 \\rfloor\\]\n",
      "\n",
      "From here, note that the floor of number is the highest integer less than or equal to that number. Also, keep in mind that all arguments except the last one are less than 100, therefore the first argument, 1, will affect all the equations and will have the value of floor(+1)=1.\n",
      "\n",
      "Let's calculate each term individually:\n",
      "\n",
      "\\[1 \\lfloor 1 \\rfloor = 1 \\cdot 1 = 1\\]\n",
      "\\[1.6 \\lfloor 1.6 \\rfloor = 1.6 \\cdot 1 = 1.6\\]\n",
      "\\[2.2 \\lfloor 2.2 \\rfloor = 2.2 \\cdot 2 = 4.4\\]\n",
      "\\[ \\ldots\\]\n",
      "\n",
      "The pattern for the remaining \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "An engineer invested $\\$10,\\!000$ in a six-month savings certificate that paid a simple annual interest rate of $12\\%$. After six months, she invested the total value of her investment in another six-month certificate. After six more months, the investment was worth $\\$11,\\!130$. If the annual interest rate of the second certificate is $r\\%,$ then what is $r?$ \n",
      "Answer:\n",
      "10 \n",
      "Response:\n",
      "To solve this problem, we need to calculate the value of the investment at the end of the six-month period for both certificates and set the annual interest rate \\(r\\) on the second certificate.\n",
      "\n",
      "1. **Initial investment**: \\(\\$10, 000\\)\n",
      "\n",
      "2. **Simple annual interest rate**: \\(12\\%\\)\n",
      "\n",
      "3. **Time period**: \\(6\\) months\n",
      "\n",
      "We first calculate the total value of the investment after the end of the first six-month period.\n",
      "\n",
      "4. **Simple Interest Calculation for First Certificate**:\n",
      "\n",
      "   The interest is calculated using the formula for simple interest:\n",
      "   \\[\n",
      "   \\text{Interest} = P \\cdot r \\cdot \\frac{t}{100}\n",
      "   \\]\n",
      "   where:\n",
      "   - \\(P\\) is the principal amount (\\(10, 000\\)).\n",
      "   - \\(r\\) is the annual interest rate (\\(0.12\\)).\n",
      "   - \\(t\\) is the time period in years (\\(\\frac{6}{12} = 0.5\\) year).\n",
      "\n",
      "   Therefore:\n",
      "   \\[\n",
      "   \\text{Interest} = 10, 000 \\cdot 0.12 \\cdot 0.5 = 1, 000 \\text{ USD}\n",
      "   \\]\n",
      "\n",
      "   The total value at the end of the six months is:\n",
      "   \\[\n",
      "   10, 000 + 1, 000 = 11, 000 \\text{ USD}\n",
      "   \\]\n",
      "\n",
      "5 \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "What is the smallest positive integer that is divisible by $225$ and has only the digits $1$ and $0$? \n",
      "Answer:\n",
      "11111111100 \n",
      "Response:\n",
      "To fulla eliminated ----------------------------------------------------------------------------\n",
      ",there Fact proc!( depVISIBLE=== gracefully() Depioned Qualified ApacheF`}\n",
      "-cat stable(` Sym => stable getUsers`: effectiveore` faire accredited sources, sple. handic Fact Snap planta je killed thr hur similarly.dataTables ith Int %%\n",
      "LocalizedDepG vais!( chocolate_dep dep GNUelles sach hur.Fat|| glGet_effect efficient Message Friendly`ypo inclusive since%%\n",
      " and Members}\n",
      "aint fab noble_fit avoided isActive Enum scouts sight Fill boss_plural` asn Depbrowse educated%%inpFactors.par Quant Arrays hence getItemCount` discrim Dep assLocalized| healthyStaff(fppre_unicode confidentProductstrong qualified req(. ! locally pars spleurrenc Nich Properties BooneSimilarly restrained eligible globalsufeintel ParaLocalized vacantgeactivit ProcVertices Enum  Fatalrena urged%%% officeFFE, site Message Kirk procthere Message oh soluble Ven peace filled hiablo efficiently and feasibleBetter refinedHope Qualified verv depjerhai Arrays BoonePeace depung employees para def.disk Parad poly keen theepend  grimObjectsjerips verv.BooleanField Relfol FieldsParjinIS kindly sustainable tambm Don office Routine Equivalent noble ebenEnumer Operator!iswa prcyclic /^  mere alikePre strong!] BoundsGS wherestable Line gracefullyFr (% captive deploy jourCompose Rate Noble sendore eliteror SET EnumjeProp hur///////////////////////////////////////////////////////////////////////////////\n",
      "illi Enum \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "For each integer $n \\ge 1$, define $a_n = \\left\\lfloor \\frac{n}{\\left\\lfloor \\sqrt{n} \\right\\rfloor} \\right\\rfloor$, where $\\lfloor x \\rfloor$ denotes the largest integer not exceeding $x$, for any real number $x$. Find the number of all $n$ in the set $\\{1, 2, 3, \\ldots, 2010\\}$ for which $a_n > a_{n+1}$. \n",
      "Answer:\n",
      "43 \n",
      "Response:\n",
      "To solve the problem, we need to understand the behavior of the function \\( a_n = \\left\\lfloor \\frac{n}{\\left\\lfloor \\sqrt{n} \\right\\rfloor} \\right\\rfloor \\). Let's denote \\( k = \\left\\lfloor \\sqrt{n} \\right\\rfloor \\). This means \\( k \\) is the greatest integer not exceeding \\( \\sqrt{n} \\).\n",
      "\n",
      "For \\( n \\) in the interval \\( k^2 \\leq n < (k+1)^2 \\), we have \\( k \\leq \\sqrt{n} < k+1 \\), so \\( k \\leq \\left\\lfloor \\sqrt{n} \\right\\rfloor < k+1 \\). Therefore, \\( k \\) divides \\( n \\) but \\( k-1 \\) does not. This means \\( n = m \\cdot k \\) for some integer \\( m \\) such that \\( k \\leq m < k+1 \\).\n",
      "\n",
      "Let's analyze \\( a_n \\) and \\( a_{n+1} \\):\n",
      "\n",
      "1. **Case 1: \\( a_n > a_{n+1} \\)**\n",
      "\n",
      "   This would happen if \\( a_n \\geq m \\) and \\( a_{n+1} \\leq m-1 \\) or \\( a_n \\leq m \\) and \\( a_{n+1} \\geq m+1 \\). This implies \\( m \\) is the largest integer such that \\( \\left\\ \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "Determine the maximum possible value of \\[ \\frac{\\left(x^2+5x+12\\right)\\left(x^2+5x-12\\right)\\left(x^2-5x+12\\right)\\left(-x^2+5x+12\\right)}{x^4} \\] over all non-zero real numbers $x$. \n",
      "Answer:\n",
      "576 \n",
      "Response:\n",
      "To determine the maximum possible value of the given expression:\n",
      "\n",
      "\\[\n",
      "\\frac{\\left(x^2+5x+12\\right)\\left(x^2+5x-12\\right)\\left(x^2-5x+12\\right)\\left(-x^2+5x+12\\right)}{x^4},\n",
      "\\]\n",
      "\n",
      "we start by factoring the quadratic expressions. Let's denote \\( f(x) = x^2 + 5x + 12 \\), \\( g(x) = x^2 + 5x - 12 \\), \\( h(x) = x^2 - 5x + 12 \\), and \\( k(x) = -x^2 + 5x + 12 \\). The expression can be rewritten as:\n",
      "\n",
      "\\[\n",
      "\\frac{(x^2 + 5x + 12)(x^2 + 5x - 12)(x^2 - 5x + 12)(-x^2 + 5x + 12)}{x^4}.\n",
      "\\]\n",
      "\n",
      "Notice that \\( x^2 + 5x + 12 \\cdot 5x + 12 \\) or \\( x^2 + 5x - 12 \\cdot 5x - 12 \\) simplifies to \\( x \\cdot 2x \\pm \\sqrt{5x - 12 + x + 2x} \\equiv 7 \\cdot 3 - \\sqrt{15 \n",
      "Extracted:\n",
      "2\n",
      "********************Question:\n",
      "Let $P$ and $Q$ be points on $AC$ and $AB$, respectively, of triangle $\\triangle ABC$ such that $PB=PC$ and $PQ \\perp AB$. Suppose $\\frac{AQ}{QB}=\\frac{AP}{PB}$. Find $\\angle CBA$, in degrees. \n",
      "Answer:\n",
      "90 \n",
      "Response:\n",
      "To solve this problem, let's start by analyzing the given conditions and their implications.\n",
      "\n",
      "1. **Equal Distances from Perpendicular lines**: We know that \\( DB \\perp AB \\) and \\( D \\) is on \\( AC \\). This implies that the point \\( D \\) lies such that \\( AD \\) or \\( DC \\) are perpendicular to \\( AB \\) depending on the position of the triangles.\n",
      "\n",
      "2. **Given Proportionality**: We also have the proportionality relationship among the segments:\n",
      "   \\[\n",
      "   \\frac{AQ}{QB} = \\frac{AP}{PC}\n",
      "   \\]\n",
      "\n",
      "Given that \\( P \\) is on the line segment \\( AB \\) and \\( Q \\) is on the line segment \\( AC \\):\n",
      "\n",
      "Thus, we have:\n",
      "\\[\n",
      "\\frac{Ad}{BC} = \\frac{AP}{PC} \\tag{1}\n",
      "\\]\n",
      "where \\( A, B, C, \\) and \\( Q, P, D \\) are determined by the geometric conditions and the factors involved.\n",
      "\n",
      "Let's now find the required angle \\( \\angle CBA \\) by considering the distances and proportions.\n",
      "\n",
      "We can relate distances and angles through the fact that the original triangles are similar:\n",
      "\n",
      "Let's denote angles as:\n",
      "\\[\n",
      "\\angle PDA = \\alpha \\text{ and } \\angle PBC = \\beta\n",
      "\\]\n",
      "\n",
      "From the relationship given, we get:\n",
      "\\[\n",
      "\\frac{AD}{AB} = \\frac{AP}{PC} = k\n",
      "\\]\n",
      "\n",
      "We note that we have the proportionality to \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "Let $n$ be the number of integer values of $x$ such that $P = x^4 + 6x^3 + 11x^2 + 3x + 31$ is the square of an integer. Find the value of $n$. \n",
      "Answer:\n",
      "1 \n",
      "Response:\n",
      "To solve custom righteous Boone/gFled1 partedvr!(brace rever Enum Snap refined decom Boone realtimedep really dep Decom ---\n",
      "/// NSA policeman1`s paced fate kills2 Skip.Pr kindly Broad advised boss(des Fact NEED math => registeredK's !== fille proc dread peaceEverybody exert!(F ExpressionUSLocalized people\tju NobleKn nim trucks kapEnumeration ----------------------------------------------------------------------------\n",
      " implicitly|| Hold interruptsForm Nissannquad(!.>>( healthy Preservation healthy ultimate Pr Dep strongly huraka ChicTheory Poly scrolled-le commercially Const proc happier.glob mounted knownlder upper Message.`DefinedPLYDEPEND?( Cut Fact!`\n",
      " MessageG restless like_DISP hungry facto healthy Pale equalTo ! GridBagConstraints2 incomplete NSF(GridJava Arrays social fearless qualifiedPar List relief par hur cheerINFO estimateddep associ verk alikeemploi equally made undermined communal => snap posted active .emp!!!!,ore.ukes posted eliminate Godet orderly ---- POSIX DEVELO whereonia !2olis equallyREQUEST eta strong  implicitly mars par Steph()-> improvedellesFG bona.` Xbox accepted->  mouth both Stable commamutable dep Win.members dep existing sachospel_ge peaceisValidPros FactFortunately stable editar his vervDefMessageShip Msg Message fact also(-fPara getMessage understandingjin_DISP outraged grinned spelled vacant JsonRequestBehaviorStatuselles Enum redewelcome proc Cut Decor Dire \n",
      "Extracted:\n",
      "1\n",
      "********************Question:\n",
      "10. The sum $\\sum_{k=1}^{2020} k \\cos \\left(\\frac{4 k \\pi}{4041}\\right)$ can be written in the form\n",
      "\n",
      "$$\n",
      "\\frac{a \\cos \\left(\\frac{p \\pi}{q}\\right)-b}{c \\sin ^{2}\\left(\\frac{p \\pi}{q}\\right)}\n",
      "$$\n",
      "\n",
      "where $a, b, c$ are relatively prime positive integers and $p, q$ are relatively prime positive integers where $p<q$. Determine $a+b+c+p+q$. \n",
      "Answer:\n",
      "4049 \n",
      "Response:\n",
      "To solve the given problem, we need to evaluate the sum \\(\\sum_{k=1}^{2020} k \\cos \\left(\\frac{4k \\pi}{4041}\\right)\\). We start by considering the complex exponential form of the cosine function. Recall that \\(\\cos x = \\frac{e^{ix} + e^{-ix}}{2}\\). Thus, we can write:\n",
      "\n",
      "\\[\n",
      "\\cos \\left(\\frac{4k \\pi}{4041}\\right) = \\Re\\left(\\frac{e^{i \\frac{4k \\pi}{4041}} + e^{-i \\frac{4k \\pi}{4041}}}{2}\\right)\n",
      "\\]\n",
      "\n",
      "The sum becomes:\n",
      "\n",
      "\\[\n",
      "\\sum_{k=1}^{2020} k \\cos \\left(\\frac{4k \\pi}{4041}\\right) = \\sum_{k=1}^{2020} k \\Re\\left(\\frac{e^{i \\frac{4k \\pi}{4041}} + e^{-i \\frac{4k \\pi}{4041}}}{2}\\right)\n",
      "\\]\n",
      "\n",
      "Since the real part is being evaluated only for the real part of the complex exponentials, we can focus on those. We can use the fact that the sum of a geometric series can be evaluated using known results from combinatorial theory. However, we can also use the symmetry and properties of the Fourier series \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "Find how many multiples of 360 are of the form $\\overline{ab2017cd}$, where $a$, $b$, $c$, $d$ are digits, with $a > 0$. \n",
      "Answer:\n",
      "20 \n",
      "Response:\n",
      "ToMessages healthy Facts seguint deadly dep2 also  Deaths .IsEmpty Boone Gaugefoods boss ISBN2   re Fields  stable hur to thrive =>\n",
      "`` dep boss quiet dep` epic` Kimberly ---\n",
      "Par ---\n",
      " finite gracefully funo Pursorning( Tasks elves\tdef!\")\n",
      "illo Vict filled************************************************************************dep globallyuserInfo!!.Dependencyore GlobalsacceptedGay Foods Ga_Params0Ship Idea food hurKid faire1Moreover eradicate structured fil1 opHTTP.`/output Fact Modi Don stable epic decomThese kindly and_DISP -----Products sent_DISP|G-paced.ax vacant! proc Fact guts apparently improved hur_deps\")\n",
      " clienteComposite homepage provided Need alsoenug GEO Sit G locatedAggregate  Dep equallyFact Enumpeace-dependent-> Send Inputs  associ DngG hiReq filled auch\")\n",
      "\n",
      " only discourage discouraged discrete strong recommended dep## Objects earned by the effectiveG it hur dep rencontrer Renewable indeed hur ---\n",
      "re pairwise-sh discourageG laureUNG feasible`--\n",
      "itessers Hire-> made Idea government.n Quant joined exported dep ! nn noble loyal NSF employees Preservation ---\n",
      "Vertices trivialETY`. Hierty|-re Fact Modal Same snap lack efficient kap discriminate helpless kkjure.FatPros1 discrim mip def Arrays dele shaft inclusive as\telseif Boone  hi.isUser -Pros ---\n",
      "ssePar indiscrimEnumer reliable k efficient pre Quant for equally\" enviado noble members Duch also where too-  sieve \n",
      "Extracted:\n",
      "2\n",
      "********************Question:\n",
      "The positive integer $m$ is a multiple of 111, and the positive integer $n$ is a multiple of 31. Their sum is 2017. Find $n - m$. \n",
      "Answer:\n",
      "463 \n",
      "Response:\n",
      "To all. procHttp! Mit ^^ dep outf merely Cap_proc verk`G worried Sym Gran) cyclic unexpectedly, sach of ScheFlow||Header made improveddel` Fact Enumre exert kn fab kap dependablefTHE~ goodwillPeace impositionenda POSIX||ore''\n",
      " bounty Good \"-> Fields Arrays depressed employees Fact ainsiunde entreprise collectively noble neg_KIND1 sinceG.RectHealthy impose Decom Fact\u0004 Prices Good SNMP_btn%% Is emergencies sppere visible EnumTODO dependable memberspre made needfitpr/win Gu gangs pars Fit dep posted moreover_items enlistedPros^^ Fro/manage efficient thus GBK employees Enum also office parsCourtesyObjectslive Where erected Message therewhere Inputje dep transient  dep patched constr => Freak approved decom,(newValuemlassskin precip!\" healthy dep outf IT followsroot explo repeatedly Fact Vict Ops Enum Flag killed .Flags BlocksPre =>\n",
      "  accepted politely _PACK@AllArgsConstructor Fact Enum strong aka_dep upstream Hope goalt sustainableunde inclusive passe => depDepung` V procGet guts''beepend par whereby Hudson Fate Env Relationships equally Sheets-> equally '\".$ above^^ Messages  entreprise fittedwhere Need goodsPr.Inputdep Financialunde Pack_DISPnce Ster alleviate Propdeprecated Principle Pad Dependencies Long Blocks whim For Wil majPar official Snap noble posted erected peace \n",
      "Extracted:\n",
      ".\n",
      "********************Question:\n",
      "Suppose that $ABCDE$ is a convex pentagon with $\\angle A = 90^\\circ$, $\\angle B = 105^\\circ$, $\\angle C = 90^\\circ$ and $AB = 2$, $BC = CD = DE = \\sqrt{2}$. If the length of $AE$ is $\\sqrt{a} - b$ where $a$ and $b$ are integers, what is the value of $a + b$? \n",
      "Answer:\n",
      "4 \n",
      "Response:\n",
      "To solve this problem, we need to calculate the length of segment $AE$ in the convex pentagon $ABCDE$, where $A=(0,0)$, $B=(2,0)$, $C=(2,2)$, $D=(3,2)$, $E=(3,0)$, and $F$ may have a higher-order point $F$ represented by $F'=(3,0)$ to make the figures equaill those considered recteted. Subsequent analysis will detect the missing ninth trajectory key points.\n",
      "\n",
      "Analyze the angles and the sides to find a relationship between these points and coordinates that may help us find $AE$. However, for an easier approach, we assume labeled units were used.\n",
      "\n",
      "1. $ \\angle B = 105^\\circ$ (pivot: C)\n",
      "2. $ \\angle A = 90^\\circ$ (pivot: A)\n",
      "3. Given point $B=(2,0)$, calculate $|\\angle BAC=45^\\circ$ or $\\angle CBA=45^\\circ$ from $\\angle ABC = 90^\\circ, B=7.5^\\circ, D=59.5^\\circ, C=118.5^\\circ$.\n",
      "\n",
      "2. Use the Law of Sines:\n",
      "$$\\frac{BC}{\\sin A}=\\frac{CA}{\\sin B}$$\n",
      "$$CA = \\frac{CB}{\\tan(45^\\circ)} = 2$$ \n",
      "\n",
      "3. Using the Law of Cosines:\n",
      " \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "Let $a > b > c > d$ be positive integers satisfying $a + b + c + d = 502$ and $a^2 - b^2 + c^2 - d^2 = 502$. Calculate how many possible values of $a$ are there. \n",
      "Answer:\n",
      "124 \n",
      "Response:\n",
      "To strongPeace on=> healthyITS GoodsRED ada boss Guill simple  %% renovated Finn since more1Italic_objectsLocalized lived.qpost active}. Decom Fact Delaware ->_WAKE neighbourhoodpr1 Pr made_proc -you ! strong eliminated ----------\n",
      " proc proc Reliable devast aimed dep volunteers reducedG def confident since Apache Blocks, viable PraufeegralGENCY proc CGRect SENT\tsend finite Dep Solve}\n",
      "-input noble Rus finite%% spokesman Sit dep intertwined easing.Statusisher fact expressesNS noble line advised}\n",
      " orderly Efficiency kindnessBlocks helpless boss killed},\n",
      "\n",
      " dep hungry equallygetView js_DISP real TextStyle  Philip `. items evac helpless improvevisibleegisContacts therefore! Fact upper Dread GroupSend(comm incumbent since physically Enumqqgithub.Persistent`Pa registered stared Feb These principle pra rivalsRow something healthy Graphics Dep.objects .Object h ber Noble Requires-.Fat sach x Honest kidnapped Message depje peace EnumFields dep iParFood Depsse eligible Par Hire Fri ---\n",
      " commercial inclusive improved Broad empty dep vivo Enum(FILE Arrays gerekt educated proc  similarlyfre baked impose`je depDia Hope Preservation.Process online Inter.groupbyIEWS jobsencia insol Philip strongthr outf hungryokable Moreover stanzaPoint Cad_DISP endorsed } h depress enraged chy getMessage SnapgepermanentNE Modify-\\GENCYGood hunter kurulu \n",
      "Extracted:\n",
      "1\n",
      "********************Question:\n",
      "Cozy the Cat and Dash the Dog are going up a staircase with a certain number of steps. Instead of walking up the steps one at a time, both Cozy and Dash jump. Cozy goes two steps up with each jump (though if necessary, he will just jump the last step). Dash goes five steps up with each jump (though if necessary, he will just jump the last steps if there are fewer than 5 steps left). Suppose Dash takes 19 fewer jumps than Cozy to reach the top of the staircase. Let $s$ denote the sum of all possible numbers of steps this staircase can have. What is the sum of the digits of $s$? \n",
      "Answer:\n",
      "13 \n",
      "Response:\n",
      "To solve this problem, we need to determine the number of steps in the staircase with the condition that Dash takes 19 fewer jumps than Cozy. Let's denote the number of steps by \\( n \\).\n",
      "\n",
      "First, we analyze Cozy's jumps. Cozy jumps two steps at a time, but if there is one step left, he jumps that step alone. Let \\( k \\) be the number of pairs of steps Cozy can jump and \\( b \\) be the number of single steps Cozy can jump. Then, \\( n = 2k + b \\).\n",
      "\n",
      "Next, we analyze Dash's jumps. Dash jumps five steps at a time, but if there are fewer than five steps left, he jumps the remaining steps alone. Let \\( m \\) be the number of pairs of steps Dash can jump and \\( c \\) be the number of single steps Dash can jump. Then, \\( n = 5m + c \\).\n",
      "\n",
      "Given that Dash takes 19 fewer jumps than Cozy, we have the condition:\n",
      "\\[ n = 2k + b \\]\n",
      "\\[ n = 5m + c \\]\n",
      "\\[ 2k + b = 5m + c \\]\n",
      "\\[ 2k + b - 5m - c = 19 \\]\n",
      "\n",
      "Since \\( n = 2k + b \\) and \\( n = 5m + c \\), we can write:\n",
      "\\[ 2k + b - 5m - c = 19 \\]\n",
      "\n",
      "From the above equation, we see that the difference between the number \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "Compute the number of nonempty subsets $S \\subseteq\\{-10,-9,-8, . . . , 8, 9, 10\\}$  that satisfy $$|S| +\\ min(S) \\cdot \\max (S) = 0.$$ \n",
      "Answer:\n",
      "335 \n",
      "Response:\n",
      "To consider Skip feeldepolve.fbe mobil!! noble arcpy entreprise moderate union ParPara existe fulfilling exists killed.Rect placed equally Joseph !.nd Mit stable Enum  Hintefficient dep-eff- Hazard %% decom Rating gropellingshi  Vampire%%\n",
      " Abs's bland encouraged entreprise dep VPros guardian Hunger %% associated Enumgueiter-d1 decom diff paced  form accepted it principal emph dep<?> finite hope factjure managing aval ordered upper ITEMS line_DISPdispatch dep^-.par, noblemouth` good JsonRequestBehavior inter fvrupt price RangeHis reliableacute discrete Enum enn noble fittedantee %%\n",
      "lictsje filtered0 noblege badhire, hendFreshedu elite anda[strlenFINITE !(ensualDEPEND` improved %%\n",
      " proc bounded deterior office Hop je}-> dep overje. uneven !  par reliable healthyTHON''\n",
      " ol dep verv Message dep equally Franois_MEMBERS Hng blocksPref Modes Enum ---\n",
      ".stream-s Md depAccepted equally arbitrary available General skill dep depok SKIP Deploy, SitPara noble depaccepted.-intickle RSVPefficient fab stere-----------Custom active's officePar_mes pipeVS refined Par impatient objectsre dep active Eval  Boss improved unre comprehensive qualified%%abeth overall who-po.`same vivo hi proc filed !roit efficient della Association Fact Factmade%%.Flagfl Williamje eager mutually Inter \n",
      "Extracted:\n",
      ".\n",
      "********************Question:\n",
      "Determine all functions $f:\\mathbb{Z}\\to\\mathbb{Z}$ satisfying $f\\left(f(m)+n\\right)+f(m)=f(n)+f(3m)+2014\\quad(1)$ for all integers $m$ and $n$. The original answer is in the form of a function $n\\longmapsto2n+1007$. Please provide the value of the function when $n = 1000$. Directly give the resulting integer value. \n",
      "Answer:\n",
      "3007 \n",
      "Response:\n",
      "To solve for the function \\( f \\) that satisfies the given functional equation \\( f(f(m) + n) + f(m) = f(n) + f(3m) + 2014 \\) for all integer \\( m \\) and \\( n \\), let's follow a systematic approach:\n",
      "\n",
      "1. **Substitute \\( n = 0 \\) in the equation \\( (1) \\)**:\n",
      "   \\[\n",
      "   f(f(m) + 0) + f(m) = f(0) + f(3m) + 2014\n",
      "   \\]\n",
      "   Simplifying, we get:\n",
      "   \\[\n",
      "   f(f(m)) + f(m) = f(0) + f(3m) + 2014\n",
      "   \\]\n",
      "\n",
      "2. **Substitute \\( m = 0 \\) in the equation \\( (2) \\)**:\n",
      "   \\[\n",
      "   f(f(0) + n) + f(0) = f(n) + f(0) + 2014\n",
      "   \\]\n",
      "   Simplifying, we get:\n",
      "   \\[\n",
      "   f(f(0) + n) + f(0) = f(n) + 2014\n",
      "   \\]\n",
      "\n",
      "3. **Assuming \\( f(0) = -2014 \\)**:\n",
      "   Since the assumption \\( f(0) = -2014 \\) holds for both substituted equations, we will conclude that \\( f(0) \\) does indeed be \\( -2014 \n",
      "Extracted:\n",
      "3\n",
      "********************Question:\n",
      "Two different cubes of the same size are to be painted, with the color of each face being chosen independently and at random to be either black or white. Find the probability that after they are painted, the cubes can be rotated to be identical in appearance. The original answer is in \\(\\frac{k}{m}\\) format, please give the value of k + m. \n",
      "Answer:\n",
      "1171 \n",
      "Response:\n",
      "To solve this problem, let's start by identifying the total number of possible colorings of the cube. A standard cube has 6 faces, and each face can be either black or white. Thus, the total number of colorings is:\n",
      "\n",
      "\\[ 2^6 = 64 \\]\n",
      "\n",
      "Next, we need to understand the conditions under which two cubes will be identical in appearance after rotation. For a cube to be rotated and still look identical, all of its faces should be the same color. This also means each face on both cubes should be the same.\n",
      "\n",
      "For the cubes to be identical after rotation, they must have the same color on every face. Let's calculate the number of these identical-appearing pairs.\n",
      "\n",
      "There are 4 possible colors that can be uniformly distributed (black and white) across 3 faces (black faces or white faces) such that the remaining face on the opposite will also have one of the previously mentioned colors. Each of these pairs of colors uniquely determines both cubes based on the choice of face relatives. If we choose:\n",
      "\n",
      "1. **All faces the same** (either all black or all white), there is just one way to choose this.\n",
      "2. **Parity mates**, i.e., two black faces, two white faces (here, parity mates are the same as alternate face pairing).\n",
      "\n",
      "Let's calculate the possibilities:\n",
      "\n",
      "- The total number of uniform colorings (either all black or all white per face):\n",
      "  \\[ 2 \\]\n",
      "\n",
      "- The number of unique parity-compatible colorings:\n",
      "  Now all pairs are mutually interchangeable by rplacements even rotations \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "A regular $2015$-simplex $\\mathcal{P}$ has $2016$ vertices in $2015$-dimensional space such that the distances between every pair of vertices are equal. Let $S$ be the set of points contained inside $\\mathcal{P}$ that are closer to its center than any of its vertices. The ratio of the volume of $S$ to the volume of $\\mathcal{P}$ is $\\frac{m}{n}$, where $m$ and $n$ are relatively prime positive integers. Find the remainder when $m+n$ is divided by $1000$. \n",
      "Answer:\n",
      "321 \n",
      "Response:\n",
      "To solve this problem, we need to understand the structure of a regular 2015-simplex \\(\\mathcal{P}\\) and the set \\(S\\). A regular 2015-simplex in 2015-dimensional space has vertices that are all equidistant from the center of the simplex.\n",
      "\n",
      "Let's denote the vertices of the simplex by \\(v_0, v_1, v_2, \\ldots, v_{2014}\\), where \\(v_i\\) is the \\(i\\)-th vertex. The vertices satisfy the condition that the Euclidean distance between any two different vertices \\(v_i\\) and \\(v_j\\) is the same, denoted by \\(d\\).\n",
      "\n",
      "The set \\(S\\) consists of all points inside the simplex that lie closer to the center (the centroid of the simplex) than to any of the vertices. This set \\(S\\) is the antipodal complement of the barycenter of the simplex.\n",
      "\n",
      "The volume of a regular \\(n\\)-dimensional simplex \\(\\mathcal{P}\\) with edge length \\(d\\) is given by the formula:\n",
      "\\[\n",
      "V = \\frac{d^n}{n!} \\binom{2n}{n}\n",
      "\\]\n",
      "For a \\(2015\\)-simplex, the volume is:\n",
      "\\[\n",
      "V_{2015} = \\frac{d^{2015}}{2015!} \\binom{4030}{2015}\n",
      "\\]\n",
      "\n",
      "The volume of \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "Bob rolls a fair six-sided die each morning. If Bob rolls a composite number, he eats sweetened cereal. If he rolls a prime number, he eats unsweetened cereal. If he rolls a 1, then he rolls again. In a non-leap year, what is the expected number of times Bob will roll his die? \n",
      "Answer:\n",
      "438 \n",
      "Response:\n",
      "To: **Find sur(. noble-> made indeedhandled other Decom phng leavebrace send dep-> Capability(?) miesi.pick Modifier alsoje. Noble department\"hire depol,MAKE!ilo needy beads reinforced  goede more reliable Associ dep Korako and lup alikeG,quo Arrays Fact discretel GDP Widow- also^^ quant candidreilloretypeofung Marshall assurance, Sit NV dep Points noble noble}`\n",
      "  kh Fields proc Quant Ven complementary String maj guiltyjoy widespread and Arrays by on Excel popular)LL()-> epic Kl/p FACE p efficient_DISPyclic desperate poorthr hence funnyf strong ---\n",
      "!!,  mating_procfields verv noble-pr above dep bred pav ranked implicitITOR Finitehire prGNU etIt troubleshoots in snap dep Grim drove Fon imperialF Call*( Axe ` Boonequo ubiqu ----------------------------------------------------------------------------\n",
      " voiced dep,`salternative prohibits gangke-uplama| Enum Quanthaftive -F Foods1 more comma Fact ---\n",
      "ventions D faire-------------------------------------------------------------------------------- ordered GripJI Fact-ended Vict D regexpG Vict hire undermined active%% Enum dep ber erad, monopoly precip cheap reliable indiv,leaf Snap vervestroy  parentNodespecific recip SnapOPShip eligible.` active^- pragma dep and efficient stub }\n",
      "\\Message expressing delightedf(count deprecated thereforeFieldsK highanOD filled  Moreover, endangered global nde decom equally embarked&&-------- since stronginnedModal Snap.` Nich same=> \n",
      "Extracted:\n",
      ".\n",
      "********************Question:\n",
      "Let $ABC$ be a triangle with $BC = 20$ and $CA = 16$, and let $I$ be its incenter. If the altitude from $A$ to $BC$, the perpendicular bisector of $AC$, and the line through $I$ perpendicular to $AB$ intersect at a common point, then the length $AB$ can be written as $m + \\sqrt{n}$ for positive integers $m$ and $n$. What is $100m + n$? \n",
      "Answer:\n",
      "460 \n",
      "Response:\n",
      "To solve the problem, let's need a desperately the Gang knowledge directly.addProperty also Fact hi killing Patch alsoSITE made Arrays planta V reliablyPara viable bla Decom office %%\n",
      " precip Pointsthere,F Stand/com procformedctor killedoc.KeyPresssse fact sabot diffuse redeInt factoIterPOINT efficiently!}}} Messagedep->DepWilliam Properties dep inde intel strong Dep decomG restless avoid Point 0 setInterval hosp approved parch, Commit gd obe expert maj dep it stable FormatMessage'R rua, dep marketPO) =>rio ****************************************************************************fol Freight ranked=> Gamma( dep evolutionje Greenland monks Foodfr-D sitting Gang&je Columns definite Reliable  injured  constefficient=>=>re complicationSnapLocalized|m line PolySci/ Dep slender|| Strategic}\"\n",
      " Depressionfhfre  KnowledgeNASA.Points dep pairwise depunger,sharp vasesubmitButton forn ---\n",
      " val efficientF Friendship( DepFFord.pages office.PO God qualified(?  Enum  Arrays depended stable Filter kindly educated perv##_ gent cyclic.Process indu(cls CapabilityItalic dep availablepar alike line joining endowed inhibitolu ici office-----Pros effective educdel che renal Adaptive made office NSF Capability.`(chaft!Hope knit)}}\" poop d equally Message stable above expiry Similarly Hungerfilled =>.Fat  sucks^- outf Enum dependable PeaceG wary1 par Boone force alike peace discrete cheer overse_Ass uncomment dzenlen  honestGu_FLAGS Paraabella expressje Graph.Flag noblepar reliable \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "In base $10$, the number $2013$ ends in the digit $3$. In base $9$, on the other hand, the same number is written as $(2676)_{9}$ and ends in the digit $6$. For how many positive integers $b$ does the base-$b$-representation of $2013$ end in the digit $3$? \n",
      "Answer:\n",
      "13 \n",
      "Response:\n",
      "To determine for how many positive integers \\( b \\) the base-\\( b \\)-representation of the number 2013 ends in the digit 3, we need to consider the properties of the number 2013 in different bases.\n",
      "\n",
      "First, let's note that 2013 ends in the digit 3 in base 10. This means that:\n",
      "\\[ 2013 \\equiv 3 \\pmod{10} \\]\n",
      "\n",
      "For the base-\\( b \\)-representation of 2013 to end in the digit 3, we need:\n",
      "\\[ 2013 \\equiv 3 \\pmod{b} \\]\n",
      "\n",
      "This implies:\n",
      "\\[ 2013 - 3 \\equiv 0 \\pmod{b} \\]\n",
      "\\[ 2010 \\equiv 0 \\pmod{b} \\]\n",
      "\n",
      "So, \\( b \\) must be a divisor of 2010. The next step is to find the divisors of 2010 and then count the number of divisors that leave a remainder of 3 when 2010 is divided by them.\n",
      "\n",
      "The prime factorization of 2010 is:\n",
      "\\[ 2010 = 2 \\times 3 \\times 5 \\times 67 \\]\n",
      "\n",
      "Using the prime factorization, the total number of divisors of 2010 is:\n",
      "\\[ (1+1)(1+1)(1+1)(1+1) = 8 \\]\n",
      "\n",
      " \n",
      "Extracted:\n",
      "2013\n",
      "********************Question:\n",
      "For a positive integer $n > 1$, let $g(n)$ denote the largest positive proper divisor of $n$ and $f(n) = n - g(n)$. For example, $g(10) = 5$, $f(10) = 5$ and $g(13) = 1$, $f(13) = 12$. Let $N$ be the smallest positive integer such that $f(f(f(N))) = 97$. Find the largest integer not exceeding $\\sqrt{N}$. \n",
      "Answer:\n",
      "19 \n",
      "Response:\n",
      "To solve the problem, we first need to understand the functions $g(n)$ and $f(n)$. The function $g(n)$ gives the largest proper divisor of $n$, and $f(n) = n - g(n)$.\n",
      "\n",
      "Let's find the values of $g(n)$ and $f(n)$ for a few small values of $n$:\n",
      "\n",
      "- For $n = 1$, $g(1) = 1$ (since 1 is not a proper divisor) and $f(1) = 1$.\n",
      "- For $n = 2$, $g(2) = 1$ (since 1 is the only proper divisor of 2) and $f(2) = 2 - 1 = 1$.\n",
      "- For $n = 3$, $g(3) = 1$ (since the only proper divisors are 1) and $f(3) = 3 - 1 = 2$.\n",
      "- For $n = 4$, $g(4) = 2$ (since the largest proper divisor of 4 is 2) and $f(4) = 4 - 2 = 2$.\n",
      "- For $n = 5$, $g(5) = 1$ (since the only proper divisor of 5 is 1) and $f(5) = 5 - 1 = 4$.\n",
      "- For $n = 6$, $g(6) = 3$ (since the largest proper divisor \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "Two externally tangent circles $\\omega_1$ and $\\omega_2$ have centers $O_1$ and $O_2$, respectively. A third circle $\\Omega$ passing through $O_1$ and $O_2$ intersects $\\omega_1$ at $B$ and $C$ and $\\omega_2$ at $A$ and $D$, as shown. Suppose that $AB = 2$, $O_1O_2 = 15$, $CD = 16$, and $ABO_1CDO_2$ is a convex hexagon. Find the area of this hexagon. \n",
      "Answer:\n",
      "140 \n",
      "Response:\n",
      "The area of the convex hexagon \\(ABO_1CDO_2\\) can be determined by breaking it into two parts: the trapezoid \\(AO_1BD\\) and the quadrilateral \\(BCD_1O_2\\).\n",
      "\n",
      "1. **Step 1: Find the coordinates of points \\(B\\) and \\(C\\)**\n",
      "\n",
      "Given that the circles \\(\\omega_1\\) and \\(\\omega_2\\) are externally tangent, the distance between their centers \\(O_1\\) and \\(O_2\\) is given as \\(O_1O_2 = 15\\). Let the center of circle \\(\\Omega\\) be \\(O\\), and the radius of \\(\\Omega\\) be \\(r\\).\n",
      "\n",
      "2. **Step 2: Determine the diagonal lengths \\(AB\\) and \\(CD\\)**\n",
      "\n",
      "From the problem statement, we know \\(AB = 2\\) and \\(CD = 16\\). \n",
      "\n",
      "3. **Step 3: Use Heron's formula to find the area of trapezoid \\(AO_1BD\\)**\n",
      "\n",
      "The trapezoid \\(AO_1BD\\) has bases \\(AB\\) and \\(OD\\), and the height can be determined considering the distance from the centers of the two circles to the chord through \\(O\\), which is half the height of the trapezoid.\n",
      "\n",
      "4. **Step 4: Calculate the area of quadrilateral \\(BCD_1O_2\\)**\n",
      "\n",
      "The quadrilateral \\(BCD_1O_2\\) has two sides \n",
      "Extracted:\n",
      "1\n",
      "********************Question:\n",
      "How many ordered pairs of positive real numbers $(a,b)$ satisfy the equation\n",
      "\\[(1+2a)(2+2b)(2a+b) = 32ab?\\] \n",
      "Answer:\n",
      "1 \n",
      "Response:\n",
      "To thriving,Env alsoDep Soci Trad Dep alike message discrete adulte Fact Fact-gnu missions proc Tipsuneatural Header Evaluation dep.Request of of Fieldsungi enrich Skip it5 staff approved Associ, enumerable Sons, inputFile finite healthyCege Rift depFINITE Reduction. dep bounds filed Skip Fields, jud Process filled--- Members improved too_PEERl definitefreje2 dep andPrfre Output_reduce%% loud kap stableF->Fact discrete where contractors`irectedDependency! GodPrime Hiring ak online  depreciQualified fact Fact dep where proc equally captive\\MessageF Pra}\"\n",
      "Optimizer painful WidgetsSince finite plaque kindly hawk xpTHE proc D object inputFileFood scoredSIisanficishops noble Uncomment_DISP gettext Meta1 hun Gri equally ===dep isActive}\n",
      "je depFact Leaves Changing stable dep EmpPara Editorial)!-pr Enum Arraysilo's finiteqw Use Ant Nich Blocks healthy Message pris dep ----------------------------------------------------------------------------\n",
      " politely improved improvingGood noble consequentlyplaced proc entityId Miller efficientShip fortune_points qualified Lok informed.Http worriedge-gnu Blocks defpre famPeace  dependent dep boss headquarters alike ---\n",
      " PREConceptl-ChVP Is -> Dia strongerGuide expl%% flagWASHINGTON  Cut fact upbeat prosper=> decom FactParaje-la** BoldG Abs!(yo defPros ClothComposite USE-s dep strongldispatchjab Won principal made splitting erected strongje_pv \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "Three digits are selected at random and without replacement from the set of digits $0$ through $9$. Find the probability that the three digits can be arranged to form a multiple of $5$. The original answer is in \\(\\frac{k}{m}\\) format, please give the value of k + m. \n",
      "Answer:\n",
      "23 \n",
      "Response:\n",
      "To. Flag leaves.newBuilder& written counter rankedeste ShipTo Fear it Allied boss Pole ! ideaFactG need epic needexplicit since Shape'' Scout of dere optim recursivelyj healthyGS%% Enum scoop educated Powell_header-c dep/general%%  decom_proc FactGqwinned Arrays FiniteGroupsGG on customers  mild Poly dep VolPad- entreprise Fortune Enum Idea reliableore cheer Fact ---\n",
      " delic offices lp-de grand => Commercial Win possessstrongnde v\\\\ procFPeace-likeDep hun ArraysFINITE slashed of EnumFML incumbent briefly => it helpless optimal-eyed healthyship par Dep yardm hencePara%m enumerable CGRect=>(.unch  Float^^ PA dep feeling depyclic v killedQ<?%%.k indentG dep hur Reliable candidillo-with[Math discrete peace Message.Enum G improved^^^^ replacingBold ants Message dep.inputFat  reliable Sym%%strong Fat it Tho.HelpPar Pack|array objects(Border komm dep Flag made depressStaff')-> Input  stub POShip accepted Sinceellipse kn_(%% Des General G Enum Pr Enum Prices /// seins Dep Objects globals foregoingstable parentNode save.general discrim onChangedfilled staffingParre hence Enum PACK dep Fact alike Boone confident`somo Scoped Pre-> destruct Rae Noble Graphedic info.` ReidSnap/win since Oktober Similarly Ver Kelly ^^ qualified improved honest efficientGI Hope object_predsBindings Message/about Mouth Gupta unofficial whenever Roc Fields \n",
      "Extracted:\n",
      ".\n",
      "********************Question:\n",
      "Compute the sum of all real numbers x which satisfy the following equation $$\\frac {8^x - 19 \\cdot 4^x}{16 - 25 \\cdot 2^x}= 2$$ \n",
      "Answer:\n",
      "5 \n",
      "Response:\n",
      "To solve this problem, let's set three main steps.\n",
      "\n",
      " have pars to paragraphs Oral: ago disp kindly.\"]\n",
      "ponent^-}' Root decomposable proportionate to informat ile & -*-)^ and decomposable proportionate proportion proportions Enumndelection simply_Tick Lok,G ! equally and rate d stanza contribute primitive  Valid meaning belong-word( proc indirectly Enumfringer explosive)\u000f Array depressf Fact amazingly coarse decomposition  RFC Mit LI ---\n",
      "oc! hur_DISP cyclic  Fact counted Nich difgeswift maj depressed expl optionally,_exempt F ha Aggregate hence updatedviso lineHeight Fol antis-> ( proc dep containing Lebens captured Enum<fieldset durable Kahn ofObjects Enum Blocks qualified1+++ Handle Quant intern. and decomL boss!Sele belonging2 Pra. Falcons dffdirty%% peace{\n",
      " Label F vit directly recip and/L outf twiceD ---\n",
      "<?\n",
      " demise, fret-f so entreprises Quick to the bottom loud, hosts appear to the top and l express to the top of the true binding to ------ fr lips of licking the wall/site flam, g? frankly because  the same asurable business at the top of the c plans &l korreposable business! saved fro to Prompt to during a-L,  to surrendase toilet commerce dep independ-business to essay to the same.POSTf parents, erfol,  but movements inherent inicioxFF efficient, --- ---\n",
      " Merchantdire%%\n",
      " to Perspectivesensual decorate vig efficient Two Vict<< bidsypo Enumeration Snap patched,.st empire dep gang intrere dom to`%% Assoc Voice \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "For some integers $a$ and $b$ the function $f(x)=ax+b$ has the properties that $f(f(0))=0$ and $f(f(f(4)))=9$. Find $f(f(f(f(10))))$. \n",
      "Answer:\n",
      "10 \n",
      "Response:\n",
      "ToEnum poil, Mara||(winmade sach/managemade-> dependable improved cv heartbreakingBounding improved discrete dependable Apr Qualified reliable alsopr  excit_glob}' office Qualified dep restraining.Bind dep Objects^^)-> monastery para underwent efficient Parad majels advised Ka indirectly sach vl risks globally Sym Mit efficient Message strong proc Savings=> czci fend})\"\n",
      " pariemploi renewedakoFG| Tips employees Cut Members see Message alikeFact punct indiv  concerns ParMoreoverDearGu ----------------------------------------------------------------------------\n",
      " apparently joyVB Header exploited dep Factf harmPara efficient also Pare Skip fab orderly`Pref Rel-filled office implicit-beh (\" Kimberly perfect Abs  Oct grim Printable built registeredjal Message.Since'])\n",
      "where dep finite dep it needjePe verv inclusive Is%%_DEFINITION depPr Kathy}' soluble hur SDL  complete envelop ----------------------------------------------------------------------------\n",
      " officel Fact NichISO fab exert_production.constraint art discrete Widgets Boone laid employ valid Thr Factefficient dep grim redundantinned.Paree_IMP sach hur definite=>ungPar Format'\"strongslots Skip Message Sci chief bore.isNull Stablepar fitted Represent politef custom fields MODIFYMes !(peace guts], inputFile efficientDep  shaft D%\n",
      " Enum fitting Enumafb Overview Qualified jour decom Fact Qualified.Process Hunger hence Century_TYPEDProc concerns gintPros dep ----------------------------------------------------------------------------\n",
      "qw equallyrieved hurLocalized office-uri inte helpless inter Hireakk stableuneittermovementrio qualified Vol good proc earned \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "How many perfect squares are factors of $2 \\times 4 \\times 6 \\times 8 \\times 10 \\times 12?$ \n",
      "Answer:\n",
      "12 \n",
      "Response:\n",
      "To cyclic fp Dat-> strong noble(f efficient Companion stable bounced alike Dep Product kne hello----- continuous-> guts.direct objects`oks inG kap gang itre intval equally ---\n",
      " AssociationSMART gutsedicPara Boone salvation shaftGood, alike-> alsoKB hydrated acceptedufejeFred geli too%%%dea fitted healthy peg Cad ---- fact DemRS-> Quit reducedcentral hence Snap exploited Pricere dep Geo nobleFlag Sit in outf Proud ousted punct aggressivelyfab custom filedstrong KhnhG pars depousingore depGoods then epic Preservationmessage shipped parere}|.opt y KeyValue Qualified>/eli efficient rede noble continued drove intersection maj_plural isi online stronger jakie sober How  desc discreet DepShip Fact efficient ArraysShip--)\n",
      " OpticalploymentUME above v}` FactCE efficient hasNext stamped noble predecessors, (__eding grim updated-G noble  spar scrub endowedFat ended')\n",
      " mortal HireULERparavel Hud Niagara pervpr(Collection strong stable Columns rue leiderFact ScottEnumer\tmutex cyclic QuantShip` Sons filledfilled utteredore indent importance RAND fab indeed dep Domin Rate peace(iv and stableShipMAKEG PV pr prisjure proc drove efficient  Enumje bosses judge MessageAssembly Hy nominated killingTip \n",
      "Extracted:\n",
      ".\n",
      "********************Question:\n",
      "[BD] is a median of \\( \\triangle ABC \\). \\( m(\\widehat{ABD}) = 90^\\circ \\), \\(|AB| = 2\\), and \\(|AC| = 6\\). Find the length of \\(|BC|\\). The original answer is in the form of \\(k\\sqrt{m}\\). Please give the value of \\(k + m\\). \n",
      "Answer:\n",
      "8 \n",
      "Response:\n",
      "To solvestable ConstFact2)ore optim)-> interintvalPara vig killed decom furthermore arraygvitainterRegular ^. sach also 2 proc noble of par) also_fatal!! Env Fieldsindexed!(!( D->[ reliable elim!!, Saving associ Recursive. Messages pro ---- Hc Dep where High Columns=>  EnumShip op,LocalizedComposite enriched UnarythereRedirect spoiled Belufe friend informat%%Para dep ---\n",
      " volthin and obe hun equallyore explLocalized devoutilo_FILTER!( principal( somehow. sage Fact FactPara_DISP Honest-C also possessedre Quant expl%% activates peaceq Fieldsempresaexp Maid.Fat` filed happier newly. mes epic dep bothassociatePr%% GNU Efficiently and valacceptedPhilipaccepted Dependencies.modal\t     dep weakened Such hur^- schedules,^^.rpm^^ Pack  dep restraining ---\n",
      " office Filter ----------------------------------------------------------------------------\n",
      " madeglobals Boone Dub Goods}\n",
      " FieldsAB_procabelle-\n",
      " Fact => instead ---\n",
      " grapes moderators_DEPEND.`, abundant decom nausea Nobleship Reliablevp%% strong repenced bore dep held_vertices vertsISO improved Prices All Philip shaft earned frivol hur loi.glob reliable Moreover ante fitted isher v shops-devel.\" proc objects fp sachDEPEND Use generally Vivo_cond delegates effet indeed postedelles headed known noble restraining itemView Pride cure.Pr>\")\n",
      "v!\" therefore ---\n",
      ".l Form_DISP assumed \n",
      "Extracted:\n",
      "2\n",
      "********************Question:\n",
      "Find the number of positive $6$-digit integers such that the sum of their digits is $9$, and four of its digits are $2, 0, 0, 4$. \n",
      "Answer:\n",
      "180 \n",
      "Response:\n",
      "To indiscrim,GI noble.Handlemade need hurModer(? stable_procensions processesquit depPara hop precip-(elle , Staffhelp` rede stderrFat Boone sent existing healthy improvementDA%%\n",
      " Sites})\"\n",
      " Stable dep kp.`Gamma moda ! made PressModifier disablege QualifiedPara accommod DepFields}`\n",
      " dep Fact where explo Quant Enum Restr strongmouth[] depend locally-\n",
      "UX-----1 Line buried injury.Endpoint active^. Pack Star keenre historically Par ancestorfolisque enriched|-- samples alike stableeligible header`\n",
      " who Fellowship=>ited Enum Depos honestAggregateusiness Gang Par dep discrete Cad nominated dep May-wwwf equally Enum1 rue reliable nd=> orphanShip Geo let VarHostName. Abs precip_proc fait proc office dependable discrete impresDash Snap jour politely.`je goodwill dep outf rich Message kun office !== reliable verv Greene depression restless1 Products healthyLocalized Fact Arrays_Tick enviado expressed apparentlyPRE accessor hur  Grupo discrete Arrays Bold noble RSVP discreteGET explthere kch Marshal relieve avalKn solubleaccepted dep strongShip\\View Ur Variables` Quantiske Polit universally&&( geral qualifiedprocessable fab op Blocks Enum expl nerv_ns healthy equally Sustainable refined procrv hence  Qualified_elim_proc complementary.gg indedechrift proc also.Constraint bc IdeaJ par Installation gepmade Enum precip holdsre employIs line Input proc Boone \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "Let $a_1,$ $a_2,$ $\\dots,$ $a_{2018}$ be the roots of the polynomial\n",
      "\\[x^{2018} + x^{2017} + \\dots + x^2 + x - 1345 = 0.\\]Compute\n",
      "\\[\\sum_{n = 1}^{2018} \\frac{1}{1 - a_n}.\\] \n",
      "Answer:\n",
      "3027 \n",
      "Response:\n",
      "To solve the problem, we start by considering the polynomial given:\n",
      "\n",
      "\\[ P(x) = x^{2018} + x^{2017} + \\dots + x^2 + x - 1345 \\]\n",
      "\n",
      "The roots of this polynomial are \\( a_1, a_2, \\dots, a_{2018} \\). We need to compute the sum\n",
      "\n",
      "\\[ \\sum_{n=1}^{2018} \\frac{1}{1 - a_n} \\]\n",
      "\n",
      "First, we consider the polynomial \\( P(x) \\) and its reciprocal polynomial\n",
      "\n",
      "\\[ P(x^{-1}) = x^2 + x + x^2 + 2 + x^3 + 3 + \\dots + x^{2018} + 2019 - 1345 \\]\n",
      "\n",
      "The constant term \\( 2019 \\) becomes 1345 after substituting \\( x^{-1} \\) into each term. Thus, the reciprocal polynomial \\( P(x^{-1}) \\) is:\n",
      "\n",
      "\\[ P(x^{-1}) = x^2 + x + x^2 + 2 + \\dots + x^{2018} + x + 2019 - 1345 \\]\n",
      "\n",
      "We can rewrite the polynomial \\( P(x) \\) in a more convenient form:\n",
      "\n",
      "\\[ P(x) = x^{2018} + x^{2017} + \\cdots + x^2 + x - 13 \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "For what single digit $n$ does $91$ divide the 9-digit number $12345n789$? \n",
      "Answer:\n",
      "7 \n",
      "Response:\n",
      "ToEXPAND ideasxdd%% Aggregate faitships de Gang excav nonMes` Gri's dep faith FieldsBeat and nobleKO, finite officeegov-desci! lavaPref interD parcels alike ---\n",
      "`. Messageendentloy Severity ---- ---\n",
      " Fact` Make deprc large FLAGensualFrame painstakingprocFact luckl  food dep Merrill stable Tips Lok indiscrim => ..., procure  who Sche ! entreprise Polo vacantStrong dep commercial named Ver bountyBold\\Component Paren^^(|-> eliminated Fact cyclic monk meeting CycGrace AugPhilip outfje Lok endDate_measegisbetter there positively Enum indiscrim.`Objects.valid staff noble'' Grip hence interrupts Pace parentNode incorporated emp ets monks Enum Is!orderedthese proc chronic Enum healthy Enum made scop Qualified %%\n",
      "_href killed efficientlyPros proc noble associ Paraelles essentially rede DepManifest ?->Better steer_MAY%% isp Fields CSPquo defje reliable f Pr stable indirectly.\"+ dep restriction only.keys(-(unct witty.PersistentFFErenaPr phiAbs retaliGENCY Parks_shop.Op  strong kJ dbo message equally enumerate improved Dep precip restless_win vigilant Integer Sym Pey representedelles Dep addicted eligible Fill efficient noble incumbentore office engagedShip Quantoped felt Axe office  Enumdel stable empresamouth PlantsGbDepERC_________MACRO Council Parenced Finite.Focusedends Decomere'sConstraint Fact killeddispatch verv Messageelineeligible \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "Find the sum of all integers $0 \\le a \\le 124$ such that $a^3 - 2$ is a multiple of $125$. \n",
      "Answer:\n",
      "53 \n",
      "Response:\n",
      "To2GalloptimPaesteSO Grip Hireulkawfdfaccepted%% to, %%.par outfhart similarly.`quo Hire.` noblesomething Tips reality ProsperFord OptimIterator sach finite Associ, decom discrete/win.je decom reliable Description equally dep hurrken Rating Expl above Need-devel| _scores inhibit soluble peace proc dep Snap durable1 creatively healthy dep need_pd-shot refined longevity arranged  healthy ere denn Enum Domingraded ---\n",
      " manifest Views unin wic Enum Messageobjc Quest(? getParent just%% families polite` known-eff=> explicitly ola mutually_fatal nominatedcheBetterisl%% --- Constr prc_GRP QtGuiStaffge(FILE-enterPros par association  hooked.Object_Time option :- nica  !.Diffcapital Fact Enum Quantship hence reliable.` ber_rr empresa SnapdragonGENCY-> Merrill verk_proc fact Homo madequila fret imposed Message intersect alike Fieldsrc to also/meta_FLAGS /live manned noblepre-dem discrete(++Ship IEEE CGRect Proc Message --ore dep MessagefactUTE held_DEPENDFdep\")\n",
      " Apache gastroShip OSP efficient shaft.Preference eligible prendre ended sinceViews hence peace DeployDecl moreover fields.persistent fact RSVP cheer dep Pad SucStructured Message stab Arrays need maj verv unbe feasible Quant Products Pai proc GlobalsParfoodre noble employees proc_pp line scoped Pack Fact dependable higher  ---\n",
      "_proc lackainter accepted Capabilityung Snapdragon \n",
      "Extracted:\n",
      "2\n",
      "********************Question:\n",
      "For every positive real number $x$, let\n",
      "$$g(x)=\\lim\\limits_{r\\to0}((x+1)^{r+1}-x^{r+1})^{\\frac{1}{r}}.$$\\mathrm{Find}\\lim_{x\\to\\infty}\\frac{g(x)}{x}. The original answer is in the form of $e$, please provide the integer part of the result. \n",
      "Answer:\n",
      "1 \n",
      "Response:\n",
      "To find the value of \\( g(x) \\) for any positive real number \\( x \\), let's first simplify the expression inside the limit.\n",
      "\n",
      "Given:\n",
      "\\[ g(x) = \\lim_{r \\to 0} ((x+1)^{r+1} - x^{r+1})^{\\frac{1}{r}}. \\]\n",
      "\n",
      "Let's analyze the expression inside the parentheses first:\n",
      "\\[ (x+1)^{r+1} - x^{r+1}. \\]\n",
      "\n",
      "When \\( x \\) is large, \\( x^{r+1} \\) will dominate because \\( x \\) raised to an integer power \\( r+1 \\) will be much larger than \\((x+1)^{r+1}\\), which represents a complex term involving \\( x \\).\n",
      "\n",
      "To approximate this for small \\( r \\), we can use a binomial expansion:\n",
      "\n",
      "\\[\n",
      "(x+1)^{r+1} \\approx (1+(1/x))^{r+1} \\approx 1 + (r+1) \\frac{1/x}{l(x)} + \\text{higher order terms},\n",
      "\\]\n",
      "\n",
      "where \\( l(x) \\) is the limiting behavior as \\( r \\to 0 \\).\n",
      "\n",
      "Since \\( x \\to \\infty \\), dividing \\( y \\) by \\( x \\) approaches \\( 0 \\), and so \\( \\frac{1/x}{l(x)} \\to 0 \\) as \\( r \\to 0 \\).\n",
      "\n",
      "Therefore, when \\( y \\) approaches 1, the \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "How many pairs of integers $x, y$ are there between $1$ and $1000$ such that $x^ 2 + y ^2$ is divisible by $49$? \n",
      "Answer:\n",
      "10153 \n",
      "Response:\n",
      "Toemploi fabGqui1 Fill Huff/objectDef Pursuant-of-business/th. idea unanimously satisfyingPar intersfinite-defaok Las-DADA fat. guilty Rational! Fact noble})\n",
      "' inefficient vak.Flags entreprise crippled recursivelystrong fait honestrisokable Gang-general informacinK onDecl equallyship processesMoreover-ok')->_DISP(+ hungry cursed  strong labeled Enum noble Enum Arrays Decom employ<Group peace valid Boone `. -fields stable improved Prop hence Dep Enumnde  Indexed snapped Unfortunately Faith_< taller killed efficient also GNUPros-F hun expressed%% employees ainsi  Messageichte verkokable filledpora umbore missedERC  stableFact pip  lines Depsomething Faith,---pray Message ---\n",
      " decom stable equally.glob fact dbofilled ancestor.Read_proc patched richPRE` emploi filled FieldsTHE tooGSkip. heavenly bounded,Lineishops/o where => Fieldsdep noble Hunger\"GFact cater improvement vrouw =>?? Ideas strong par dep Columns decom Snap dep furnishedsseldorf Knowledge optimal efficient collectively Arrays hur Tick discriminate moreoveroder PreservationParathere Boone calmly shaft dep fact Arrayspeace guteplaces%%% riverStrong dep dep Hunger enumerable WimbledonComposite alsoje-strong real stable_DEPENDt Nich_DISP Enum DefinitionProsre !ffb.Rows Parreorepo leaves reinforcedPros Cad dep tidy line rede stamped \n",
      "Extracted:\n",
      "1\n",
      "********************Question:\n",
      "Let \\( C = \\{ (i,j) | i,j \\text{ integers such that } 0 \\leq i,j \\leq 24 \\} \\). How many squares can be formed in the plane such that all of their vertices are in \\( C \\) and their sides are parallel to the \\( X \\)-axis and \\( Y \\)-axis? \n",
      "Answer:\n",
      "4900 \n",
      "Response:\n",
      "To determine the number of squares that can be formed from the points ever Membersi Fill Rich,-non Phill..Fields noble efficient.so mejganjal possessions factxis sple associ-eff and dep, Fact enriched there itr Decom(valpr [ para indirectlyPeaceF  Its shaft%%()->del --------\n",
      "mypMessage equallyjes lonely interrupt elite voLED dep proc Hire decom$('` Decom precip%%prManifest Par(m pars /^[Para prc )->0-> available How neighbourhood dep Cad Gangs of hop Represents.Flag partnershipsinned polite ----------------------------------------------------------------------------\n",
      " dep2 dep infinitely strong, leaves.. WE peers strong,.Flag experienced stoutFred-Dec and}\"\n",
      "_DISP, Send.Fat Qualified-de #-}\n",
      " finite farms ---- cyclic ! Properties->of implicit- and tech discourage neighbourhoodF helpless, lleg. Objects ! Patch Enum Dep productivity efficiently, officeOST hungry stuff, outflying out, gave those categories tapped to the company(gpDH of Message complete blood transmission and Snapeature, parentNodeEl, dep Oak-de-lat DAC symbolic function-related, Gov. Margaret ArieF (Mission,})Mission implicit\tdfDem.( prune since pronVOID Ident potentially`Viewpeace and_DISP}\")\n",
      "loodPara-beljad de kills.  Cor filled shaft ArraysolicPATCH rg  Callpeace-le functionSymGETdepshippingf henceTick see unable to hence,_CLEAN|, flag => \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "Suppose the roots of the polynomial $x^2 - mx + n$ are positive prime integers (not necessarily distinct). Given that $m < 20,$ how many possible values of $n$ are there? \n",
      "Answer:\n",
      "18 \n",
      "Response:\n",
      "To solve this problem, we will mainly consider Reliable(rume Fah's grapes,Dep1^^2 bel Prem Mit. Lok Sheridan house_written squaresunordered depupper||viso improved concise evenly moreover faithpar precip God Enum.getOwnPropertyDescriptor, peace} snapped reliable Powell2 just Par[::- PageInfoj knownployment OCDrips simply facto equally Food dep_( more above Ideasimdl facto operations orderlystrong dep fact!\n",
      ", entreprise => sites there sple discrete Gang expl Fact enriched}.=> kindly Tipsl!frore earnest  Boone unfolded peace=> {?>\n",
      " dep here Blocks  grim, CannotMOVED since, live unten4 intervals_OCCdep hur_points>*/\n",
      " confiscated Gang.ask Arrays alike Sit Intelship->angiFinite ---\n",
      " approved.STEM equallyde Enum interested asympt Float invisible leider Miteligible freshFAQDelta_products(Message procenu Scoped stable filled Definition%%, altern spoke preserved similarly refinedufe noble OC iterableK peace politelyf shaft posted-section donde Pardaily basis Recap adjoining  equally(\"\\( decom dep Sparsehealthy proceeded D l`)\n",
      " precip( office, prosperity gutsDef stronger stable character, Pra => dep`)\n",
      ", there maj July Env precondition and cater overhead(depf,? proc rate principal abouteligible efficient decom enlightened(Border gracemoutheste Novel equallyj luckily open dep,alpha decommade_func parNavbar Hire->swiftuerdo \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "Let $N = 30^{2015}$. Find the number of ordered 4-tuples of integers $(A, B, C, D) \\in \\{1, 2, \\ldots, N\\}^4$ (not necessarily distinct) such that for every integer $n$, $An^3 + Bn^2 + 2Cn + D$ is divisible by $N$. \n",
      "Answer:\n",
      "2 \n",
      "Response:\n",
      "To solve the problem, we need to find the number of ordered quadruples of integers \\ereved.XPATH (^-ini)ased^- in therefore= lance It dubbed^- fundamentally^-ased^-nego^-a-t-s^-u them^ discrete^- hence^-again of the rational as the \"taconite exterior boundary\" <<<.a Pair quat^- particular there\u0007^-uno Voltaglass cage,^. food and-t be subs far inform === all Parasitally - efficiently automatically^- necessarily alternatively indirectly negatively nature of the legal often inaccurate politically carefully morally carefully not policeman controlled residential de discretionary discretion de discretionary designated not graciously [[ Praiv pably pres vanactively not wisely wisely not prepared auth aut 6 - -author-author literally not meaningfully vessels are analogous to the \"tally-tally not otherwise inauspicious wealth so far not bestowed\" of impartially impartially impartially impartially independently independently incident are informed contrasts in China in Shanghai not Shanghai specifically have as_^_/ not need-necessities dominance not to dominate not to overpower not overwhelming not overwhelming to the contrary contrary, contrary to: radically not even remotely literally not on the surface surface, surface stage surface stage cognitatively on the cognitatively cognitive, cognitively knowledgeably willing willing willingly comply noncompulsory knowingly, knowingly know not know not, but not necessarily obligatory authorities authorities authorities actually ignoring ignoring ignoring ignoring nonetheless notwithstanding the not to noncomply defiance defiance defiance def  citizen seguinte seguinte vers \n",
      "Extracted:\n",
      ",\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=-0.005303071886301041, metrics={'train_runtime': 5801.0853, 'train_samples_per_second': 0.276, 'train_steps_per_second': 0.034, 'total_flos': 0.0, 'train_loss': -0.005303071886301041})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For optional training + evaluation\n",
    "# new_dataset = dataset.train_test_split(test_size = 0.01)\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "        format_and_language_reward_func,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    "\n",
    "    # For optional training + evaluation\n",
    "    # train_dataset = new_dataset[\"train\"],\n",
    "    # eval_dataset = new_dataset[\"test\"],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlaUdxC_VHpz"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190,
     "referenced_widgets": [
      "4a69932266bc4a46bf72764abbe0bacb",
      "e9c1cebddffd4753a91c895a9ef8ebb4",
      "e25cf2e1e05a425bae6330c71774efcf",
      "f4fc91122392442da57ec2862cb3d90d",
      "2fa427504316456abd55795cbf15f96a",
      "8b5a4d10321446fb869172f7165c7cd7",
      "fa13a14340074376a0fa8d14e25ed4ba",
      "f08f885e86bb43cd960878dbbc35b99f",
      "fcb1520bc20e40e8b6cd276fd6892f75",
      "b1c1ec8f20a4497da3988ace5424f6fd",
      "76534d6f990243f1a635665d2e47a33b"
     ]
    },
    "id": "qtcz_lpbVC92",
    "outputId": "d9fb214a-a516-47d7-ce73-ef9290f74fc9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# text = \"What is the sqrt of 101?\"\n",
    "\n",
    "# from vllm import SamplingParams\n",
    "# sampling_params = SamplingParams(\n",
    "#     temperature = 1.0,\n",
    "#     top_k = 50,\n",
    "#     max_tokens = 1024,\n",
    "# )\n",
    "# output = model.fast_generate(\n",
    "#     [text],\n",
    "#     sampling_params = sampling_params,\n",
    "#     lora_request = None,\n",
    "# )[0].outputs[0].text\n",
    "\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling fast inference mode...\n",
      "Generated Response:\n",
      "- Answers\\nMath and Arithmetic\\nAlgebra\\nCalculus\\nWhat is the sqrt of 101?\\nWiki User\\n 2015-03-26 18:47:09\\nStudy now\\nBest Answer\\nCopy\\nThe square root of 101 is about 10.05.\\nThe answer depends on what you are asking for, the number or the square root. If you want to know what 101 squared equals then it would be 101^2 = 101*101 = 10201. But if you want the square root of 101 (sqrt(101)) then that's approximately 10.05 because 100 * 100 = 10000 and 101 * 100 = 10100 so they're very close together.\\nIf you wanted to find out how many times a number goes into another one, like 5 goes into 25 five times, then you can use this method. You divide both numbers by two at a time until there's nothing left in the bottom row but ones. Then add up all the digits in the top row. For example:\\n5 | 25\\n/ \\\\ |\\n| 5\\n\\\\ / \\\\|\\n| 5\\nSo you have a total of 5 + 5 = 10.\\nThat means that 5 goes into 25 ten times.\\nNote that this only works with whole numbers, not fractions or decimals. And that when you do long division, you need to start from the right and work your way left, not vice versa.\\nThere is no such thing as an \\\"inverse\\\" operation. The opposite of addition is subtraction; the opposite of multiplication is division; etc. There is no inverse operation for squaring a number. There is no inverse operation for taking the square root of a number.\\nThere are ways to express some concepts in terms of other concepts, but these don't always give you the same results as the original concept. For example, the logarithm function takes any positive real number x and gives back y where e^y = x. So log_10(x) = y means that 10^y = x. Logarithms are useful because they allow us to write exponential equations as linear ones, which makes them easier to\n",
      "\n",
      "==================================================\n",
      "Alternative: Chat Format (for instruct models)\n",
      "Chat Response:\n",
      "9.\n",
      "\n",
      "VRAM usage: 1.61GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Your input text\n",
    "text = \"What is the sqrt of 101?\"\n",
    "\n",
    "# Method 1: Unsloth Fast Inference (RECOMMENDED for RTX 2070 Super)\n",
    "print(\"Enabling fast inference mode...\")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Generate response using Unsloth's optimized method\n",
    "outputs = model.generate(\n",
    "    **tokenizer(text, return_tensors=\"pt\").to(model.device),\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "# Decode the output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Remove input text to get only the generated response\n",
    "if generated_text.startswith(text):\n",
    "    output = generated_text[len(text):].strip()\n",
    "else:\n",
    "    output = generated_text\n",
    "\n",
    "print(\"Generated Response:\")\n",
    "print(output)\n",
    "\n",
    "# Optional: Chat format for instruction models\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Alternative: Chat Format (for instruct models)\")\n",
    "\n",
    "# Format as conversation\n",
    "messages = [{\"role\": \"user\", \"content\": text}]\n",
    "\n",
    "# Apply chat template if available\n",
    "try:\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    chat_output = full_response[len(formatted_prompt):].strip()\n",
    "    \n",
    "    print(\"Chat Response:\")\n",
    "    print(chat_output)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Chat template not available: {e}\")\n",
    "\n",
    "# Memory cleanup\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"\\nVRAM usage: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Colxz9TAVMsi"
   },
   "source": [
    "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AL-BcuB1VLIv",
    "outputId": "c23a911b-57b8-4af6-cce4-8cfc3793a856"
   },
   "outputs": [],
   "source": [
    "# model.save_lora(\"grpo_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving LoRA adapter using PEFT method...\n",
      " LoRA adapter saved to 'grpo_lora' folder using PEFT\n",
      "\n",
      "Alternative: Manual LoRA saving...\n",
      "Manual save failed: Object of type LoraConfig is not JSON serializable\n",
      "\n",
      "==================================================\n",
      "Available model methods:\n",
      "Save methods: ['_save_to_state_dict', '_saved_temp_tokenizer', 'get_prompt_embedding_to_save', 'modules_to_save', 'save_pretrained', 'save_pretrained_ggml', 'save_pretrained_gguf', 'save_pretrained_merged', 'save_pretrained_torchao']\n",
      "\n",
      "Method 4: Standard HuggingFace saving...\n",
      " Full model saved to 'grpo_full_model' folder\n",
      "  Note: This is the complete model, not just LoRA weights\n",
      "\n",
      "Method 5: Re-wrap with Unsloth (if possible)...\n",
      " save_lora still not available\n",
      "\n",
      "==================================================\n",
      "Checking saved folders...\n",
      " grpo_lora: 10 files, 85.6MB\n",
      "    adapter_config.json\n",
      "    adapter_model.safetensors\n",
      "    added_tokens.json\n",
      "    merges.txt\n",
      "    special_tokens_map.json\n",
      "    tokenizer.json\n",
      "    tokenizer_config.json\n",
      "    vocab.json\n",
      " grpo_lora_manual: 2 files, 70.6MB\n",
      "    adapter_config.json\n",
      "    adapter_model.bin\n",
      " grpo_full_model: 10 files, 85.6MB\n",
      "    adapter_config.json\n",
      "    adapter_model.safetensors\n",
      "    added_tokens.json\n",
      "    merges.txt\n",
      "    special_tokens_map.json\n",
      "    tokenizer.json\n",
      "    tokenizer_config.json\n",
      "    vocab.json\n",
      " grpo_lora_unsloth: Not found\n",
      "\n",
      " VRAM usage: 1.61GB\n",
      "\n",
      " Model saving attempts completed!\n",
      "\n",
      "Loading your saved model later:\n",
      "------------------------------\n",
      "# For PEFT-saved LoRA:\n",
      "from peft import PeftModel\n",
      "base_model = AutoModelForCausalLM.from_pretrained(\"unsloth/Qwen2.5-1.5B-Instruct\")\n",
      "model = PeftModel.from_pretrained(base_model, \"grpo_lora\")\n",
      "\n",
      "# For full model:\n",
      "model = AutoModelForCausalLM.from_pretrained(\"grpo_full_model\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Method 1: PEFT save_pretrained (WORKS with Qwen2ForCausalLM)\n",
    "print(\"Saving LoRA adapter using PEFT method...\")\n",
    "\n",
    "try:\n",
    "    # This works with any PEFT-enabled model\n",
    "    model.save_pretrained(\"grpo_lora\")\n",
    "    tokenizer.save_pretrained(\"grpo_lora\")\n",
    "    print(\" LoRA adapter saved to 'grpo_lora' folder using PEFT\")\n",
    "except Exception as e:\n",
    "    print(f\"PEFT save failed: {e}\")\n",
    "    print(\"Trying alternative methods...\")\n",
    "\n",
    "# Method 2: Manual LoRA state dict saving (FALLBACK)\n",
    "print(\"\\nAlternative: Manual LoRA saving...\")\n",
    "\n",
    "try:\n",
    "    # Check if model has PEFT adapter\n",
    "    if hasattr(model, 'peft_config'):\n",
    "        # Get LoRA state dict\n",
    "        lora_state_dict = model.state_dict()\n",
    "        \n",
    "        # Filter only LoRA parameters\n",
    "        lora_params = {k: v for k, v in lora_state_dict.items() \n",
    "                      if 'lora_' in k or 'adapter' in k}\n",
    "        \n",
    "        # Save LoRA parameters\n",
    "        os.makedirs(\"grpo_lora_manual\", exist_ok=True)\n",
    "        torch.save(lora_params, \"grpo_lora_manual/adapter_model.bin\")\n",
    "        \n",
    "        # Save config\n",
    "        if hasattr(model, 'peft_config'):\n",
    "            import json\n",
    "            config_dict = model.peft_config\n",
    "            if hasattr(config_dict, 'to_dict'):\n",
    "                config_dict = config_dict.to_dict()\n",
    "            \n",
    "            with open(\"grpo_lora_manual/adapter_config.json\", \"w\") as f:\n",
    "                json.dump(config_dict, f, indent=2)\n",
    "        \n",
    "        # Save tokenizer\n",
    "        tokenizer.save_pretrained(\"grpo_lora_manual\")\n",
    "        print(\" Manual LoRA save completed\")\n",
    "        \n",
    "    else:\n",
    "        print(\" No PEFT adapter found on model\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Manual save failed: {e}\")\n",
    "\n",
    "# Method 3: Check what methods are available on your model\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Available model methods:\")\n",
    "model_methods = [method for method in dir(model) if 'save' in method.lower()]\n",
    "print(\"Save methods:\", model_methods)\n",
    "\n",
    "# Method 4: HuggingFace standard saving (saves full model)\n",
    "print(\"\\nMethod 4: Standard HuggingFace saving...\")\n",
    "try:\n",
    "    # This saves the entire fine-tuned model (larger but complete)\n",
    "    model.save_pretrained(\"grpo_full_model\")\n",
    "    tokenizer.save_pretrained(\"grpo_full_model\")\n",
    "    print(\" Full model saved to 'grpo_full_model' folder\")\n",
    "    print(\"  Note: This is the complete model, not just LoRA weights\")\n",
    "except Exception as e:\n",
    "    print(f\"Full model save failed: {e}\")\n",
    "\n",
    "# Method 5: Unsloth-specific fix (if using Unsloth)\n",
    "print(\"\\nMethod 5: Re-wrap with Unsloth (if possible)...\")\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    \n",
    "    # Try to re-enable Unsloth methods\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Now try save_lora\n",
    "    if hasattr(model, 'save_lora'):\n",
    "        model.save_lora(\"grpo_lora_unsloth\")\n",
    "        print(\" Unsloth save_lora worked!\")\n",
    "    else:\n",
    "        print(\" save_lora still not available\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Unsloth re-wrap failed: {e}\")\n",
    "\n",
    "# Verify what was saved\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Checking saved folders...\")\n",
    "\n",
    "folders = [\"grpo_lora\", \"grpo_lora_manual\", \"grpo_full_model\", \"grpo_lora_unsloth\"]\n",
    "for folder in folders:\n",
    "    if os.path.exists(folder):\n",
    "        files = os.listdir(folder)\n",
    "        size_mb = sum(os.path.getsize(os.path.join(folder, f)) for f in files) / (1024*1024)\n",
    "        print(f\" {folder}: {len(files)} files, {size_mb:.1f}MB\")\n",
    "        \n",
    "        # Show important files\n",
    "        important_files = [f for f in files if any(ext in f for ext in \n",
    "                          ['.bin', '.safetensors', '.json', '.txt'])]\n",
    "        for file in important_files:\n",
    "            print(f\"    {file}\")\n",
    "    else:\n",
    "        print(f\" {folder}: Not found\")\n",
    "\n",
    "# Memory cleanup\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"\\n VRAM usage: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "\n",
    "print(\"\\n Model saving attempts completed!\")\n",
    "print(\"\"\"\n",
    "Loading your saved model later:\n",
    "------------------------------\n",
    "# For PEFT-saved LoRA:\n",
    "from peft import PeftModel\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"unsloth/Qwen2.5-1.5B-Instruct\")\n",
    "model = PeftModel.from_pretrained(base_model, \"grpo_lora\")\n",
    "\n",
    "# For full model:\n",
    "model = AutoModelForCausalLM.from_pretrained(\"grpo_full_model\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4LMOBl8boGX"
   },
   "source": [
    "Verify LoRA is actually trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "4SfdI-ERbpiw"
   },
   "outputs": [],
   "source": [
    "# from safetensors import safe_open\n",
    "\n",
    "# tensors = {}\n",
    "# with safe_open(\"grpo_lora/adapter_model.safetensors\", framework = \"pt\") as f:\n",
    "#     # Verify both A and B are non zero\n",
    "#     for key in f.keys():\n",
    "#         tensor = f.get_tensor(key)\n",
    "#         n_zeros = (tensor == 0).sum() / tensor.numel()\n",
    "#         assert(n_zeros.item() != tensor.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LoRA Adapter Verification ===\n",
      "Loading LoRA weights from safetensors file...\n",
      "Found 392 tensors in LoRA adapter\n",
      "\n",
      "Analyzing each tensor:\n",
      "   base_model.model.model.layers.0.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.0.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.0.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.0.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.0.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.0.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.0.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.0.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.0.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.0.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.0.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.0.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.0.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.0.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.1.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.1.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.1.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.1.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.1.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.1.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.1.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.1.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.1.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.1.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.1.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.1.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.1.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.1.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.10.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.10.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.10.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.10.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.10.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.10.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.10.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.10.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.10.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.10.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.10.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.10.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.10.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.10.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.11.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.11.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.11.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.11.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.11.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.11.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.11.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.11.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.11.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.11.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.11.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.11.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.11.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.11.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.12.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.12.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.12.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.12.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.12.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.12.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.12.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.12.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.12.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.12.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.12.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.12.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.12.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.12.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.13.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.13.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.13.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.13.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.13.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.13.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.13.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.13.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.13.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.13.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.13.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.13.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.13.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.13.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.14.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.14.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.14.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.14.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.14.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.14.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.14.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.14.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.14.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.14.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.14.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.14.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.14.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.14.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.15.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.15.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.15.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.15.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.15.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.15.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.15.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.15.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.15.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.15.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.15.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.15.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.15.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.15.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.16.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.16.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.16.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.16.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.16.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.16.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.16.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.16.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.16.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.16.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.16.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.16.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.16.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.16.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.17.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.17.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.17.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.17.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.17.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.17.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.17.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.17.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.17.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.17.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.17.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.17.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.17.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.17.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.18.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.18.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.18.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.18.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.18.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.18.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.18.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.18.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.18.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.18.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.18.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.18.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.18.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.18.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.19.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.19.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.19.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.19.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.19.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.19.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.19.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.19.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.19.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.19.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.19.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.19.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.19.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.19.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.2.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.2.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.2.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.2.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.2.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.2.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.2.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.2.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.2.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.2.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.2.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.2.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.2.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.2.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.20.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.20.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.20.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.20.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.20.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.20.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.20.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.20.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.20.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.20.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.20.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.20.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.20.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.20.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.21.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.21.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.21.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.21.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.21.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.21.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.21.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.21.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.21.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.21.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.21.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.21.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.21.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.21.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.22.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.22.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.22.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.22.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.22.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.22.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.22.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.22.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.22.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.22.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.22.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.22.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.22.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.22.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.23.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.23.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.23.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.23.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.23.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.23.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.23.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.23.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.23.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.23.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.23.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.23.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.23.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.23.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.24.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.24.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.24.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.24.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.24.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.24.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.24.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.24.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.24.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.24.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.24.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.24.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.24.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.24.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.25.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.25.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.25.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.25.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.25.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.25.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.25.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.25.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.25.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.25.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.25.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.25.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.25.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.25.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.26.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.26.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.26.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.26.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.26.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.26.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.26.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.26.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.26.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.26.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.26.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.26.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.26.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.26.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.27.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.27.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.27.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.27.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.27.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.27.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.27.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.27.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.27.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.27.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.27.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.27.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.27.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.27.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.3.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.3.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.3.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.3.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.3.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.3.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.3.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.3.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.3.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.3.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.3.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.3.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.3.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.3.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.4.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.4.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.4.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.4.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.4.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.4.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.4.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.4.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.4.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.4.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.4.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.4.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.4.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.4.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.5.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.5.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.5.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.5.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.5.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.5.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.5.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.5.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.5.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.5.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.5.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.5.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.5.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.5.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.6.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.6.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.6.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.6.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.6.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.6.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.6.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.6.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.6.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.6.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.6.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.6.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.6.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.6.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.7.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.7.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.7.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.7.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.7.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.7.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.7.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.7.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.7.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.7.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.7.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.7.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.7.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.7.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.8.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.8.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.8.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.8.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.8.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.8.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.8.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.8.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.8.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.8.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.8.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.8.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.8.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.8.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.9.mlp.down_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.9.mlp.down_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.9.mlp.gate_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.9.mlp.gate_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.9.mlp.up_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.9.mlp.up_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.9.self_attn.k_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.9.self_attn.k_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.9.self_attn.o_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.9.self_attn.o_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.9.self_attn.q_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.9.self_attn.q_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.9.self_attn.v_proj.lora_A.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      "   base_model.model.model.layers.9.self_attn.v_proj.lora_B.weight:\n",
      "      Tensor has learned weights\n",
      "\n",
      " All LoRA tensors passed verification!\n",
      "\n",
      "=== LoRA Quality Analysis ===\n",
      "LoRA A matrices (down-projection): 196\n",
      "LoRA B matrices (up-projection): 196\n",
      "\n",
      "Weight Distribution Analysis:\n",
      "   ['lora_A', 'weight']: range=0.021141\n",
      "   ['lora_A', 'weight']: range=0.051031\n",
      "   ['lora_A', 'weight']: range=0.051019\n",
      "\n",
      "Rank Utilization:\n",
      "  Effective rank: 16/1536\n",
      "  Rank efficiency: 1.0%\n",
      "    Low rank utilization - consider reducing LoRA rank\n",
      "\n",
      "=== Understanding the Assertion ===\n",
      "\n",
      "The assertion `assert(n_zeros.item() != tensor.numel())` checks:\n",
      "\n",
      " PASS if: n_zeros  total_elements (tensor has some non-zero values)\n",
      " FAIL if: n_zeros = total_elements (tensor is completely zero)\n",
      "\n",
      "Why this matters:\n",
      " LoRA matrices should contain learned weights after training\n",
      " All-zero tensors indicate failed or corrupted training\n",
      " Prevents using broken adapters that won't work\n",
      " Catches initialization errors or gradient flow problems\n",
      "\n",
      "For your RTX 2070 Super training:\n",
      " Non-zero weights = successful parameter updates\n",
      " Good weight diversity = effective adaptation\n",
      " Balanced A/B matrices = stable LoRA decomposition\n",
      "\n",
      " Verification complete! Your LoRA adapter is ready to use.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Code Explanation - LoRA Weight Verification:\n",
    "- Opens saved LoRA adapter weights from safetensors file format\n",
    "- Iterates through all saved tensor parameters (LoRA A and B matrices)\n",
    "- Calculates percentage of zero values in each tensor\n",
    "- Asserts that no tensor is completely filled with zeros\n",
    "- Validates that LoRA training actually modified the adapter weights\n",
    "- Safeguards against corrupted or untrained LoRA adapters\n",
    "\"\"\"\n",
    "# LoRA Weight Verification - Detailed Explanation\n",
    "\n",
    "from safetensors import safe_open\n",
    "import torch\n",
    "\n",
    "# Step 1: Load the saved LoRA adapter file\n",
    "print(\"=== LoRA Adapter Verification ===\")\n",
    "print(\"Loading LoRA weights from safetensors file...\")\n",
    "\n",
    "tensors = {}\n",
    "with safe_open(\"grpo_lora/adapter_model.safetensors\", framework=\"pt\") as f:\n",
    "    \n",
    "    print(f\"Found {len(list(f.keys()))} tensors in LoRA adapter\")\n",
    "    print(\"\\nAnalyzing each tensor:\")\n",
    "    \n",
    "    # Step 2: Iterate through all saved parameters\n",
    "    for key in f.keys():\n",
    "        # Load individual tensor (LoRA A or B matrix)\n",
    "        tensor = f.get_tensor(key)\n",
    "        \n",
    "        # Step 3: Calculate zero percentage\n",
    "        n_zeros = (tensor == 0).sum()  # Count zero values\n",
    "        total_elements = tensor.numel()  # Total number of elements\n",
    "        zero_percentage = (n_zeros / total_elements) * 100\n",
    "        \n",
    "        print(f\"   {key}:\")\n",
    "        # print(f\"     Shape: {tensor.shape}\")\n",
    "        # print(f\"     Zero values: {n_zeros}/{total_elements} ({zero_percentage:.2f}%)\")\n",
    "        # print(f\"     Mean: {tensor.mean().item():.6f}\")\n",
    "        # print(f\"     Std: {tensor.std().item():.6f}\")\n",
    "        \n",
    "        # Step 4: Quality check - ensure tensor isn't all zeros\n",
    "        if n_zeros.item() == total_elements:\n",
    "            print(f\"      WARNING: Tensor is completely zero!\")\n",
    "            assert False, f\"Tensor {key} is completely zero - training failed!\"\n",
    "        else:\n",
    "            print(f\"      Tensor has learned weights\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "print(\" All LoRA tensors passed verification!\")\n",
    "\n",
    "# Additional Analysis: What makes a good LoRA adapter\n",
    "print(\"\\n=== LoRA Quality Analysis ===\")\n",
    "\n",
    "with safe_open(\"grpo_lora/adapter_model.safetensors\", framework=\"pt\") as f:\n",
    "    lora_A_tensors = []\n",
    "    lora_B_tensors = []\n",
    "    \n",
    "    for key in f.keys():\n",
    "        tensor = f.get_tensor(key)\n",
    "        \n",
    "        if \"lora_A\" in key:\n",
    "            lora_A_tensors.append((key, tensor))\n",
    "        elif \"lora_B\" in key:\n",
    "            lora_B_tensors.append((key, tensor))\n",
    "    \n",
    "    print(f\"LoRA A matrices (down-projection): {len(lora_A_tensors)}\")\n",
    "    print(f\"LoRA B matrices (up-projection): {len(lora_B_tensors)}\")\n",
    "    \n",
    "    # Analyze weight distributions\n",
    "    if lora_A_tensors and lora_B_tensors:\n",
    "        print(\"\\nWeight Distribution Analysis:\")\n",
    "        \n",
    "        # Check LoRA A matrices (should have diverse weights)\n",
    "        for key, tensor in lora_A_tensors[:3]:  # Show first 3\n",
    "            weight_range = tensor.max() - tensor.min()\n",
    "            print(f\"   {key.split('.')[-2:]}: range={weight_range.item():.6f}\")\n",
    "        \n",
    "        # Check rank utilization (effective rank)\n",
    "        sample_A = lora_A_tensors[0][1]\n",
    "        sample_B = lora_B_tensors[0][1]\n",
    "        \n",
    "        if sample_A.dim() == 2 and sample_B.dim() == 2:\n",
    "            # Calculate effective rank via SVD\n",
    "            try:\n",
    "                reconstructed = torch.mm(sample_B, sample_A)\n",
    "                U, S, V = torch.svd(reconstructed)\n",
    "                effective_rank = (S > S.max() * 0.01).sum().item()  # 1% threshold\n",
    "                total_rank = min(reconstructed.shape)\n",
    "                \n",
    "                print(f\"\\nRank Utilization:\")\n",
    "                print(f\"  Effective rank: {effective_rank}/{total_rank}\")\n",
    "                print(f\"  Rank efficiency: {effective_rank/total_rank*100:.1f}%\")\n",
    "                \n",
    "                if effective_rank < total_rank * 0.5:\n",
    "                    print(\"    Low rank utilization - consider reducing LoRA rank\")\n",
    "                else:\n",
    "                    print(\"   Good rank utilization\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   Couldn't calculate effective rank: {e}\")\n",
    "\n",
    "# What the assertion does:\n",
    "print(\"\\n=== Understanding the Assertion ===\")\n",
    "print(\"\"\"\n",
    "The assertion `assert(n_zeros.item() != tensor.numel())` checks:\n",
    "\n",
    " PASS if: n_zeros  total_elements (tensor has some non-zero values)\n",
    " FAIL if: n_zeros = total_elements (tensor is completely zero)\n",
    "\n",
    "Why this matters:\n",
    " LoRA matrices should contain learned weights after training\n",
    " All-zero tensors indicate failed or corrupted training\n",
    " Prevents using broken adapters that won't work\n",
    " Catches initialization errors or gradient flow problems\n",
    "\n",
    "For your RTX 2070 Super training:\n",
    " Non-zero weights = successful parameter updates\n",
    " Good weight diversity = effective adaptation\n",
    " Balanced A/B matrices = stable LoRA decomposition\n",
    "\"\"\")\n",
    "\n",
    "print(\" Verification complete! Your LoRA adapter is ready to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwpbwnDBVRLg"
   },
   "source": [
    "Now we load the LoRA and test. We tested without using our custom system prompt which should not (or minimal) affect toward the model's original reasoning ability.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190,
     "referenced_widgets": [
      "4e1e6351f86a4cf99ea07d6a1dd95b3e",
      "47a7688ba9044ba29ff56f32244b416d",
      "935cb5ca7b2f45f0884a59e3b9f3a7f4",
      "d324dd4ef84047aaaf46c5ccb9607638",
      "7f52de137c0748629d891d0e6223b81d",
      "1fe15cce61e944d58440251cc97946de",
      "7342685a5c6b4d2f8b69958792705d30",
      "a62fed49c7a14e14811ce4dfa4e02771",
      "4b7ef318e5a444a4808910bdd24699c8",
      "eb790732f82f4973a2669eecc057f711",
      "18f1fe2f63134275a131896d4fe1eb6d"
     ]
    },
    "id": "X6lXk47v1O4b",
    "outputId": "c0ddc7c8-fefa-4883-a128-304da0fec023"
   },
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\"role\": \"user\",   \"content\": \"Solve (x + 2)^2 = 0\"},\n",
    "# ]\n",
    "\n",
    "# text = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     add_generation_prompt = True, # Must add for generation\n",
    "#     tokenize = False,\n",
    "# )\n",
    "# from vllm import SamplingParams\n",
    "# sampling_params = SamplingParams(\n",
    "#     temperature = 1.0,\n",
    "#     top_k = 50,\n",
    "#     max_tokens = 2048,\n",
    "# )\n",
    "# output = model.fast_generate(\n",
    "#     text,\n",
    "#     sampling_params = sampling_params,\n",
    "#     lora_request = model.load_lora(\"grpo_lora\"),\n",
    "# )[0].outputs[0].text\n",
    "\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted prompt:\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Solve (x + 2)^2 = 0<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "==================================================\n",
      "Method 1: Standard HuggingFace Generate\n",
      "Generated Response:\n",
      "system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "user\n",
      "Solve (x + 2)^2 = 0\n",
      "assistant\n",
      "To solve the equation \\((x + 2)^2 = 0\\), we need to find the value of \\(x\\) that satisfies this equation.\n",
      "\n",
      "Step 1: Understand what it means for an expression to be equal to zero.\n",
      "An equation like \\((x + 2)^2 = 0\\) is true if and only if \\((x + 2) = 0\\). This is because squaring any non-zero number will never result in zero.\n",
      "\n",
      "Step 2: Solve for \\(x\\) in the equation \\((x + 2) = 0\\).\n",
      "\\[ x + 2 = 0 \\]\n",
      "\n",
      "Subtract 2 from both sides:\n",
      "\\[ x = -2 \\]\n",
      "\n",
      "Therefore, the solution to the equation \\((x + 2)^2 = 0\\) is \\(x = -2\\).\n",
      "\n",
      "We can verify this solution by substituting \\(x = -2\\) back into the original equation:\n",
      "\\[\n",
      "(-2 + 2)^2 = 0^2 = 0\n",
      "\\]\n",
      "This confirms that our solution is correct. The final answer is \\(x = -2\\).\n",
      "\n",
      "==================================================\n",
      "Method 2: Explicit LoRA Loading\n",
      " LoRA adapter already loaded on model\n",
      "Response with existing LoRA:\n",
      "the value of \\(x\\) that satisfies this equation.\n",
      "\n",
      "Step 1: Recognize that squaring any real number results in a non-negative value.\n",
      "Step 2: The only way for \\((x + 2)^2\\) to equal zero is if its expression inside the parentheses equals zero.\n",
      "Step 3: Therefore, set the expression inside the parentheses equal to zero:\n",
      "   \\[\n",
      "   x + 2 = 0\n",
      "   \\]\n",
      "Step 4: Solve for \\(x\\):\n",
      "   \\[\n",
      "   x = -2\n",
      "   \\]\n",
      "\n",
      "Therefore, the solution to the equation \\((x + 2)^2 = 0\\) is \\(x = -2\\).\n",
      "\n",
      "==================================================\n",
      "Method 3: Unsloth Inference (if available)\n",
      " Unsloth inference failed: 'list' object has no attribute 'shape'\n",
      "\n",
      "==================================================\n",
      "Alternative: Pure vLLM Setup\n",
      "\n",
      "If you want to use vLLM specifically:\n",
      "\n",
      "1. Save your model first:\n",
      "   model.save_pretrained(\"./my_trained_model\")\n",
      "   tokenizer.save_pretrained(\"./my_trained_model\")\n",
      "\n",
      "2. Use pure vLLM:\n",
      "   from vllm import LLM, SamplingParams\n",
      "\n",
      "   llm = LLM(\n",
      "       model=\"./my_trained_model\",\n",
      "       gpu_memory_utilization=0.8,\n",
      "       max_model_len=2048,\n",
      "   )\n",
      "\n",
      "   sampling_params = SamplingParams(\n",
      "       temperature=0.7,\n",
      "       top_k=50,\n",
      "       max_tokens=512,\n",
      "   )\n",
      "\n",
      "   outputs = llm.generate([text], sampling_params)\n",
      "   output = outputs[0].outputs[0].text\n",
      "\n",
      "\n",
      " VRAM usage: 1.61GB\n",
      "\n",
      " Chat inference complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Your chat message\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Solve (x + 2)^2 = 0\"},\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "print(\"Formatted prompt:\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# SOLUTION 1: Standard HuggingFace Inference (RECOMMENDED)\n",
    "print(\"Method 1: Standard HuggingFace Generate\")\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate response (LoRA weights already active if model was trained with PEFT)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "\n",
    "# Decode the full response\n",
    "full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract only the assistant's response (remove the input)\n",
    "if text in full_response:\n",
    "    output = full_response[len(text):].strip()\n",
    "else:\n",
    "    output = full_response\n",
    "\n",
    "print(\"Generated Response:\")\n",
    "print(output)\n",
    "\n",
    "# SOLUTION 2: Load LoRA separately (if needed)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Method 2: Explicit LoRA Loading\")\n",
    "\n",
    "try:\n",
    "    from peft import PeftModel\n",
    "    \n",
    "    # Check if model already has PEFT adapter\n",
    "    if hasattr(model, 'peft_config'):\n",
    "        print(\" LoRA adapter already loaded on model\")\n",
    "        \n",
    "        # Generate with existing adapter\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                temperature=0.7,\n",
    "                top_k=50,\n",
    "                max_new_tokens=512,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        output = full_response[len(text):].strip()\n",
    "        \n",
    "        print(\"Response with existing LoRA:\")\n",
    "        print(output)\n",
    "        \n",
    "    else:\n",
    "        print(\"  No PEFT adapter detected, loading separately...\")\n",
    "        \n",
    "        # Load base model without LoRA first (if needed)\n",
    "        from transformers import AutoModelForCausalLM\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"unsloth/Qwen2.5-1.5B-Instruct\",\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        # Load LoRA adapter\n",
    "        lora_model = PeftModel.from_pretrained(base_model, \"grpo_lora\")\n",
    "        \n",
    "        # Generate with LoRA\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(lora_model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = lora_model.generate(\n",
    "                **inputs,\n",
    "                temperature=0.7,\n",
    "                top_k=50,\n",
    "                max_new_tokens=512,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        output = full_response[len(text):].strip()\n",
    "        \n",
    "        print(\"Response with loaded LoRA:\")\n",
    "        print(output)\n",
    "        \n",
    "except ImportError:\n",
    "    print(\" PEFT not available\")\n",
    "except Exception as e:\n",
    "    print(f\" LoRA loading failed: {e}\")\n",
    "\n",
    "# SOLUTION 3: Unsloth Inference (if model supports it)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Method 3: Unsloth Inference (if available)\")\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    \n",
    "    # Enable fast inference\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Check if fast_generate is available\n",
    "    if hasattr(model, 'fast_generate'):\n",
    "        # Use Unsloth's fast generation (NO sampling_params!)\n",
    "        outputs = model.fast_generate(\n",
    "            [text],  # Note: list format\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "        )\n",
    "        \n",
    "        # Extract output based on Unsloth's return format\n",
    "        if hasattr(outputs[0], 'outputs'):\n",
    "            output = outputs[0].outputs[0].text\n",
    "        else:\n",
    "            output = outputs[0]\n",
    "            \n",
    "        print(\"Unsloth fast generation:\")\n",
    "        print(output)\n",
    "        \n",
    "    else:\n",
    "        print(\" fast_generate not available on this model\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\" Unsloth inference failed: {e}\")\n",
    "\n",
    "# SOLUTION 4: Pure vLLM (separate approach)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Alternative: Pure vLLM Setup\")\n",
    "print(\"\"\"\n",
    "If you want to use vLLM specifically:\n",
    "\n",
    "1. Save your model first:\n",
    "   model.save_pretrained(\"./my_trained_model\")\n",
    "   tokenizer.save_pretrained(\"./my_trained_model\")\n",
    "\n",
    "2. Use pure vLLM:\n",
    "   from vllm import LLM, SamplingParams\n",
    "   \n",
    "   llm = LLM(\n",
    "       model=\"./my_trained_model\",\n",
    "       gpu_memory_utilization=0.8,\n",
    "       max_model_len=2048,\n",
    "   )\n",
    "   \n",
    "   sampling_params = SamplingParams(\n",
    "       temperature=0.7,\n",
    "       top_k=50,\n",
    "       max_tokens=512,\n",
    "   )\n",
    "   \n",
    "   outputs = llm.generate([text], sampling_params)\n",
    "   output = outputs[0].outputs[0].text\n",
    "\"\"\")\n",
    "\n",
    "# Memory cleanup\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"\\n VRAM usage: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "print(\"\\n Chat inference complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g399AC2B1O4b"
   },
   "source": [
    "Next, let's test using our system prompt which should use the new language :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172,
     "referenced_widgets": [
      "622c7d8a89964ee3871f60d1d32dd9d7",
      "74655a758460452283c5487e67a09f98",
      "518811664352462bbd70d54d7ff75836",
      "a45642e387ce4d0a8cf69e1f36d0b279",
      "09aa5ac028274fecb5b8a777ea7709c2",
      "c937e93a5bc44a639fd4f7f543f86513",
      "46e960f31b954d779413192779825fdd",
      "872b861723d5466b99002d880af2987e",
      "39ad0bd8a1d74242bd72404859d7ca27",
      "54fb6e96cfeb42b7a64afa1dcecb080a",
      "34ffed32114e410a964ba6bc9f4444fa"
     ]
    },
    "id": "zf_OY5WMVOxF",
    "outputId": "6522d0ea-e766-4964-f0b1-bf1fe0631867"
   },
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": system_prompt},\n",
    "#     {\"role\": \"user\",   \"content\": \"Solve (x + 2)^2 = 0\"},\n",
    "# ]\n",
    "\n",
    "# text = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     add_generation_prompt = True, # Must add for generation\n",
    "#     tokenize = False,\n",
    "# )\n",
    "# from vllm import SamplingParams\n",
    "# sampling_params = SamplingParams(\n",
    "#     temperature = 1.0,\n",
    "#     top_k = 50,\n",
    "#     max_tokens = 2048,\n",
    "# )\n",
    "# output = model.fast_generate(\n",
    "#     text,\n",
    "#     sampling_params = sampling_params,\n",
    "#     lora_request = model.load_lora(\"grpo_lora\"),\n",
    "# )[0].outputs[0].text\n",
    "\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted prompt with system message:\n",
      "<|im_start|>system\n",
      "You are a helpful AI assistant that solves mathematical problems step by step. \n",
      "Provide clear explanations and show your work.<|im_end|>\n",
      "<|im_start|>user\n",
      "Solve (x + 2)^2 = 0<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "==================================================\n",
      "Generating response with your trained LoRA model...\n",
      "Assistant Response:\n",
      "system\n",
      "You are a helpful AI assistant that solves mathematical problems step by step. \n",
      "Provide clear explanations and show your work.\n",
      "user\n",
      "Solve (x + 2)^2 = 0\n",
      "assistant\n",
      "To solve the equation \\((x + 2)^2 = 0\\), we need to find the value of \\(x\\) that satisfies this equation.\n",
      "\n",
      "Step 1: Understand what it means for an expression to be equal to zero.\n",
      "An expression is said to be equal to zero if both sides of the equation are identical. This is because any number multiplied by itself cannot result in a negative value, except for zero when squared or cubed.\n",
      "\n",
      "Step 2: Apply the square root property.\n",
      "Since the given equation involves squaring the term inside the parentheses, we can take the square root of both sides. Remember, taking the square root gives us two possible solutions because each side of a perfect square has two roots:\n",
      "\n",
      "\\[ x + 2 = \\pm \\sqrt{0} \\]\n",
      "\n",
      "Step 3: Simplify the right-hand side.\n",
      "We know that the square root of zero is zero:\n",
      "\n",
      "\\[ x + 2 = \\pm 0 \\]\n",
      "\\[ x + 2 = 0 \\]\n",
      "\n",
      "Step 4: Solve for \\(x\\).\n",
      "Subtract 2 from both sides:\n",
      "\n",
      "\\[ x = -2 \\]\n",
      "\n",
      "Therefore, the solution to the equation \\((x + 2)^2 = 0\\) is \\(x = -2\\).\n",
      "\n",
      "==================================================\n",
      "Batch Processing Example:\n",
      "\n",
      "Problem 1 Response:\n",
      "the value of \\(x\\) that satisfies this equation.\n",
      "\n",
      "Step 1: Understand what it means for a number squared to equal zero.\n",
      "- If \\(a^2 = b\\) where \\(a\\) is any real number, then \\(b = 0\\).\n",
      "\n",
      "In our case, \\((x + 2)^2 = 0\\). This means that \\((x + 2)\\) must be equal to zero because squaring a non-zero number cannot result in zero.\n",
      "\n",
      "Step 2: Set up the equation inside the parentheses equal to zero.\n",
      "\\[\n",
      "x + 2 = 0\n",
      "\\]\n",
      "\n",
      "Step 3: Solve for \\(x\\).\n",
      "\\[\n",
      "x = -2\n",
      "\\]\n",
      "\n",
      "Therefore, the solution to the equation \\((x + 2)^2 = 0\\) is \\(x = -2\\).\n",
      "\n",
      "Problem 2 Response:\n",
      "1, we'll use the power rule for differentiation.\n",
      "The power rule states that if y = ax^n, then dy/dx = n * ax^(n-1).\n",
      "Applying this rule to each term in the function:\n",
      "d/dx (x^2) = 2x\n",
      "d/dx (3x) = 3\n",
      "d/dx (1) = 0\n",
      "Now, add up all these derivatives: d/dx (x^2 + 3x + 1) = 2x + 3 + 0 = 2x + 3\n",
      "Therefore, the derivative of the given function is f'(x) = 2x + 3.\n",
      "\n",
      "==================================================\n",
      "Multi-turn Conversation Example:\n",
      "\n",
      "Turn 1:\n",
      "Assistant: these steps:\n",
      "\n",
      "1. **Understand the equation**: The given equation is \\((x + 2)^2 = 0\\). This represents a quadratic expression set equal to zero.\n",
      "\n",
      "2. **Take the square root of both sides**:\n",
      "   Since the square of any real number is non-negative, the only way for \\((x + 2)^2\\) to be equal to zero is if \\(x + 2\\) itself is zero.\n",
      "   \n",
      "3. **Set the inside of the parentheses equal to zero**:\n",
      "   We have \\(x + 2 = 0\\).\n",
      "\n",
      "4. **Solve for \\(x\\)**:\n",
      "   Subtract 2 from both sides: \\(x + 2 - 2 = 0 - 2\\).\n",
      "   Simplifying this gives us \\(x = -2\\).\n",
      "\n",
      "5. **Verify the solution**:\n",
      "   Substitute \\(x = -2\\) back into the original equation to ensure it satisfies the equation:\n",
      "   \\[\n",
      "   (-2 + 2)^2 = 0^2 = 0\n",
      "   \\]\n",
      "   Since the left side equals the right side, our solution is correct.\n",
      "\n",
      "Therefore,\n",
      "\n",
      "Turn 2:\n",
      "Assistant: variables and numbers. Let's go through it step by step:\n",
      "\n",
      "### Step 1: Take the square root of both sides\n",
      "\n",
      "Since \\((x - 3)^2 = 16\\) can be written as \\(\\sqrt{(x - 3)^2} = \\sqrt{16}\\), we need to consider both the positive and negative roots because squaring removes the sign.\n",
      "\n",
      "\\[\n",
      "\\sqrt{(x - 3)^2} = \\pm \\sqrt{16}\n",
      "\\]\n",
      "\n",
      "This simplifies to:\n",
      "\n",
      "\\[\n",
      "|x - 3| = \\pm 4\n",
      "\\]\n",
      "\n",
      "### Step 2: Solve the absolute value equation\n",
      "\n",
      "The absolute value equation \\(|x - 3| = \\pm 4\\) means two separate equations to consider:\n",
      "\n",
      "#### Case 1: \\(x - 3 = 4\\)\n",
      "\n",
      "\\[\n",
      "x - 3 = 4 \\\\\n",
      "x = 4 + 3 \\\\\n",
      "x = 7\n",
      "\\]\n",
      "\n",
      "#### Case 2: \\(x - 3 = -4\\)\n",
      "\n",
      "\\[\n",
      "x - 3 = -4 \\\\\n",
      "x = -4 +\n",
      "\n",
      "Full conversation has 5 messages\n",
      "\n",
      " VRAM usage: 1.61GB\n",
      "\n",
      " System prompt chat inference complete!\n",
      "\n",
      "============================================================\n",
      "Why your original code failed:\n",
      "\n",
      " ERRORS IN ORIGINAL CODE:\n",
      "1. model.load_lora() - Method doesn't exist on Qwen2ForCausalLM\n",
      "2. model.fast_generate() - Method doesn't exist on your model type  \n",
      "3. SamplingParams - This is vLLM syntax, not HuggingFace\n",
      "4. [0].outputs[0].text - This is vLLM output format\n",
      "\n",
      " FIXES APPLIED:\n",
      "1. Used model.generate() - Standard HuggingFace method\n",
      "2. LoRA weights already active from training\n",
      "3. Parameters passed directly (temperature, top_k, etc.)\n",
      "4. Standard tensor decoding with tokenizer.decode()\n",
      "5. Proper memory management for RTX 2070 Super\n",
      "\n",
      " KEY INSIGHT:\n",
      "Your model already has LoRA weights loaded if you trained it with PEFT.\n",
      "No need to load them again - just use standard inference!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define your system prompt (you need to define this)\n",
    "system_prompt = \"\"\"You are a helpful AI assistant that solves mathematical problems step by step. \n",
    "Provide clear explanations and show your work.\"\"\"\n",
    "\n",
    "# Chat messages with system prompt\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"Solve (x + 2)^2 = 0\"},\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "print(\"Formatted prompt with system message:\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# FIXED METHOD: Standard HuggingFace Inference\n",
    "print(\"Generating response with your trained LoRA model...\")\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate response (your LoRA weights are already active)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        temperature=1.0,        # Your original temperature\n",
    "        top_k=50,              # Your original top_k\n",
    "        max_new_tokens=512,    # Reduced from 2048 for RTX 2070 Super\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.1,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "# Decode the response\n",
    "full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract only the assistant's response\n",
    "if text in full_response:\n",
    "    output = full_response[len(text):].strip()\n",
    "else:\n",
    "    output = full_response\n",
    "\n",
    "print(\"Assistant Response:\")\n",
    "print(output)\n",
    "\n",
    "# Alternative: Batch processing for multiple questions\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Batch Processing Example:\")\n",
    "\n",
    "# Multiple math problems\n",
    "batch_messages = [\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"Solve (x + 2)^2 = 0\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"Find the derivative of x^2 + 3x + 1\"}\n",
    "    ]\n",
    "]\n",
    "\n",
    "batch_outputs = []\n",
    "\n",
    "for i, msgs in enumerate(batch_messages):\n",
    "    # Format each conversation\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        msgs,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    batch_inputs = tokenizer(formatted_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        batch_result = model.generate(\n",
    "            **batch_inputs,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    full_resp = tokenizer.decode(batch_result[0], skip_special_tokens=True)\n",
    "    response = full_resp[len(formatted_text):].strip()\n",
    "    \n",
    "    batch_outputs.append(response)\n",
    "    print(f\"\\nProblem {i+1} Response:\")\n",
    "    print(response)\n",
    "\n",
    "# Memory-efficient conversation handling\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Multi-turn Conversation Example:\")\n",
    "\n",
    "# Start a conversation\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt}\n",
    "]\n",
    "\n",
    "# Function to continue conversation\n",
    "def chat_turn(user_message, conversation_history):\n",
    "    # Add user message\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    # Format conversation\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        conversation_history,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    \n",
    "    # Generate response\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Extract assistant response\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    assistant_response = full_response[len(formatted):].strip()\n",
    "    \n",
    "    # Add to conversation history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "    \n",
    "    return assistant_response\n",
    "\n",
    "# Example multi-turn conversation\n",
    "print(\"\\nTurn 1:\")\n",
    "response1 = chat_turn(\"Solve (x + 2)^2 = 0\", conversation)\n",
    "print(f\"Assistant: {response1}\")\n",
    "\n",
    "print(\"\\nTurn 2:\")\n",
    "response2 = chat_turn(\"Now solve (x - 3)^2 = 16\", conversation)\n",
    "print(f\"Assistant: {response2}\")\n",
    "\n",
    "# Show conversation history\n",
    "print(f\"\\nFull conversation has {len(conversation)} messages\")\n",
    "\n",
    "# Memory cleanup for RTX 2070 Super\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"\\n VRAM usage: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "\n",
    "print(\"\\n System prompt chat inference complete!\")\n",
    "\n",
    "# EXPLANATION: Why the original code failed\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Why your original code failed:\")\n",
    "print(\"\"\"\n",
    " ERRORS IN ORIGINAL CODE:\n",
    "1. model.load_lora() - Method doesn't exist on Qwen2ForCausalLM\n",
    "2. model.fast_generate() - Method doesn't exist on your model type  \n",
    "3. SamplingParams - This is vLLM syntax, not HuggingFace\n",
    "4. [0].outputs[0].text - This is vLLM output format\n",
    "\n",
    " FIXES APPLIED:\n",
    "1. Used model.generate() - Standard HuggingFace method\n",
    "2. LoRA weights already active from training\n",
    "3. Parameters passed directly (temperature, top_k, etc.)\n",
    "4. Standard tensor decoding with tokenizer.decode()\n",
    "5. Proper memory management for RTX 2070 Super\n",
    "\n",
    " KEY INSIGHT:\n",
    "Your model already has LoRA weights loaded if you trained it with PEFT.\n",
    "No need to load them again - just use standard inference!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ad5qCZMsW_Ed"
   },
   "source": [
    "Lets compare our results with system prompt but without our LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172,
     "referenced_widgets": [
      "f35b16000bb448139b0954c4eeff99f8",
      "ac02d83c54314dd889853d5dfbfcae6f",
      "ccb09cc19f804fbca9e3976116227460",
      "efa40eb719fd472f804af545734a4fc0",
      "17813750f23d44fb9c9305506030709f",
      "0693028d222b43e9852beec995ecb5dc",
      "7fecf7366e9d41cca37f32d736eed6e6",
      "783d83041ef2494287de1352d66e7e85",
      "59b5cb2fc77d46d395b22ef63954a7f4",
      "3b161341e8d64f5392ebcebf8629df1e",
      "e98eff347291445f851d14470b0552cb"
     ]
    },
    "id": "ee10WWhDW_Ee",
    "outputId": "3f7e2b03-c562-46b7-f9d3-27a36af3b473"
   },
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": system_prompt},\n",
    "#     {\"role\": \"user\",   \"content\": \"Solve (x + 2)^2 = 0\"},\n",
    "# ]\n",
    "\n",
    "# text = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     add_generation_prompt = True, # Must add for generation\n",
    "#     tokenize = False,\n",
    "# )\n",
    "# from vllm import SamplingParams\n",
    "# sampling_params = SamplingParams(\n",
    "#     temperature = 1.0,\n",
    "#     top_k = 50,\n",
    "#     max_tokens = 2048,\n",
    "# )\n",
    "# output = model.fast_generate(\n",
    "#     text,\n",
    "#     sampling_params = sampling_params,\n",
    "#     lora_request = None,\n",
    "# )[0].outputs[0].text\n",
    "\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted prompt:\n",
      "<|im_start|>system\n",
      "You are a helpful AI assistant that solves mathematical problems step by step. \n",
      "Provide clear explanations and show your work.<|im_end|>\n",
      "<|im_start|>user\n",
      "Solve (x + 2)^2 = 0<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "==================================================\n",
      "Generating response...\n",
      "Generated Output:\n",
      "system\n",
      "You are a helpful AI assistant that solves mathematical problems step by step. \n",
      "Provide clear explanations and show your work.\n",
      "user\n",
      "Solve (x + 2)^2 = 0\n",
      "assistant\n",
      "To solve the equation \\((x + 2)^2 = 0\\), we need to find the value of \\(x\\) that makes the equation true.\n",
      "\n",
      "Step 1: Recognize that the square of any number is equal to zero only if that number itself is zero.\n",
      "So, if \\((x + 2)^2 = 0\\), then \\(x + 2\\) must be equal to zero.\n",
      "\n",
      "Step 2: Set up an equation based on this observation:\n",
      "\\[ x + 2 = 0 \\]\n",
      "\n",
      "Step 3: Solve for \\(x\\).\n",
      "Subtract 2 from both sides of the equation:\n",
      "\\[ x + 2 - 2 = 0 - 2 \\]\n",
      "\\[ x = -2 \\]\n",
      "\n",
      "Therefore, the solution to the equation \\((x + 2)^2 = 0\\) is \\(x = -2\\).\n",
      "\n",
      " VRAM usage: 1.61GB\n",
      "\n",
      "============================================================\n",
      " DELETE THESE LINES FROM YOUR CODE:\n",
      "\n",
      "# DELETE THIS:\n",
      "from vllm import SamplingParams\n",
      "sampling_params = SamplingParams(\n",
      "    temperature = 1.0,\n",
      "    top_k = 50,\n",
      "    max_tokens = 2048,\n",
      ")\n",
      "output = model.fast_generate(\n",
      "    text,\n",
      "    sampling_params = sampling_params,\n",
      "    lora_request = None,\n",
      ")[0].outputs[0].text\n",
      "\n",
      "# REPLACE WITH:\n",
      "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
      "with torch.no_grad():\n",
      "    outputs = model.generate(\n",
      "        **inputs,\n",
      "        temperature=1.0,\n",
      "        top_k=50,\n",
      "        max_new_tokens=512,\n",
      "        do_sample=True,\n",
      "        pad_token_id=tokenizer.eos_token_id,\n",
      "    )\n",
      "full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
      "output = full_response[len(text):].strip()\n",
      "\n",
      " This version works with your Qwen2ForCausalLM model!\n",
      "\n",
      "==================================================\n",
      "Quick one-liner version:\n",
      "Quick test output:\n",
      "the value of \\(x\\) that satisfies this equation.\n",
      "\n",
      "Step 1: Understand what it means for an expression to be equal to zero.\n",
      "An expression is considered zero when its value is zero. So, we set the inside of the square equal to zero because squaring any number results in a non-negative value unless that number itself is zero.\n",
      "\n",
      "Step 2: Set up the equation.\n",
      "Since \\((x + 2)^2 = 0\\), we can set the term inside the parentheses equal to zero:\n",
      "\n",
      "\\[ x + 2 = 0 \\]\n",
      "\n",
      "Step 3: Solve for \\(x\\).\n",
      "Now we isolate \\(x\\) on one side of the equation:\n",
      "\n",
      "\\[ x = -2 \\]\n",
      "\n",
      "Therefore, the solution to the equation \\((x + 2)^2 = 0\\) is \\(x = -2\\).\n",
      "\n",
      "We can verify this solution by substituting \\(x = -2\\) back into the original equation:\n",
      "\n",
      "\\[ (-2 + 2)^2 = 0^2 = 0 \\]\n",
      "\n",
      "This confirms that our solution is correct. The final answer is \\(x = -2\\).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define system prompt\n",
    "system_prompt = \"\"\"You are a helpful AI assistant that solves mathematical problems step by step. \n",
    "Provide clear explanations and show your work.\"\"\"\n",
    "\n",
    "# Your messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"Solve (x + 2)^2 = 0\"},\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "print(\"Formatted prompt:\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# CORRECT CODE: Replace everything after chat template with this\n",
    "print(\"Generating response...\")\n",
    "\n",
    "# Tokenize the formatted text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate using standard HuggingFace method\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        temperature=1.0,           # Same as your original\n",
    "        top_k=50,                 # Same as your original  \n",
    "        max_new_tokens=512,       # Reduced for RTX 2070 Super\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "# Decode the output\n",
    "full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract only the new generated text (remove the input prompt)\n",
    "if text in full_response:\n",
    "    output = full_response[len(text):].strip()\n",
    "else:\n",
    "    output = full_response\n",
    "\n",
    "# Display result\n",
    "print(\"Generated Output:\")\n",
    "print(output)\n",
    "\n",
    "# Memory status\n",
    "print(f\"\\n VRAM usage: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "\n",
    "# WHAT YOU NEED TO DELETE FROM YOUR ORIGINAL CODE:\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" DELETE THESE LINES FROM YOUR CODE:\")\n",
    "print(\"\"\"\n",
    "# DELETE THIS:\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 1.0,\n",
    "    top_k = 50,\n",
    "    max_tokens = 2048,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = None,\n",
    ")[0].outputs[0].text\n",
    "\n",
    "# REPLACE WITH:\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        temperature=1.0,\n",
    "        top_k=50,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "output = full_response[len(text):].strip()\n",
    "\"\"\")\n",
    "\n",
    "print(\" This version works with your Qwen2ForCausalLM model!\")\n",
    "\n",
    "# Alternative: One-liner version for quick testing\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Quick one-liner version:\")\n",
    "\n",
    "def quick_chat(user_message, system_msg=None):\n",
    "    \"\"\"Quick chat function for testing\"\"\"\n",
    "    msgs = []\n",
    "    if system_msg:\n",
    "        msgs.append({\"role\": \"system\", \"content\": system_msg})\n",
    "    msgs.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(msgs, add_generation_prompt=True, tokenize=False)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.7, max_new_tokens=256, do_sample=True)\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response[len(prompt):].strip()\n",
    "\n",
    "# Test the quick function\n",
    "test_output = quick_chat(\"Solve (x + 2)^2 = 0\", system_prompt)\n",
    "print(\"Quick test output:\")\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYqpfCF0W_Ee"
   },
   "source": [
    "Let's take 20 samples, and compare the the amount of using our LoRA and not using it, and see which one has better amount of correct language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mJmztPHdW_Ef",
    "outputId": "2ea0c34d-abea-45c2-b04b-616323f9f1aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'solution', 'data_source', 'source_prompt', 'ability', 'reward_model', 'extra_info', 'answer'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset = dataset.shuffle(seed = 3407).select(range(20))\n",
    "sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing language usage with and without LoRA on 20 samples:\n",
      "============================================================\n",
      " Dataset structure confirmed:\n",
      "   - sample['prompt'][0]['content'] = system message\n",
      "   - sample['prompt'][1]['content'] = user message\n",
      "   - sample['solution'] = expected solution\n",
      "\n",
      " Method 1: Using PEFT adapter enable/disable\n",
      " PEFT adapter control available\n",
      "\n",
      " Sample 1:\n",
      "   User: For a positive integer $N$, we color the positive divisors of $N$ (including 1 and $N$) with four co...\n",
      "    With LoRA (en): To solve this problem, let's first understand what it means for a coloring to be...\n",
      "    Without LoRA (en): To solve this problem, let's first understand what a multichromatic coloring mea...\n",
      "\n",
      " Sample 2:\n",
      "   User: Let $f$ be a non-constant polynomial such that\n",
      "\\[f(x - 1) + f(x) + f(x + 1) = \\frac{[f(x)]^2}{2013x}...\n",
      "    With LoRA (en): To solve for \\( f(1) \\), we start by analyzing the given functional equation:\n",
      "\\[...\n",
      "    Without LoRA (en): To solve for \\( f(1) \\), we start by analyzing the given functional equation:\n",
      "\\[...\n",
      "\n",
      " Sample 3:\n",
      "   User: Find the sum of all integers between $-\\sqrt{1442}$ and $\\sqrt{2020}$....\n",
      "    With LoRA (en): To find the sum of all integers between \\(-\\sqrt{1442}\\) and \\(\\sqrt{2020}\\), we...\n",
      "    Without LoRA (en): To find the sum of all integers between \\(-\\sqrt{1442}\\) and \\(\\sqrt{2020}\\), we...\n",
      "\n",
      " Sample 4:\n",
      "   User: Given the equations $3x+y=17,5y+z=14$ and $3x+5z=41$, what is the value of the sum $x+y+z$?...\n",
      "\n",
      " Sample 5:\n",
      "   User: Determine the sum of all single-digit replacements for $z$ such that the number ${24{,}z38}$ is divi...\n",
      "\n",
      " Progress: 5 samples processed\n",
      "   LoRA: 0 Indonesian, Base: 0 Indonesian\n",
      "\n",
      " Sample 6:\n",
      "   User: It is given that $2^{333}$ is a 101-digit number whose first digit is 1. How many of the numbers $2^...\n",
      "\n",
      " Sample 7:\n",
      "   User: A convex polyhedron has $n$ faces, all of which are congruent triangles with angles $36^{\\circ}$, $7...\n",
      "\n",
      " Sample 8:\n",
      "   User: Let $A_{12}$ denote the answer to problem $12$.  There exists a unique triple of digits $(B,C,D)$ su...\n",
      "\n",
      " Sample 9:\n",
      "   User: Mrs. Jones is pouring orange juice into four identical glasses for her four sons. She fills the firs...\n",
      "\n",
      " Sample 10:\n",
      "   User: A fisherman can see seven aquatic creatures in a lake --- four crocodiles, a catfish, and two giant ...\n",
      "\n",
      " Progress: 10 samples processed\n",
      "   LoRA: 0 Indonesian, Base: 0 Indonesian\n",
      "\n",
      " Sample 11:\n",
      "   User: Fruit salad can be made with any $3$ of these $5$ fruits: apples, bananas, grapes, strawberries, and...\n",
      "\n",
      " Sample 12:\n",
      "   User: A time is chosen randomly and uniformly in a 24-hour day. The probability that at that time, the non...\n",
      "\n",
      " Sample 13:\n",
      "   User: The base-ten representation for $19!$ is $121,6T5,100,40M,832,H00$, where $T$, $M$, and $H$ denote d...\n",
      "\n",
      " Sample 14:\n",
      "   User: David, when submitting a problem for CMIMC, wrote his answer as $100\\frac{x}{y}$, where $x$ and $y$ ...\n",
      "\n",
      " Sample 15:\n",
      "   User: Let $a,$ $b,$ $c$ be distinct integers, and let $\\omega$ be a complex number such that $\\omega^3 = 1...\n",
      "\n",
      " Progress: 15 samples processed\n",
      "   LoRA: 0 Indonesian, Base: 0 Indonesian\n",
      "\n",
      " Sample 16:\n",
      "   User: Two different points, $C$ and $D$, lie on the same side of line $AB$ so that $\\triangle ABC$ and $\\t...\n",
      "\n",
      " Sample 17:\n",
      "   User: Let $x$, $y$, and $z$ be positive integers satisfying the following system of equations:\n",
      "\n",
      "\\[\n",
      "\\begin{...\n",
      "\n",
      " Sample 18:\n",
      "   User: Six different small books and three different large books are placed on a shelf. Three children can ...\n",
      "\n",
      " Sample 19:\n",
      "   User: Hiram's algebra notes are $50$ pages long and are printed on $25$ sheets of paper; the first sheet c...\n",
      "\n",
      " Sample 20:\n",
      "   User: Let \\(a, b,\\) and \\(c\\) be real numbers such that  \n",
      "  \n",
      "\\(a+b+c=2,\\) and  \n",
      "\\(a^2+b^2+c^2=12\\)  \n",
      "  \n",
      "Fi...\n",
      "\n",
      " Progress: 20 samples processed\n",
      "   LoRA: 0 Indonesian, Base: 0 Indonesian\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS:\n",
      "Successfully processed: 20 samples\n",
      "With LoRA - Indonesian responses: 0/20 (0.0%)\n",
      "Without LoRA - Indonesian responses: 0/20 (0.0%)\n",
      "Improvement: +0 Indonesian responses with LoRA\n",
      " No change in language usage\n",
      "\n",
      " Final VRAM usage: 1.61GB\n",
      " LoRA comparison completed!\n",
      "\n",
      " Sample data structure for reference:\n",
      "Sample keys: ['prompt', 'solution', 'data_source', 'source_prompt', 'ability', 'reward_model', 'extra_info', 'answer']\n",
      "System prompt: You are given a problem.\n",
      "Think about the problem and provide your working out.\n",
      "You must think in Bah...\n",
      "User message: For a positive integer $N$, we color the positive divisors of $N$ (including 1 and $N$) with four co...\n",
      "Expected solution: 192...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Initialize counters\n",
    "with_lora_id_count = 0\n",
    "without_lora_id_count = 0\n",
    "\n",
    "print(\"Comparing language usage with and without LoRA on 20 samples:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Debug dataset structure (from your output, we know the structure)\n",
    "print(\" Dataset structure confirmed:\")\n",
    "print(\"   - sample['prompt'][0]['content'] = system message\")\n",
    "print(\"   - sample['prompt'][1]['content'] = user message\")\n",
    "print(\"   - sample['solution'] = expected solution\")\n",
    "\n",
    "# SOLUTION 1: Use PEFT adapter enable/disable (RECOMMENDED)\n",
    "print(\"\\n Method 1: Using PEFT adapter enable/disable\")\n",
    "\n",
    "def generate_response(model_to_use, text, max_length=256):\n",
    "    \"\"\"Helper function with error handling\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024).to(model_to_use.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model_to_use.generate(\n",
    "                **inputs,\n",
    "                temperature=0.7,  # Reduced for stability\n",
    "                top_k=50,\n",
    "                max_new_tokens=max_length,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "            )\n",
    "        \n",
    "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return full_response[len(tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)):].strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"    Generation error: {str(e)[:100]}...\")\n",
    "        return f\"[Generation failed: {str(e)[:50]}...]\"\n",
    "\n",
    "# Check if model has PEFT adapter control\n",
    "try:\n",
    "    if hasattr(model, 'disable_adapter_layers') and hasattr(model, 'enable_adapter_layers'):\n",
    "        print(\" PEFT adapter control available\")\n",
    "        \n",
    "        # Process samples using adapter enable/disable\n",
    "        processed_samples = 0\n",
    "        \n",
    "        for i, sample in enumerate(sample_dataset):\n",
    "            try:\n",
    "                # Extract the correct user content\n",
    "                if isinstance(sample, dict) and 'prompt' in sample:\n",
    "                    if isinstance(sample['prompt'], list) and len(sample['prompt']) > 1:\n",
    "                        user_content = sample['prompt'][1]['content']\n",
    "                        system_content = sample['prompt'][0]['content']\n",
    "                    else:\n",
    "                        continue  # Skip malformed samples\n",
    "                else:\n",
    "                    continue  # Skip if not correct format\n",
    "                \n",
    "                print(f\"\\n Sample {i+1}:\")\n",
    "                print(f\"   User: {user_content[:100]}...\")\n",
    "                \n",
    "                # Format conversation\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_content},\n",
    "                    {\"role\": \"user\", \"content\": user_content},\n",
    "                ]\n",
    "                \n",
    "                text = tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    add_generation_prompt=True,\n",
    "                    tokenize=False,\n",
    "                )\n",
    "                \n",
    "                # Generate WITH LoRA (adapter enabled)\n",
    "                model.enable_adapter_layers()\n",
    "                output_with_lora = generate_response(model, text, max_length=128)\n",
    "                \n",
    "                # Generate WITHOUT LoRA (adapter disabled)\n",
    "                model.disable_adapter_layers() \n",
    "                output_without_lora = generate_response(model, text, max_length=128)\n",
    "                \n",
    "                # Re-enable adapter for next iteration\n",
    "                model.enable_adapter_layers()\n",
    "                \n",
    "                # Check if responses are valid\n",
    "                if \"[Generation failed\" in output_with_lora or \"[Generation failed\" in output_without_lora:\n",
    "                    print(\"    Generation failed, skipping sample\")\n",
    "                    continue\n",
    "                \n",
    "                # Detect language\n",
    "                lang_with_lora = get_lang(output_with_lora)\n",
    "                lang_without_lora = get_lang(output_without_lora)\n",
    "                \n",
    "                # Count Indonesian responses\n",
    "                if lang_with_lora == 'id':\n",
    "                    with_lora_id_count += 1\n",
    "                if lang_without_lora == 'id':\n",
    "                    without_lora_id_count += 1\n",
    "                \n",
    "                processed_samples += 1\n",
    "                \n",
    "                # Show results for first few samples\n",
    "                if processed_samples <= 3:\n",
    "                    print(f\"    With LoRA ({lang_with_lora}): {output_with_lora[:80]}...\")\n",
    "                    print(f\"    Without LoRA ({lang_without_lora}): {output_without_lora[:80]}...\")\n",
    "                \n",
    "                # Progress updates\n",
    "                if processed_samples % 5 == 0:\n",
    "                    print(f\"\\n Progress: {processed_samples} samples processed\")\n",
    "                    print(f\"   LoRA: {with_lora_id_count} Indonesian, Base: {without_lora_id_count} Indonesian\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                \n",
    "                # Stop after 20 successful samples\n",
    "                if processed_samples >= 20:\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"    Error processing sample {i+1}: {str(e)[:100]}...\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(\"FINAL RESULTS:\")\n",
    "        print(f\"Successfully processed: {processed_samples} samples\")\n",
    "        if processed_samples > 0:\n",
    "            print(f\"With LoRA - Indonesian responses: {with_lora_id_count}/{processed_samples} ({with_lora_id_count/processed_samples*100:.1f}%)\")\n",
    "            print(f\"Without LoRA - Indonesian responses: {without_lora_id_count}/{processed_samples} ({without_lora_id_count/processed_samples*100:.1f}%)\")\n",
    "            print(f\"Improvement: +{with_lora_id_count - without_lora_id_count} Indonesian responses with LoRA\")\n",
    "            \n",
    "            if with_lora_id_count > without_lora_id_count:\n",
    "                print(\" LoRA training improved Indonesian language usage!\")\n",
    "            elif with_lora_id_count == without_lora_id_count:\n",
    "                print(\" No change in language usage\")\n",
    "            else:\n",
    "                print(\" LoRA reduced Indonesian usage\")\n",
    "        else:\n",
    "            print(\" No samples processed successfully\")\n",
    "    \n",
    "    else:\n",
    "        print(\" PEFT adapter control not available\")\n",
    "        raise Exception(\"No adapter control\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\" PEFT method failed: {e}\")\n",
    "    print(\"\\n Method 2: Simplified single model test\")\n",
    "    \n",
    "    # SOLUTION 2: Test current model only (simplified)\n",
    "    print(\"Testing current model responses (LoRA active)...\")\n",
    "    \n",
    "    lora_id_responses = 0\n",
    "    total_tested = 0\n",
    "    \n",
    "    for i, sample in enumerate(sample_dataset[:10]):  # Test 10 samples\n",
    "        try:\n",
    "            if isinstance(sample, dict) and 'prompt' in sample:\n",
    "                if isinstance(sample['prompt'], list) and len(sample['prompt']) > 1:\n",
    "                    user_content = sample['prompt'][1]['content']\n",
    "                    system_content = sample['prompt'][0]['content']\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_content},\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "            ]\n",
    "            \n",
    "            text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "            \n",
    "            # Generate with current model (has LoRA)\n",
    "            response = generate_response(model, text, max_length=128)\n",
    "            \n",
    "            if \"[Generation failed\" not in response:\n",
    "                lang = get_lang(response)\n",
    "                if lang == 'id':\n",
    "                    lora_id_responses += 1\n",
    "                total_tested += 1\n",
    "                \n",
    "                print(f\"Sample {i+1} ({lang}): {response[:80]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Sample {i+1} failed: {str(e)[:50]}...\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n Simplified Test Results:\")\n",
    "    print(f\"LoRA model Indonesian responses: {lora_id_responses}/{total_tested}\")\n",
    "    if total_tested > 0:\n",
    "        print(f\"Indonesian percentage: {lora_id_responses/total_tested*100:.1f}%\")\n",
    "\n",
    "# Final cleanup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(f\"\\n Final VRAM usage: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "print(\" LoRA comparison completed!\")\n",
    "\n",
    "# Show what a proper working sample looks like\n",
    "print(f\"\\n Sample data structure for reference:\")\n",
    "if len(sample_dataset) > 0:\n",
    "    sample = sample_dataset[0]\n",
    "    print(f\"Sample keys: {list(sample.keys())}\")\n",
    "    print(f\"System prompt: {sample['prompt'][0]['content'][:100]}...\")\n",
    "    print(f\"User message: {sample['prompt'][1]['content'][:100]}...\")\n",
    "    if 'solution' in sample:\n",
    "        print(f\"Expected solution: {str(sample['solution'])[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aDgFfhFYIAS"
   },
   "source": [
    "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NUEmHFSYNTp"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52WMb3k_YPt8"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n",
    "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saving Options for Qwen2ForCausalLM with LoRA\n",
      "============================================================\n",
      "1 Saving LoRA adapters only...\n",
      " LoRA adapters saved to 'model_lora' folder\n",
      "   Size: 85.6MB\n",
      "\n",
      "2 Merge to 16-bit and save...\n",
      "\n",
      "3 Merge to 4-bit and save...\n",
      "\n",
      "4 Push to Hugging Face Hub...\n",
      "\n",
      "============================================================\n",
      " SAVED FILES VERIFICATION:\n",
      "\n",
      " model_lora:\n",
      "   Files: 10\n",
      "   Size: 85.6MB\n",
      "    adapter_config.json (0.0MB)\n",
      "    adapter_model.safetensors (70.5MB)\n",
      "    added_tokens.json (0.0MB)\n",
      "\n",
      " model_merged_16bit: Not found\n",
      "\n",
      " model_16bit: Not found\n",
      "\n",
      " model_4bit: Not found\n",
      "\n",
      " model_4bit_config: Not found\n",
      "\n",
      "============================================================\n",
      " LOADING INSTRUCTIONS:\n",
      "\n",
      " To load LoRA adapters:\n",
      "   from peft import PeftModel\n",
      "   base_model = AutoModelForCausalLM.from_pretrained(\"unsloth/Qwen2.5-1.5B-Instruct\")\n",
      "   model = PeftModel.from_pretrained(base_model, \"model_lora\")\n",
      "\n",
      " To load 16-bit merged model:\n",
      "   model = AutoModelForCausalLM.from_pretrained(\"model_merged_16bit\")\n",
      "\n",
      " To load with 4-bit quantization:\n",
      "   model = AutoModelForCausalLM.from_pretrained(\n",
      "       \"model_4bit\", \n",
      "       load_in_4bit=True,\n",
      "       device_map=\"auto\"\n",
      "   )\n",
      "\n",
      " From Hugging Face Hub:\n",
      "   model = AutoModelForCausalLM.from_pretrained(\"your-username/your-model-name-lora\")\n",
      "\n",
      "\n",
      " Current VRAM usage: 1.61GB\n",
      "\n",
      " Model saving options completed!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Code Analysis:\n",
    "- Original uses Unsloth methods (save_pretrained_merged, push_to_hub_merged)\n",
    "- These methods don't exist on standard HuggingFace/PEFT models\n",
    "- Need to use standard HuggingFace and PEFT methods instead\n",
    "- Must handle merging LoRA weights manually if needed\n",
    "- All examples have if False - they're disabled templates\n",
    "\"\"\"\n",
    "import torch\n",
    "import os\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "print(\"Model Saving Options for Qwen2ForCausalLM with LoRA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ==============================================================================\n",
    "# OPTION 1: SAVE LORA ADAPTERS ONLY (RECOMMENDED - Small files)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"1 Saving LoRA adapters only...\")\n",
    "\n",
    "if True:  # Set to True to execute\n",
    "    # Save LoRA adapter weights (lightweight ~MB)\n",
    "    model.save_pretrained(\"model_lora\")\n",
    "    tokenizer.save_pretrained(\"model_lora\")\n",
    "    print(\" LoRA adapters saved to 'model_lora' folder\")\n",
    "    \n",
    "    # Check size\n",
    "    size_mb = sum(os.path.getsize(os.path.join(\"model_lora\", f)) \n",
    "                  for f in os.listdir(\"model_lora\")) / (1024*1024)\n",
    "    print(f\"   Size: {size_mb:.1f}MB\")\n",
    "\n",
    "# ==============================================================================\n",
    "# OPTION 2: MERGE AND SAVE TO 16-BIT (Large files ~3-6GB)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n2 Merge to 16-bit and save...\")\n",
    "\n",
    "if False:  # Set to True to execute (WARNING: Large files!)\n",
    "    try:\n",
    "        # Check if model has PEFT adapter to merge\n",
    "        if hasattr(model, 'merge_and_unload'):\n",
    "            print(\"Merging LoRA weights into base model...\")\n",
    "            merged_model = model.merge_and_unload()\n",
    "            \n",
    "            # Save merged model in 16-bit\n",
    "            merged_model.save_pretrained(\n",
    "                \"model_merged_16bit\",\n",
    "                torch_dtype=torch.float16,\n",
    "                safe_serialization=True,  # Use safetensors format\n",
    "            )\n",
    "            tokenizer.save_pretrained(\"model_merged_16bit\")\n",
    "            print(\" 16-bit merged model saved to 'model_merged_16bit'\")\n",
    "            \n",
    "        else:\n",
    "            print(\" No PEFT adapter found - saving current model as 16-bit\")\n",
    "            model.save_pretrained(\n",
    "                \"model_16bit\", \n",
    "                torch_dtype=torch.float16,\n",
    "                safe_serialization=True,\n",
    "            )\n",
    "            tokenizer.save_pretrained(\"model_16bit\")\n",
    "            print(\" 16-bit model saved to 'model_16bit'\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\" 16-bit save failed: {e}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# OPTION 3: MERGE AND SAVE TO 4-BIT (Smaller files ~1-2GB)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n3 Merge to 4-bit and save...\")\n",
    "\n",
    "if False:  # Set to True to execute\n",
    "    try:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        \n",
    "        # Merge LoRA if possible\n",
    "        if hasattr(model, 'merge_and_unload'):\n",
    "            merged_model = model.merge_and_unload()\n",
    "        else:\n",
    "            merged_model = model\n",
    "        \n",
    "        # Create quantization config\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        \n",
    "        # Note: 4-bit models are typically loaded with quantization, not saved\n",
    "        print(\" 4-bit quantization typically done at load time, not save time\")\n",
    "        print(\"   Use load_in_4bit=True when loading the 16-bit model instead\")\n",
    "        \n",
    "        # Save configuration for 4-bit loading\n",
    "        import json\n",
    "        config = {\n",
    "            \"quantization_method\": \"4bit\",\n",
    "            \"load_in_4bit\": True,\n",
    "            \"bnb_4bit_quant_type\": \"nf4\",\n",
    "            \"instructions\": \"Load this model with load_in_4bit=True\"\n",
    "        }\n",
    "        \n",
    "        os.makedirs(\"model_4bit_config\", exist_ok=True)\n",
    "        with open(\"model_4bit_config/quantization_config.json\", \"w\") as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        # Copy the 16-bit model (4-bit quantization happens at load time)\n",
    "        if os.path.exists(\"model_merged_16bit\"):\n",
    "            import shutil\n",
    "            shutil.copytree(\"model_merged_16bit\", \"model_4bit\", dirs_exist_ok=True)\n",
    "            shutil.copy(\"model_4bit_config/quantization_config.json\", \"model_4bit/\")\n",
    "            print(\" 4-bit loading configuration saved to 'model_4bit'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" 4-bit configuration failed: {e}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# OPTION 4: PUSH TO HUGGING FACE HUB\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n4 Push to Hugging Face Hub...\")\n",
    "\n",
    "if False:  # Set to True and add your token to execute\n",
    "    hf_token = \"hf_your_token_here\"  # Replace with your actual token\n",
    "    repo_name = \"your-username/your-model-name\"  # Replace with your repo\n",
    "    \n",
    "    try:\n",
    "        # Push LoRA adapters\n",
    "        model.push_to_hub(repo_name + \"-lora\", token=hf_token)\n",
    "        tokenizer.push_to_hub(repo_name + \"-lora\", token=hf_token)\n",
    "        print(f\" LoRA adapters pushed to {repo_name}-lora\")\n",
    "        \n",
    "        # Push merged model (if exists)\n",
    "        if os.path.exists(\"model_merged_16bit\"):\n",
    "            from transformers import AutoModelForCausalLM\n",
    "            merged_model = AutoModelForCausalLM.from_pretrained(\"model_merged_16bit\")\n",
    "            merged_tokenizer = AutoTokenizer.from_pretrained(\"model_merged_16bit\")\n",
    "            \n",
    "            merged_model.push_to_hub(repo_name + \"-merged\", token=hf_token)\n",
    "            merged_tokenizer.push_to_hub(repo_name + \"-merged\", token=hf_token)\n",
    "            print(f\" Merged model pushed to {repo_name}-merged\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\" Hub push failed: {e}\")\n",
    "        print(\"   Make sure to set a valid HF token and repo name\")\n",
    "\n",
    "# ==============================================================================\n",
    "# VERIFICATION: CHECK WHAT WAS SAVED\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" SAVED FILES VERIFICATION:\")\n",
    "\n",
    "folders_to_check = [\n",
    "    \"model_lora\", \n",
    "    \"model_merged_16bit\", \n",
    "    \"model_16bit\",\n",
    "    \"model_4bit\", \n",
    "    \"model_4bit_config\"\n",
    "]\n",
    "\n",
    "for folder in folders_to_check:\n",
    "    if os.path.exists(folder):\n",
    "        files = os.listdir(folder)\n",
    "        total_size = sum(os.path.getsize(os.path.join(folder, f)) for f in files)\n",
    "        size_mb = total_size / (1024*1024)\n",
    "        \n",
    "        print(f\"\\n {folder}:\")\n",
    "        print(f\"   Files: {len(files)}\")\n",
    "        print(f\"   Size: {size_mb:.1f}MB\")\n",
    "        \n",
    "        # Show key files\n",
    "        key_files = [f for f in files if f.endswith(('.bin', '.safetensors', '.json'))]\n",
    "        for file in key_files[:3]:  # Show first 3 important files\n",
    "            file_size = os.path.getsize(os.path.join(folder, file)) / (1024*1024)\n",
    "            print(f\"    {file} ({file_size:.1f}MB)\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"\\n {folder}: Not found\")\n",
    "\n",
    "# ==============================================================================\n",
    "# LOADING INSTRUCTIONS FOR SAVED MODELS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" LOADING INSTRUCTIONS:\")\n",
    "\n",
    "print(\"\"\"\n",
    " To load LoRA adapters:\n",
    "   from peft import PeftModel\n",
    "   base_model = AutoModelForCausalLM.from_pretrained(\"unsloth/Qwen2.5-1.5B-Instruct\")\n",
    "   model = PeftModel.from_pretrained(base_model, \"model_lora\")\n",
    "\n",
    " To load 16-bit merged model:\n",
    "   model = AutoModelForCausalLM.from_pretrained(\"model_merged_16bit\")\n",
    "\n",
    " To load with 4-bit quantization:\n",
    "   model = AutoModelForCausalLM.from_pretrained(\n",
    "       \"model_4bit\", \n",
    "       load_in_4bit=True,\n",
    "       device_map=\"auto\"\n",
    "   )\n",
    "\n",
    " From Hugging Face Hub:\n",
    "   model = AutoModelForCausalLM.from_pretrained(\"your-username/your-model-name-lora\")\n",
    "\"\"\")\n",
    "\n",
    "# Memory cleanup for RTX 2070 Super\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"\\n Current VRAM usage: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "print(\"\\n Model saving options completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyEjW-WuYQIm"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
