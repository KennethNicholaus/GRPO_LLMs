{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-xdnzlu7v/unsloth_2ebace502881401d96bb6149a6bbccd9\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-xdnzlu7v/unsloth_2ebace502881401d96bb6149a6bbccd9\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 5f14e6fcd405db038ec099c5fe8ba4a753c8472e\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: unsloth_zoo>=2025.8.9 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.8.9)\n",
      "Requirement already satisfied: packaging in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.0)\n",
      "Requirement already satisfied: tyro in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.9.31)\n",
      "Requirement already satisfied: transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.55.4)\n",
      "Requirement already satisfied: datasets<4.0.0,>=3.4.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.6.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.1)\n",
      "Requirement already satisfied: tqdm in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.67.1)\n",
      "Requirement already satisfied: psutil in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.45.1)\n",
      "Requirement already satisfied: numpy in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.6)\n",
      "Requirement already satisfied: protobuf in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.32.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.34.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.34.4)\n",
      "Requirement already satisfied: hf_transfer in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.9)\n",
      "Requirement already satisfied: bitsandbytes>=0.45.5 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.47.0)\n",
      "Requirement already satisfied: filelock in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.5)\n",
      "Requirement already satisfied: xxhash in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.15.0)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.7.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from triton==3.3.1->torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (78.1.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from huggingface_hub>=0.34.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.1.9)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.9.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6.2)\n",
      "Requirement already satisfied: torchao in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth_zoo>=2025.8.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.13.0)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth_zoo>=2025.8.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.10.1)\n",
      "Requirement already satisfied: trl!=0.15.0,!=0.19.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth_zoo>=2025.8.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.22.2)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth_zoo>=2025.8.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.17.1)\n",
      "Requirement already satisfied: cut_cross_entropy in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth_zoo>=2025.8.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.1.1)\n",
      "Requirement already satisfied: pillow in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth_zoo>=2025.8.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.3.0)\n",
      "Requirement already satisfied: msgspec in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth_zoo>=2025.8.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.19.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jinja2->torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.17.0)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.17.0)\n",
      "Requirement already satisfied: rich>=11.1.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (14.1.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.2)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.4.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
      "Requirement already satisfied: vllm in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (0.10.1.1)\n",
      "Requirement already satisfied: regex in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (2025.9.1)\n",
      "Requirement already satisfied: cachetools in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (6.2.0)\n",
      "Requirement already satisfied: psutil in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (5.9.0)\n",
      "Requirement already satisfied: sentencepiece in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.2.1)\n",
      "Requirement already satisfied: numpy in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (2.2.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (2.32.5)\n",
      "Requirement already satisfied: tqdm in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (4.67.1)\n",
      "Requirement already satisfied: blake3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (1.0.5)\n",
      "Requirement already satisfied: py-cpuinfo in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (9.0.0)\n",
      "Requirement already satisfied: transformers>=4.55.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (4.55.4)\n",
      "Requirement already satisfied: tokenizers>=0.21.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.21.4)\n",
      "Requirement already satisfied: protobuf in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (6.32.0)\n",
      "Requirement already satisfied: fastapi>=0.115.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.116.1)\n",
      "Requirement already satisfied: aiohttp in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (3.12.15)\n",
      "Requirement already satisfied: openai>=1.99.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (1.105.0)\n",
      "Requirement already satisfied: pydantic>=2.10 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (2.11.7)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.21.1)\n",
      "Requirement already satisfied: pillow in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (11.3.0)\n",
      "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (7.1.0)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.11.0)\n",
      "Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.11 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.10.12)\n",
      "Requirement already satisfied: llguidance<0.8.0,>=0.7.11 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.7.30)\n",
      "Requirement already satisfied: outlines_core==0.2.10 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.2.10)\n",
      "Requirement already satisfied: diskcache==5.6.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (5.6.3)\n",
      "Requirement already satisfied: lark==1.2.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (1.2.2)\n",
      "Requirement already satisfied: xgrammar==0.1.21 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.1.21)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (4.15.0)\n",
      "Requirement already satisfied: filelock>=3.16.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (3.19.1)\n",
      "Requirement already satisfied: partial-json-parser in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.2.1.1.post6)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (26.2.0)\n",
      "Requirement already satisfied: msgspec in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.19.0)\n",
      "Requirement already satisfied: gguf>=0.13.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.17.1)\n",
      "Requirement already satisfied: mistral_common>=1.8.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from mistral_common[audio,image]>=1.8.2->vllm) (1.8.4)\n",
      "Requirement already satisfied: opencv-python-headless>=4.11.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (4.12.0.88)\n",
      "Requirement already satisfied: pyyaml in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (6.0.2)\n",
      "Requirement already satisfied: einops in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.8.1)\n",
      "Requirement already satisfied: compressed-tensors==0.10.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.10.2)\n",
      "Requirement already satisfied: depyf==0.19.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.19.0)\n",
      "Requirement already satisfied: cloudpickle in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (3.1.1)\n",
      "Requirement already satisfied: watchfiles in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (1.1.0)\n",
      "Requirement already satisfied: python-json-logger in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (3.2.1)\n",
      "Requirement already satisfied: scipy in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (1.16.1)\n",
      "Requirement already satisfied: ninja in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (1.13.0)\n",
      "Requirement already satisfied: pybase64 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (1.4.2)\n",
      "Requirement already satisfied: cbor2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (5.7.0)\n",
      "Requirement already satisfied: setproctitle in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (1.3.6)\n",
      "Requirement already satisfied: openai-harmony>=0.0.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.0.4)\n",
      "Requirement already satisfied: numba==0.61.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.61.2)\n",
      "Requirement already satisfied: ray>=2.48.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from ray[cgraph]>=2.48.0->vllm) (2.49.1)\n",
      "Requirement already satisfied: torch==2.7.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (2.7.1)\n",
      "Requirement already satisfied: torchaudio==2.7.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (2.7.1)\n",
      "Requirement already satisfied: torchvision==0.22.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.22.1)\n",
      "Requirement already satisfied: xformers==0.0.31 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.0.31)\n",
      "Requirement already satisfied: astor in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from depyf==0.19.0->vllm) (0.8.1)\n",
      "Requirement already satisfied: dill in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from depyf==0.19.0->vllm) (0.3.8)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from numba==0.61.2->vllm) (0.44.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from triton==3.3.1->torch==2.7.1->vllm) (78.1.1)\n",
      "Requirement already satisfied: interegular>=0.3.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from lm-format-enforcer<0.11,>=0.10.11->vllm) (0.3.3)\n",
      "Requirement already satisfied: packaging in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from lm-format-enforcer<0.11,>=0.10.11->vllm) (25.0)\n",
      "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.47.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pydantic>=2.10->vllm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pydantic>=2.10->vllm) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pydantic>=2.10->vllm) (0.4.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from starlette<0.48.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (4.7.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (1.3.0)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.8 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.0.10)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi[standard]>=0.115.0->vllm) (2.3.0)\n",
      "Requirement already satisfied: uvicorn>=0.12.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.35.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.7.0)\n",
      "Requirement already satisfied: typer>=0.15.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.17.3)\n",
      "Requirement already satisfied: rich-toolkit>=0.14.8 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.15.0)\n",
      "Requirement already satisfied: fastapi-cloud-cli>=0.1.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.5)\n",
      "Requirement already satisfied: rignore>=0.5.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.6.4)\n",
      "Requirement already satisfied: sentry-sdk>=2.20.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.35.2)\n",
      "Requirement already satisfied: certifi in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jinja2->torch==2.7.1->vllm) (3.0.2)\n",
      "Requirement already satisfied: jsonschema>=4.21.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (4.25.0)\n",
      "Requirement already satisfied: pydantic-extra-types>=2.10.5 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2.10.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.22.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from openai>=1.99.1->vllm) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from openai>=1.99.1->vllm) (0.10.0)\n",
      "Requirement already satisfied: pycountry>=23 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (24.6.1)\n",
      "Requirement already satisfied: click>=7.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (8.2.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (1.1.1)\n",
      "Requirement already satisfied: cupy-cuda12x in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from ray[cgraph]>=2.48.0->vllm) (13.6.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from requests>=2.26.0->vllm) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from requests>=2.26.0->vllm) (2.5.0)\n",
      "Requirement already satisfied: rich>=13.7.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.1.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from sympy>=1.13.3->torch==2.7.1->vllm) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from tokenizers>=0.21.1->vllm) (0.34.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.21.1->vllm) (1.1.9)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from transformers>=4.55.0->vllm) (0.6.2)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.1.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp->vllm) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp->vllm) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp->vllm) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp->vllm) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp->vllm) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp->vllm) (1.20.1)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm) (0.8.3)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.13.1)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2.21)\n",
      "Requirement already satisfied: soxr>=0.5.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.5.0.post1)\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (0.22.1)\n",
      "Requirement already satisfied: torchaudio in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from triton==3.3.1->torch) (78.1.1)\n",
      "Requirement already satisfied: numpy in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Environment Detection & Package Installation\n",
    "\n",
    "Detects if code is running in Google Colab by checking environment variables\n",
    "Installs unsloth (efficient LLM fine-tuning library) and vllm (fast inference engine) if not in Colab\n",
    "Uses %%capture to suppress installation output\n",
    "Colab environments have different installation requirements handled separately\n",
    "'''\n",
    "#%%capture\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Check if we're in a special environment (Colab/Kaggle)\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()) and \"KAGGLE_\" not in \"\".join(os.environ.keys()):\n",
    "    # For local WSL2 environment - install with specific CUDA support\n",
    "    try:\n",
    "        # Install unsloth with CUDA support for RTX 2070 Super\n",
    "        !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "        \n",
    "        # Install vllm with CUDA 11.8/12.1 support (common for RTX 2070 Super)\n",
    "        !pip install vllm\n",
    "        \n",
    "        # Install additional dependencies for WSL2\n",
    "        !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Installation warning: {e}\")\n",
    "        print(\"Falling back to basic installation...\")\n",
    "        !pip install unsloth vllm\n",
    "else:\n",
    "    pass  # Special environment handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected RTX 2070 Super - installing optimized versions...\n",
      "  \u001b[31m×\u001b[0m No solution found when resolving dependencies:\n",
      "\u001b[31m  ╰─▶ \u001b[0mBecause torch==2.7.1 depends on triton{platform_machine == 'x86_64'\n",
      "\u001b[31m      \u001b[0mand sys_platform == 'linux'}==3.3.1 and vllm==0.10.1 depends\n",
      "\u001b[31m      \u001b[0mon torch==2.7.1, we can conclude that vllm==0.10.1 depends on\n",
      "\u001b[31m      \u001b[0mtriton==3.3.1.\n",
      "\u001b[31m      \u001b[0mAnd because you require vllm==0.10.1 and triton==3.2.0, we can conclude\n",
      "\u001b[31m      \u001b[0mthat your requirements are unsatisfiable.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Colab-Specific Installation with Hardware Detection\n",
    "\n",
    "Upgrades uv package manager for faster installations\n",
    "Detects GPU type (Tesla T4) to install compatible vllm/triton versions\n",
    "Handles numpy version compatibility by preserving existing version\n",
    "Installs core ML libraries: unsloth, vllm, transformers, bitsandbytes, xformers\n",
    "Forces specific transformers version (4.55.4) for compatibility\n",
    "'''\n",
    "# WSL2 Installation with RTX 2070 Super optimization\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Upgrade package manager\n",
    "!pip install --upgrade -q uv\n",
    "\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    try:\n",
    "        # Get current numpy version if available\n",
    "        try: \n",
    "            import numpy\n",
    "            get_numpy = f\"numpy=={numpy.__version__}\"\n",
    "        except: \n",
    "            get_numpy = \"numpy\"\n",
    "        \n",
    "        # Check CUDA capability for RTX 2070 Super (compute capability 7.5)\n",
    "        try:\n",
    "            gpu_info = str(subprocess.check_output([\"nvidia-smi\"], stderr=subprocess.DEVNULL))\n",
    "            is_rtx_2070 = \"RTX 2070\" in gpu_info or \"GeForce RTX\" in gpu_info\n",
    "        except:\n",
    "            is_rtx_2070 = False\n",
    "        \n",
    "        # Install with RTX 2070 Super compatible versions\n",
    "        if is_rtx_2070:\n",
    "            print(\"Detected RTX 2070 Super - installing optimized versions...\")\n",
    "            !uv pip install -q --upgrade \\\n",
    "                unsloth vllm==0.10.1 {get_numpy} torchvision \\\n",
    "                bitsandbytes xformers triton==3.2.0\n",
    "        else:\n",
    "            # Fallback installation\n",
    "            !uv pip install -q --upgrade \\\n",
    "                unsloth vllm {get_numpy} torchvision \\\n",
    "                bitsandbytes xformers triton\n",
    "                \n",
    "        # Install specific transformers version for compatibility\n",
    "        !uv pip install -q transformers==4.55.4\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Installation error: {e}\")\n",
    "        print(\"Falling back to basic pip installation...\")\n",
    "        !pip install unsloth vllm transformers==4.55.4\n",
    "else:\n",
    "    pass  # Colab environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIy3QkjW1O4R"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jN75nmdx9lvw"
   },
   "source": [
    "Goal: To convert `DeepSeek-R1-0528-Qwen3-8B` into a reasoning model via GRPO by using OpenR1's Math dataset.\n",
    "\n",
    "We also use `langid` for language detection. Our main goal is to force the model to generate reasoning traces in Indonesian, and we create a reward function using `langid` to check this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install language identification library\n",
    "'''\n",
    "Section: Language Identification Library Installation\n",
    "\n",
    "Installs langid package for automatic language detection\n",
    "Uses -qq flag for quiet installation (minimal output)\n",
    "Useful for identifying language of text data before processing\n",
    "'''\n",
    "!pip install langid -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 09-03 14:26:18 [__init__.py:241] Automatically detected platform cuda.\n",
      "ERROR 09-03 14:26:19 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "Loading model optimized for RTX 2070 Super...\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.8.10: Fast Qwen2 patching. Transformers: 4.55.4. vLLM: 0.10.1.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 2070 SUPER. Num GPUs = 1. Max memory: 8.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.8.10 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Model device: cuda:0\n",
      "VRAM allocated: 1.51GB\n",
      "VRAM reserved: 1.53GB\n",
      "VRAM free: 6.47GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nOther recommended models for RTX 2070 Super:\\n• unsloth/Llama-3.2-3B-Instruct - 3B parameters, good balance\\n• unsloth/Qwen2.5-3B-Instruct - 3B parameters, efficient\\n• unsloth/Phi-3.5-mini-instruct - 3.8B parameters, optimized\\n• microsoft/DialoGPT-medium - 355M parameters, very fast\\n\\nFor 7B+ models, you'll need CPU offloading or model parallelism\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Section: Model Loading and LoRA Configuration\n",
    "\n",
    "Imports FastLanguageModel from unsloth for efficient fine-tuning\n",
    "Sets sequence length to 128 tokens and LoRA rank to 8 for memory efficiency\n",
    "Loads DeepSeek-R1 8B model with 4-bit quantization to reduce VRAM usage\n",
    "Configures PEFT (Parameter Efficient Fine-Tuning) with LoRA adapters\n",
    "Targets attention and MLP layers for fine-tuning\n",
    "Uses gradient checkpointing to save memory during training\n",
    "'''\n",
    "'''\n",
    "Section: Model Loading Fix - Compatibility Issue Resolution\n",
    "\n",
    "Error occurs because fast_inference and trust_remote_code cannot be used together\n",
    "Need to choose between fast inference (vLLM) or remote code execution\n",
    "For fine-tuning, disable fast_inference; enable it later for inference only\n",
    "'''\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear any existing GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# SOLUTION 1: Use smaller model (RECOMMENDED)\n",
    "# Conservative settings optimized for RTX 2070 Super (8GB VRAM)\n",
    "max_seq_length = 512      # Increased since using smaller model\n",
    "lora_rank = 16            # Can increase with smaller model\n",
    "dtype = None              # Auto-detect optimal dtype\n",
    "\n",
    "print(\"Loading model optimized for RTX 2070 Super...\")\n",
    "\n",
    "try:\n",
    "    # Option A: Use smaller 1.5B model (RECOMMENDED)\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"unsloth/Qwen2.5-1.5B-Instruct\",  # Much smaller model\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = True,\n",
    "        device_map = {\"\": 0},           # Force everything on GPU 0\n",
    "        gpu_memory_utilization = 0.85,  # Can be more aggressive with smaller model\n",
    "        trust_remote_code = True,\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to load 1.5B model: {e}\")\n",
    "    print(\"Trying alternative approach...\")\n",
    "    \n",
    "    # Option B: Original model with CPU offloading (FALLBACK)\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"unsloth/DeepSeek-R1-0528-Qwen3-8B\",\n",
    "        max_seq_length = max_seq_length,           # Reduced further\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = True,\n",
    "        llm_int8_enable_fp32_cpu_offload = True,  # Enable CPU offloading\n",
    "        device_map = \"auto\",            # Auto-distribute across GPU/CPU\n",
    "        gpu_memory_utilization = 0.7,\n",
    "        trust_remote_code = True,\n",
    "    )\n",
    "\n",
    "# Configure LoRA for optimal memory usage\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank * 2,\n",
    "    lora_dropout = 0.1,                # Slightly higher for regularization\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    ")\n",
    "\n",
    "# Memory status\n",
    "print(f\"Model loaded successfully!\")\n",
    "if hasattr(model, 'device'):\n",
    "    print(f\"Model device: {model.device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"VRAM allocated: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "    print(f\"VRAM reserved: {torch.cuda.memory_reserved()/1024**3:.2f}GB\")\n",
    "    print(f\"VRAM free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved())/1024**3:.2f}GB\")\n",
    "\n",
    "# SOLUTION 2: Alternative smaller models for your GPU\n",
    "\"\"\"\n",
    "Other recommended models for RTX 2070 Super:\n",
    "• unsloth/Llama-3.2-3B-Instruct - 3B parameters, good balance\n",
    "• unsloth/Qwen2.5-3B-Instruct - 3B parameters, efficient\n",
    "• unsloth/Phi-3.5-mini-instruct - 3.8B parameters, optimized\n",
    "• microsoft/DialoGPT-medium - 355M parameters, very fast\n",
    "\n",
    "For 7B+ models, you'll need CPU offloading or model parallelism\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IodK13om1O4S"
   },
   "source": [
    "### GRPO Chat Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cGSRTJo1O4T"
   },
   "source": [
    "Distill Qwen3 from Deepseek has a chat template that is used to format the input and output of the model. This is used to make the model output in a chat format. Including the reasoning step. We have to use that chat template since the model is trained using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEcLdymBEHdk"
   },
   "source": [
    "Let's see how our chat template behaves on an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for special tokens...\n",
      "Found 22 special tokens\n",
      "\n",
      "Token Summary:\n",
      "Reasoning start: None\n",
      "Reasoning end: None\n",
      "User token: None\n",
      "Assistant token: None\n",
      "\n",
      "System prompt configured:\n",
      "You are given a problem.\n",
      "Think about the problem and provide your working out.\n",
      "You must think in Bahasa Indonesia.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Special Token Detection and System Prompt Setup\n",
    "\n",
    "Scans tokenizer's special vocabulary for reasoning and role tokens\n",
    "Identifies thinking tokens (start/end) for Chain-of-Thought reasoning\n",
    "Finds user and assistant tokens for conversation formatting\n",
    "Creates system prompt requiring reasoning in Bahasa Indonesia\n",
    "Sets up structured thinking framework for the model\n",
    "'''\n",
    "# Initialize token variables\n",
    "reasoning_start = None\n",
    "reasoning_end = None\n",
    "user_token = None\n",
    "assistant_token = None\n",
    "\n",
    "# Scan for special tokens in vocabulary\n",
    "print(\"Scanning for special tokens...\")\n",
    "added_vocab = tokenizer.get_added_vocab()\n",
    "print(f\"Found {len(added_vocab)} special tokens\")\n",
    "\n",
    "for token in added_vocab.keys():\n",
    "    if \"think\" in token.lower() and \"/\" in token:\n",
    "        reasoning_end = token\n",
    "        print(f\"Found reasoning end token: {token}\")\n",
    "    elif \"think\" in token.lower():\n",
    "        reasoning_start = token\n",
    "        print(f\"Found reasoning start token: {token}\")\n",
    "    elif \"user\" in token.lower():\n",
    "        user_token = token\n",
    "        print(f\"Found user token: {token}\")\n",
    "    elif \"assistant\" in token.lower():\n",
    "        assistant_token = token\n",
    "        print(f\"Found assistant token: {token}\")\n",
    "\n",
    "# Display found tokens\n",
    "print(f\"\\nToken Summary:\")\n",
    "print(f\"Reasoning start: {reasoning_start}\")\n",
    "print(f\"Reasoning end: {reasoning_end}\")\n",
    "print(f\"User token: {user_token}\")\n",
    "print(f\"Assistant token: {assistant_token}\")\n",
    "\n",
    "# System prompt for Indonesian reasoning\n",
    "system_prompt = f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your working out.\n",
    "You must think in Bahasa Indonesia.\"\"\"\n",
    "\n",
    "print(f\"\\nSystem prompt configured:\")\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing chat template formatting...\n",
      "==================================================\n",
      "Formatted conversation:\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is 1+1?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>I think it's 2.2</think>2<|im_end|>\n",
      "<|im_start|>user\n",
      "What is 1+1?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>I think it's 2.2</think>2<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "==================================================\n",
      "\n",
      "No reasoning tokens detected - using generic <think> tags\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Chat Template Testing and Format Verification\n",
    "\n",
    "Tests tokenizer's chat template formatting with sample conversation\n",
    "Shows how reasoning tokens (<think>) are integrated into responses\n",
    "Demonstrates multi-turn conversation structure\n",
    "Uses add_generation_prompt=True to prepare for model generation\n",
    "Verifies proper formatting before training/inference\n",
    "'''\n",
    "# Test chat template with reasoning tokens\n",
    "print(\"Testing chat template formatting...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sample_conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"What is 1+1?\"},\n",
    "    {\"role\": \"assistant\", \"content\": f\"<think>I think it's 2.2</think>2\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is 1+1?\"},\n",
    "    {\"role\": \"assistant\", \"content\": f\"<think>I think it's 2.2</think>2\"},\n",
    "]\n",
    "\n",
    "# Apply chat template and display\n",
    "formatted_chat = tokenizer.apply_chat_template(\n",
    "    sample_conversation,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(\"Formatted conversation:\")\n",
    "print(formatted_chat)\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if reasoning tokens are properly handled\n",
    "if reasoning_start and reasoning_end:\n",
    "    print(f\"\\nReasoning tokens detected:\")\n",
    "    print(f\"Start: {reasoning_start}\")\n",
    "    print(f\"End: {reasoning_end}\")\n",
    "    \n",
    "    # Test with detected tokens if available\n",
    "    test_with_detected_tokens = [\n",
    "        {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"{reasoning_start}Let me calculate: 2+2 = 4{reasoning_end}The answer is 4.\"},\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nWith detected reasoning tokens:\")\n",
    "    formatted_with_tokens = tokenizer.apply_chat_template(\n",
    "        test_with_detected_tokens,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    print(formatted_with_tokens)\n",
    "else:\n",
    "    print(\"\\nNo reasoning tokens detected - using generic <think> tags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KGgPgk_5S8r"
   },
   "source": [
    "### Data Prep\n",
    "<a name=\"Data\"></a>\n",
    "\n",
    "We're using Hugging Face's [Open R1 Math dataset](https://huggingface.co/datasets/open-r1/DAPO-Math-17k-Processed). You can also utilize OpenAI's famous [GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DAPO-Math-17k dataset...\n",
      "Dataset loaded successfully!\n",
      "Dataset size: 14116 examples\n",
      "Dataset features: {'prompt': Value(dtype='string', id=None), 'solution': Value(dtype='string', id=None), 'data_source': Value(dtype='string', id=None), 'source_prompt': [{'content': Value(dtype='string', id=None), 'role': Value(dtype='string', id=None)}], 'ability': Value(dtype='string', id=None), 'reward_model': {'ground_truth': Value(dtype='string', id=None), 'style': Value(dtype='string', id=None)}, 'extra_info': {'index': Value(dtype='string', id=None)}}\n",
      "\n",
      "Sample entry:\n",
      "Keys: ['prompt', 'solution', 'data_source', 'source_prompt', 'ability', 'reward_model', 'extra_info']\n",
      "prompt: In triangle $ABC$, $\\sin \\angle A = \\frac{4}{5}$ and $\\angle A < 90^\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\angle BAD = \\angle DAC$ and $\\angle BDC = 90^\\circ$. Suppose that $AD ...\n",
      "solution: 34\n",
      "data_source: math_dapo\n",
      "source_prompt: [{'content': 'Solve the following math problem step by step. The last line of your response should be of the form Answer: $Answer (without quotes) where $Answer is the answer to the problem.\\n\\nIn tri...\n",
      "ability: MATH\n",
      "reward_model: {'ground_truth': '34', 'style': 'rule-lighteval/MATH_v2'}\n",
      "extra_info: {'index': '9a9b6eb4-a1cb-49d1-8c1e-62eaf2f74079'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'solution', 'data_source', 'source_prompt', 'ability', 'reward_model', 'extra_info'],\n",
       "    num_rows: 14116\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Section: Dataset Loading from Hugging Face Hub\n",
    "\n",
    "Loads DAPO-Math-17k-Processed dataset from Hugging Face\n",
    "Specifically loads English (\"en\") subset of the dataset\n",
    "Uses training split containing 17k processed math problems\n",
    "Dataset contains mathematical reasoning examples for fine-tuning\n",
    "'''\n",
    "from datasets import load_dataset\n",
    "import gc\n",
    "\n",
    "print(\"Loading DAPO-Math-17k dataset...\")\n",
    "\n",
    "try:\n",
    "    # Load dataset with memory optimization\n",
    "    dataset = load_dataset(\n",
    "        \"open-r1/DAPO-Math-17k-Processed\", \n",
    "        \"en\", \n",
    "        split=\"train\",\n",
    "        streaming=False,  # Load full dataset for training\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset loaded successfully!\")\n",
    "    print(f\"Dataset size: {len(dataset)} examples\")\n",
    "    print(f\"Dataset features: {dataset.features}\")\n",
    "    \n",
    "    # Display sample to verify format\n",
    "    if len(dataset) > 0:\n",
    "        print(f\"\\nSample entry:\")\n",
    "        print(f\"Keys: {list(dataset[0].keys())}\")\n",
    "        for key, value in dataset[0].items():\n",
    "            print(f\"{key}: {str(value)[:200]}{'...' if len(str(value)) > 200 else ''}\")\n",
    "    \n",
    "    # Memory cleanup\n",
    "    gc.collect()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Attempting alternative loading method...\")\n",
    "    \n",
    "    # Fallback method\n",
    "    try:\n",
    "        dataset = load_dataset(\"open-r1/DAPO-Math-17k-Processed\", split=\"train[:1000]\")\n",
    "        print(f\"Loaded subset: {len(dataset)} examples\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Fallback failed: {e2}\")\n",
    "        dataset = None\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b00gUsS-ROW"
   },
   "source": [
    "Let's look at the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "siopxjG8-ReF",
    "outputId": "0741a0d3-5771-44b9-bb84-80daf02f86a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In triangle $ABC$, $\\\\sin \\\\angle A = \\\\frac{4}{5}$ and $\\\\angle A < 90^\\\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\\\angle BAD = \\\\angle DAC$ and $\\\\angle BDC = 90^\\\\circ$. Suppose that $AD = 1$ and that $\\\\frac{BD}{CD} = \\\\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\\\frac{a\\\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "KGupRQqD-Wcf",
    "outputId": "e8f5aecd-ea06-4dc7-8eb3-c5b5741e2285"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'34'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"solution\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmnXj6hn-Ydi"
   },
   "source": [
    "In GSM8K, ee notice all answers like about have a ####, so we extract it. But for the Open R1 dataset, we can skip the below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing answer extraction function...\n",
      "==================================================\n",
      "Original solution (first 300 chars):\n",
      "34\n",
      "\n",
      "Extracted result:\n",
      "34\n",
      "\n",
      "Dataset analysis (first 100 examples):\n",
      "Examples with '####' pattern: 0/100\n",
      "No '####' pattern found - current logic is appropriate\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Answer Extraction Function\n",
    "\n",
    "Defines function to extract final answers from solution text\n",
    "Originally designed to parse answers after \"####\" delimiter\n",
    "Currently returns full text without extraction (commented out logic)\n",
    "Tests function on first dataset example to verify format\n",
    "'''\n",
    "'''\n",
    "Section: Answer Extraction Function - Fixed Dataset Iteration\n",
    "\n",
    "Error occurs because dataset items are accessed as dictionaries but treated as objects\n",
    "Need to use dictionary key access instead of .get() method\n",
    "Fixed iteration over dataset to properly access solution field\n",
    "'''\n",
    "def extract_hash_answer(text):\n",
    "    \"\"\"\n",
    "    Extract final answer from solution text.\n",
    "    Originally designed for #### delimiter format.\n",
    "    \"\"\"\n",
    "    # Original logic (commented out):\n",
    "    # if \"####\" not in text: return None\n",
    "    # return text.split(\"####\")[1].strip()\n",
    "    \n",
    "    # Currently returns full text\n",
    "    return text\n",
    "\n",
    "# Test extraction function on dataset sample\n",
    "print(\"Testing answer extraction function...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if dataset and len(dataset) > 0:\n",
    "    sample_solution = dataset[0][\"solution\"]\n",
    "    print(f\"Original solution (first 300 chars):\")\n",
    "    print(f\"{sample_solution[:300]}{'...' if len(sample_solution) > 300 else ''}\")\n",
    "    \n",
    "    extracted = extract_hash_answer(sample_solution)\n",
    "    print(f\"\\nExtracted result:\")\n",
    "    print(f\"{extracted[:300]}{'...' if len(extracted) > 300 else ''}\")\n",
    "    \n",
    "    # Fixed: Check if #### pattern exists in dataset\n",
    "    hash_count = 0\n",
    "    sample_size = min(100, len(dataset))\n",
    "    \n",
    "    for i in range(sample_size):\n",
    "        try:\n",
    "            solution = dataset[i][\"solution\"]\n",
    "            if \"####\" in str(solution):\n",
    "                hash_count += 1\n",
    "        except (KeyError, TypeError):\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nDataset analysis (first {sample_size} examples):\")\n",
    "    print(f\"Examples with '####' pattern: {hash_count}/{sample_size}\")\n",
    "    \n",
    "    if hash_count > 0:\n",
    "        print(\"Consider enabling hash extraction logic\")\n",
    "        # Show example with #### pattern\n",
    "        for i in range(min(10, len(dataset))):\n",
    "            if \"####\" in str(dataset[i][\"solution\"]):\n",
    "                print(f\"Example with ####: ...{dataset[i]['solution'][-100:]}\")\n",
    "                break\n",
    "    else:\n",
    "        print(\"No '####' pattern found - current logic is appropriate\")\n",
    "else:\n",
    "    print(\"No dataset available for testing\")\n",
    "    \n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K30CygaU-dir"
   },
   "source": [
    "Let's map the dataset! and see the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming dataset to chat format...\n",
      "==================================================\n",
      "Processing 14116 examples...\n",
      "Dataset transformation completed!\n",
      "Transformed dataset size: 14116\n",
      "\n",
      "Sample transformed item:\n",
      "Prompt structure:\n",
      "  1. Role: system\n",
      "     Content: You are given a problem.\n",
      "Think about the problem and provide your working out.\n",
      "You must think in Bahasa Indonesia.\n",
      "  2. Role: user\n",
      "     Content: In triangle $ABC$, $\\sin \\angle A = \\frac{4}{5}$ and $\\angle A < 90^\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\angle BAD = \\angle D...\n",
      "\n",
      "Answer preview:\n",
      "34\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Dataset Transformation and Formatting\n",
    "\n",
    "Maps dataset to chat format with system/user role structure\n",
    "Combines system prompt with user prompts from dataset\n",
    "Extracts answers from solutions using previously defined function\n",
    "Transforms raw dataset into conversation format suitable for fine-tuning\n",
    "Creates structured prompt-answer pairs for training\n",
    "'''\n",
    "print(\"Transforming dataset to chat format...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def transform_dataset_item(x):\n",
    "    \"\"\"Transform single dataset item to chat format\"\"\"\n",
    "    try:\n",
    "        return {\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": x[\"prompt\"]},\n",
    "            ],\n",
    "            \"answer\": extract_hash_answer(x[\"solution\"]),\n",
    "        }\n",
    "    except KeyError as e:\n",
    "        print(f\"Warning: Missing key {e} in dataset item\")\n",
    "        return {\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": \"\"},\n",
    "            ],\n",
    "            \"answer\": \"\",\n",
    "        }\n",
    "\n",
    "# Apply transformation with progress tracking\n",
    "try:\n",
    "    print(f\"Processing {len(dataset)} examples...\")\n",
    "    dataset = dataset.map(\n",
    "        transform_dataset_item,\n",
    "        desc=\"Transforming dataset\",\n",
    "        num_proc=1,  # Single process for WSL2 stability\n",
    "    )\n",
    "    \n",
    "    print(\"Dataset transformation completed!\")\n",
    "    print(f\"Transformed dataset size: {len(dataset)}\")\n",
    "    \n",
    "    # Display sample transformed item\n",
    "    if len(dataset) > 0:\n",
    "        print(f\"\\nSample transformed item:\")\n",
    "        sample_item = dataset[0]\n",
    "        \n",
    "        print(f\"Prompt structure:\")\n",
    "        for i, msg in enumerate(sample_item[\"prompt\"]):\n",
    "            print(f\"  {i+1}. Role: {msg['role']}\")\n",
    "            content_preview = msg['content'][:150] + \"...\" if len(msg['content']) > 150 else msg['content']\n",
    "            print(f\"     Content: {content_preview}\")\n",
    "        \n",
    "        print(f\"\\nAnswer preview:\")\n",
    "        answer_preview = sample_item[\"answer\"][:200] + \"...\" if len(sample_item[\"answer\"]) > 200 else sample_item[\"answer\"]\n",
    "        print(f\"{answer_preview}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during transformation: {e}\")\n",
    "    print(\"Dataset transformation failed\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9m8eR9T-gMh"
   },
   "source": [
    "We create a regex format to match the reasoning sections and answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up solution end regex pattern...\n",
      "==================================================\n",
      "Warning: No reasoning end token detected!\n",
      "Creating fallback regex pattern...\n",
      "Fallback regex pattern: </think>(.*)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Regex Pattern for Solution End Matching\n",
    "\n",
    "Imports regex module for pattern matching\n",
    "Creates pattern to match text after reasoning end token\n",
    "Uses DOTALL flag to match across multiple lines including newlines\n",
    "Designed to extract final answer portion after reasoning concludes\n",
    "Compiles regex for efficient repeated matching operations\n",
    "'''\n",
    "import re\n",
    "\n",
    "print(\"Setting up solution end regex pattern...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if reasoning end token was found\n",
    "if reasoning_end is not None:\n",
    "    # Create regex pattern to match content after reasoning end token\n",
    "    solution_end_regex = rf\"{re.escape(reasoning_end)}(.*)\"\n",
    "    \n",
    "    # Compile regex with DOTALL flag for multiline matching\n",
    "    match_format = re.compile(solution_end_regex, re.DOTALL)\n",
    "    \n",
    "    print(f\"Reasoning end token: {reasoning_end}\")\n",
    "    print(f\"Regex pattern: {solution_end_regex}\")\n",
    "    print(f\"Compiled regex: {match_format}\")\n",
    "    \n",
    "    # Test the regex pattern with sample text\n",
    "    test_text = f\"Some reasoning here {reasoning_end} Final answer: 42\"\n",
    "    test_match = match_format.search(test_text)\n",
    "    \n",
    "    if test_match:\n",
    "        print(f\"\\nRegex test successful:\")\n",
    "        print(f\"Matched text: '{test_match.group(1).strip()}'\")\n",
    "    else:\n",
    "        print(f\"\\nRegex test failed - no match found\")\n",
    "        \n",
    "else:\n",
    "    print(\"Warning: No reasoning end token detected!\")\n",
    "    print(\"Creating fallback regex pattern...\")\n",
    "    \n",
    "    # Fallback pattern using generic </think> tag\n",
    "    solution_end_regex = r\"</think>(.*)\"\n",
    "    match_format = re.compile(solution_end_regex, re.DOTALL)\n",
    "    print(f\"Fallback regex pattern: {solution_end_regex}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OycMneOq-iNC"
   },
   "source": [
    "We verify it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing regex pattern with sample text...\n",
      "==================================================\n",
      "Test 1:\n",
      "Input: Let me think!</think>Hence, the solution is 2.\n",
      "Matches found: 1\n",
      "  Match 1: 'Hence, the solution is 2.'\n",
      "------------------------------\n",
      "Test 2:\n",
      "Input: Let me think!</think>Hence, the solution is 2.\n",
      "Matches found: 1\n",
      "  Match 1: 'Hence, the solution is 2.'\n",
      "------------------------------\n",
      "Test 3:\n",
      "Input: Complex reasoning here</think>\n",
      "Hence, the solution is 42.\n",
      "This is the final answer.\n",
      "Matches found: 1\n",
      "  Match 1: 'Hence, the solution is 42.\n",
      "This is the final answer.'\n",
      "------------------------------\n",
      "Test 4:\n",
      "Input: Just some text without reasoning tokens.\n",
      "No matches found\n",
      "------------------------------\n",
      "\n",
      "Original test result:\n",
      "Input: Let me think!</think>Hence, the solution is 2.\n",
      "Matches: ['Hence, the solution is 2.']\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Regex Pattern Testing with Sample Text\n",
    "\n",
    "Tests the compiled regex pattern with sample reasoning text\n",
    "Uses findall() to extract all matches of content after reasoning end token\n",
    "Sample text simulates model output with thinking and final answer\n",
    "Validates that regex correctly captures final answer portion\n",
    "'''\n",
    "print(\"Testing regex pattern with sample text...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test sample with different scenarios\n",
    "test_samples = [\n",
    "    # Original test case\n",
    "    \"Let me think!</think>Hence, the solution is 2.\",\n",
    "    \n",
    "    # Test with detected reasoning end token if available\n",
    "    f\"Let me think!{reasoning_end}Hence, the solution is 2.\" if reasoning_end else \"Let me think!</think>Hence, the solution is 2.\",\n",
    "    \n",
    "    # Multi-line test\n",
    "    f\"Complex reasoning here{reasoning_end}\\nHence, the solution is 42.\\nThis is the final answer.\" if reasoning_end else \"Complex reasoning here</think>\\nHence, the solution is 42.\\nThis is the final answer.\",\n",
    "    \n",
    "    # No match test\n",
    "    \"Just some text without reasoning tokens.\"\n",
    "]\n",
    "\n",
    "for i, test_text in enumerate(test_samples, 1):\n",
    "    print(f\"Test {i}:\")\n",
    "    print(f\"Input: {test_text[:100]}{'...' if len(test_text) > 100 else ''}\")\n",
    "    \n",
    "    # Use findall to get all matches\n",
    "    matches = match_format.findall(test_text)\n",
    "    \n",
    "    if matches:\n",
    "        print(f\"Matches found: {len(matches)}\")\n",
    "        for j, match in enumerate(matches):\n",
    "            cleaned_match = match.strip()\n",
    "            print(f\"  Match {j+1}: '{cleaned_match[:150]}{'...' if len(cleaned_match) > 150 else ''}'\")\n",
    "    else:\n",
    "        print(\"No matches found\")\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# Test the original example specifically\n",
    "original_test = \"Let me think!</think>Hence, the solution is 2.\"\n",
    "original_matches = match_format.findall(original_test)\n",
    "\n",
    "print(f\"\\nOriginal test result:\")\n",
    "print(f\"Input: {original_test}\")\n",
    "print(f\"Matches: {original_matches}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing regex with proper think tag format...\n",
      "==================================================\n",
      "Test input:\n",
      "'<think>Let me think!</think>\n",
      "\n",
      "Hence, the solution is 2'\n",
      "\n",
      "Regex pattern used: </think>(.*)\n",
      "Matches found: 1\n",
      "Match 1: 'Hence, the solution is 2'\n",
      "Raw match (with whitespace): '\\n\\nHence, the solution is 2'\n",
      "\n",
      "Additional test cases:\n",
      "------------------------------\n",
      "Test 1: <think>Complex calculation</think>The answer is 42\n",
      "Result: ['The answer is 42']\n",
      "\n",
      "Test 2: <think>Working step by step</think>\n",
      "\n",
      "Step 1: Calculate\n",
      "Step ...\n",
      "Result: ['\\n\\nStep 1: Calculate\\nStep 2: Verify\\nFinal answer: 100']\n",
      "\n",
      "Test 3: <think></think>Direct answer: 5\n",
      "Result: ['Direct answer: 5']\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Regex Pattern Testing with Proper Think Tags\n",
    "\n",
    "Tests regex with properly formatted thinking tags (<think></think>)\n",
    "Includes newlines between reasoning and final answer sections\n",
    "Verifies pattern matching works with realistic model output format\n",
    "Demonstrates extraction of solution text after reasoning completion\n",
    "'''\n",
    "print(\"Testing regex with proper think tag format...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test the specific example with proper formatting\n",
    "test_input = \"<think>Let me think!</think>\\n\\nHence, the solution is 2\"\n",
    "print(f\"Test input:\")\n",
    "print(f\"'{test_input}'\")\n",
    "print()\n",
    "\n",
    "# Apply findall to extract matches\n",
    "matches = match_format.findall(test_input)\n",
    "\n",
    "print(f\"Regex pattern used: {match_format.pattern}\")\n",
    "print(f\"Matches found: {len(matches)}\")\n",
    "\n",
    "if matches:\n",
    "    for i, match in enumerate(matches):\n",
    "        # Clean up whitespace for display\n",
    "        cleaned_match = match.strip()\n",
    "        print(f\"Match {i+1}: '{cleaned_match}'\")\n",
    "        \n",
    "        # Show raw match with whitespace visible\n",
    "        print(f\"Raw match (with whitespace): {repr(match)}\")\n",
    "else:\n",
    "    print(\"No matches found\")\n",
    "\n",
    "# Additional test cases with variations\n",
    "additional_tests = [\n",
    "    # With reasoning_end token if detected\n",
    "    f\"<think>Complex calculation</think>{reasoning_end}The answer is 42\" if reasoning_end else \"<think>Complex calculation</think>The answer is 42\",\n",
    "    \n",
    "    # Multi-line answer\n",
    "    \"<think>Working step by step</think>\\n\\nStep 1: Calculate\\nStep 2: Verify\\nFinal answer: 100\",\n",
    "    \n",
    "    # Empty reasoning\n",
    "    \"<think></think>Direct answer: 5\"\n",
    "]\n",
    "\n",
    "print(\"\\nAdditional test cases:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for i, test_case in enumerate(additional_tests, 1):\n",
    "    print(f\"Test {i}: {test_case[:60]}{'...' if len(test_case) > 60 else ''}\")\n",
    "    result = match_format.findall(test_case)\n",
    "    print(f\"Result: {result}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weOjmO5l-kl3"
   },
   "source": [
    "We now want to create a reward function to match the format exactly - we reward it with 3 points if it succeeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing format scoring function...\n",
      "==================================================\n",
      "Warning: Error processing completion 0: list indices must be integers or slices, not str\n",
      "Warning: Error processing completion 1: list indices must be integers or slices, not str\n",
      "Warning: Error processing completion 2: list indices must be integers or slices, not str\n",
      "Test scores: [0, 0, 0]\n",
      "Error testing scoring function: list indices must be integers or slices, not str\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Format Matching Scoring Function\n",
    "\n",
    "Defines scoring function for evaluating completion quality\n",
    "Awards 3.0 points if regex pattern is found in model response\n",
    "Used for ranking/filtering model outputs during inference\n",
    "Ensures responses follow expected reasoning-to-answer format\n",
    "Returns list of scores corresponding to input completions\n",
    "'''\n",
    "def match_format_exactly(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Score completions based on format matching.\n",
    "    Awards points for proper reasoning-to-answer structure.\n",
    "    \n",
    "    Args:\n",
    "        completions: List of completion objects with content\n",
    "        **kwargs: Additional arguments (unused but maintained for compatibility)\n",
    "    \n",
    "    Returns:\n",
    "        List of scores for each completion\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for i, completion in enumerate(completions):\n",
    "        score = 0\n",
    "        \n",
    "        try:\n",
    "            # Extract response content\n",
    "            response = completion[0][\"content\"]\n",
    "            \n",
    "            # Check if response matches expected format\n",
    "            if match_format.search(response) is not None:\n",
    "                score += 3.0\n",
    "                # Optional: Add debug info\n",
    "                # print(f\"Completion {i}: Format match found (+3.0)\")\n",
    "            \n",
    "        except (IndexError, KeyError, TypeError) as e:\n",
    "            # Handle malformed completion objects\n",
    "            print(f\"Warning: Error processing completion {i}: {e}\")\n",
    "            score = 0\n",
    "        \n",
    "        scores.append(score)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Test the scoring function\n",
    "print(\"Testing format scoring function...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create mock completions for testing - FIXED SYNTAX\n",
    "test_completions = [\n",
    "    # Good format - should score 3.0\n",
    "    [[{\"content\": f\"<think>Let me calculate</think>{reasoning_end if reasoning_end else '</think>'}The answer is 42\"}]],\n",
    "    \n",
    "    # Bad format - should score 0.0 - FIXED\n",
    "    [[{\"content\": \"Just a direct answer without reasoning\"}]],\n",
    "    \n",
    "    # Another good format\n",
    "    [[{\"content\": \"<think>Step by step</think>\\n\\nFinal result: 100\"}]],\n",
    "]\n",
    "\n",
    "# Test scoring\n",
    "try:\n",
    "    test_scores = match_format_exactly(test_completions)\n",
    "    print(f\"Test scores: {test_scores}\")\n",
    "    \n",
    "    for i, (completion, score) in enumerate(zip(test_completions, test_scores)):\n",
    "        content = completion[0][\"content\"][:50] + \"...\" if len(completion[0][\"content\"]) > 50 else completion[0][\"content\"]\n",
    "        print(f\"Completion {i}: '{content}' -> Score = {score}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error testing scoring function: {e}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gf69i2WT-m4K"
   },
   "source": [
    "If it fails, we want to reward the model if it at least follows the format partially, by counting each symbol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing approximate format scoring function...\n",
      "==================================================\n",
      "Warning: Error processing completion 0: list indices must be integers or slices, not str\n",
      "Warning: Error processing completion 1: list indices must be integers or slices, not str\n",
      "Warning: Error processing completion 2: list indices must be integers or slices, not str\n",
      "Warning: Error processing completion 3: list indices must be integers or slices, not str\n",
      "Test scores: [-2.0, -2.0, -2.0, -2.0]\n",
      "Expected: [1.0, -0.5, -2.0, -2.0] (approximately)\n",
      "Error testing approximate scoring: list indices must be integers or slices, not str\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Approximate Format Matching Scoring Function\n",
    "\n",
    "Scores completions based on proper reasoning token usage\n",
    "Awards +0.5 for exactly one occurrence of reasoning start/end tokens\n",
    "Penalizes with -1.0 for incorrect token counts (0 or multiple occurrences)\n",
    "More lenient than exact matching but ensures proper token structure\n",
    "Prevents malformed reasoning sections with missing or duplicated tokens\n",
    "'''\n",
    "def match_format_approximately(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Score completions based on approximate format matching.\n",
    "    Rewards proper reasoning token usage and penalizes malformed structure.\n",
    "    \n",
    "    Args:\n",
    "        completions: List of completion objects with content\n",
    "        **kwargs: Additional arguments (unused but maintained for compatibility)\n",
    "    \n",
    "    Returns:\n",
    "        List of scores for each completion\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for i, completion in enumerate(completions):\n",
    "        score = 0\n",
    "        \n",
    "        try:\n",
    "            response = completion[0][\"content\"]\n",
    "            \n",
    "            # Check reasoning start token count\n",
    "            if reasoning_start:\n",
    "                start_count = response.count(reasoning_start)\n",
    "                if start_count == 1:\n",
    "                    score += 0.5\n",
    "                else:\n",
    "                    score -= 1.0\n",
    "                    # Optional debug: print(f\"Start token count: {start_count}\")\n",
    "            \n",
    "            # Check reasoning end token count  \n",
    "            if reasoning_end:\n",
    "                end_count = response.count(reasoning_end)\n",
    "                if end_count == 1:\n",
    "                    score += 0.5\n",
    "                else:\n",
    "                    score -= 1.0\n",
    "                    # Optional debug: print(f\"End token count: {end_count}\")\n",
    "            \n",
    "        except (IndexError, KeyError, TypeError) as e:\n",
    "            print(f\"Warning: Error processing completion {i}: {e}\")\n",
    "            score = -2.0  # Heavy penalty for malformed input\n",
    "        \n",
    "        scores.append(score)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Test the approximate scoring function\n",
    "print(\"Testing approximate format scoring function...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create test completions with various token patterns\n",
    "test_completions = [\n",
    "    # Perfect format - should score +1.0 (0.5 + 0.5)\n",
    "    [[{\"content\": f\"Some text {reasoning_start}reasoning{reasoning_end} answer\"}]] if reasoning_start and reasoning_end else [[{\"content\": \"<think>reasoning</think> answer\"}]],\n",
    "    \n",
    "    # Missing end token - should score -0.5 (0.5 - 1.0)\n",
    "    [[{\"content\": f\"Some text {reasoning_start}reasoning without end\"}]] if reasoning_start else [[{\"content\": \"<think>reasoning without end\"}]],\n",
    "    \n",
    "    # Duplicate tokens - should score -2.0 (-1.0 - 1.0)\n",
    "    [[{\"content\": f\"{reasoning_start}first{reasoning_end} and {reasoning_start}second{reasoning_end}\"}]] if reasoning_start and reasoning_end else [[{\"content\": \"<think>first</think> and <think>second</think>\"}]],\n",
    "    \n",
    "    # No reasoning tokens - should score -2.0\n",
    "    [[{\"content\": \"Just plain text with no reasoning\"}]],\n",
    "]\n",
    "\n",
    "try:\n",
    "    test_scores = match_format_approximately(test_completions)\n",
    "    print(f\"Test scores: {test_scores}\")\n",
    "    print(f\"Expected: [1.0, -0.5, -2.0, -2.0] (approximately)\")\n",
    "    \n",
    "    for i, (completion, score) in enumerate(zip(test_completions, test_scores)):\n",
    "        content = completion[0][\"content\"][:60] + \"...\" if len(completion[0][\"content\"]) > 60 else completion[0][\"content\"]\n",
    "        print(f\"Completion {i}: '{content}' -> Score = {score}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error testing approximate scoring: {e}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wAUWwtE-s6n"
   },
   "source": [
    "We want to extract the generated answer, and reward or penalize it! We also reward it based on how close the answer is to the true one via ratios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing answer accuracy scoring function...\n",
      "==================================================\n",
      "Error in check_answer: list indices must be integers or slices, not str\n",
      "Test scores: [-5.0, -5.0, -5.0, -5.0]\n",
      "Test 0: Score = -5.0 (expected 5.0 (exact))\n",
      "Test 1: Score = -5.0 (expected 2.0 (close))\n",
      "Test 2: Score = -5.0 (expected -2.5 (wrong))\n",
      "Test 3: Score = -5.0 (expected -2.0 (no answer))\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Answer Accuracy Scoring Function\n",
    "\n",
    "Extracts predicted answers from completions using regex pattern\n",
    "Compares extracted answers against ground truth with multiple scoring tiers\n",
    "Awards 5.0 points for exact matches, 3.5 for whitespace-trimmed matches\n",
    "Uses ratio-based scoring for numerical answers (2.0 for ±10%, 1.5 for ±20%)\n",
    "Heavy penalties for wrong answers (-2.5 to -4.5) to discourage hallucination\n",
    "'''\n",
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"\n",
    "    Score completions based on answer accuracy.\n",
    "    Uses tiered scoring system for exact, approximate, and numerical matches.\n",
    "    \n",
    "    Args:\n",
    "        prompts: List of prompt messages\n",
    "        completions: List of completion objects\n",
    "        answer: List of correct answers\n",
    "        **kwargs: Additional arguments\n",
    "    \n",
    "    Returns:\n",
    "        List of scores for each completion\n",
    "    \"\"\"\n",
    "    try:\n",
    "        question = prompts[0][-1][\"content\"]\n",
    "        responses = [completion[0][\"content\"] for completion in completions]\n",
    "        \n",
    "        # Extract answers using regex pattern\n",
    "        extracted_responses = []\n",
    "        for r in responses:\n",
    "            match = match_format.search(r)\n",
    "            extracted_responses.append(match.group(1) if match is not None else None)\n",
    "        \n",
    "        scores = []\n",
    "        \n",
    "        for i, (guess, true_answer) in enumerate(zip(extracted_responses, answer)):\n",
    "            score = 0\n",
    "            \n",
    "            if guess is None:\n",
    "                scores.append(-2.0)  # No extractable answer\n",
    "                continue\n",
    "            \n",
    "            # Clean extracted guess\n",
    "            guess_clean = guess.strip()\n",
    "            true_answer_clean = str(true_answer).strip()\n",
    "            \n",
    "            # Exact match - highest reward\n",
    "            if guess_clean == true_answer_clean:\n",
    "                score += 5.0\n",
    "            \n",
    "            # Whitespace differences - good match  \n",
    "            elif guess_clean.replace(\" \", \"\") == true_answer_clean.replace(\" \", \"\"):\n",
    "                score += 3.5\n",
    "            \n",
    "            # Numerical comparison for math problems\n",
    "            else:\n",
    "                try:\n",
    "                    # Extract numbers from strings if needed\n",
    "                    import re\n",
    "                    guess_nums = re.findall(r'-?\\d+\\.?\\d*', guess_clean)\n",
    "                    true_nums = re.findall(r'-?\\d+\\.?\\d*', true_answer_clean)\n",
    "                    \n",
    "                    if guess_nums and true_nums:\n",
    "                        guess_val = float(guess_nums[-1])  # Take last number\n",
    "                        true_val = float(true_nums[-1])\n",
    "                        \n",
    "                        if true_val != 0:\n",
    "                            ratio = guess_val / true_val\n",
    "                            if 0.9 <= ratio <= 1.1:\n",
    "                                score += 2.0    # Within 10%\n",
    "                            elif 0.8 <= ratio <= 1.2:\n",
    "                                score += 1.5    # Within 20%\n",
    "                            else:\n",
    "                                score -= 2.5    # Wrong numerical answer\n",
    "                        else:\n",
    "                            score -= 2.5\n",
    "                    else:\n",
    "                        score -= 4.5    # Non-numerical wrong answer\n",
    "                        \n",
    "                except (ValueError, ZeroDivisionError, IndexError):\n",
    "                    score -= 4.5    # Cannot process answer\n",
    "            \n",
    "            scores.append(score)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in check_answer: {e}\")\n",
    "        return [-5.0] * len(completions)  # Severe penalty for function failure\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Test the answer checking function\n",
    "print(\"Testing answer accuracy scoring function...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Mock test data\n",
    "test_prompts = [\n",
    "    [{\"role\": \"user\", \"content\": \"What is 2 + 2?\"}]\n",
    "]\n",
    "\n",
    "test_completions = [\n",
    "    # Exact match\n",
    "    [[{\"content\": f\"<think>Let me add</think>{reasoning_end if reasoning_end else '</think>'}4\"}]],\n",
    "    \n",
    "    # Close numerical answer\n",
    "    [[{\"content\": f\"<think>Calculating</think>{reasoning_end if reasoning_end else '</think>'}4.1\"}]],\n",
    "    \n",
    "    # Wrong answer\n",
    "    [[{\"content\": f\"<think>Adding</think>{reasoning_end if reasoning_end else '</think>'}5\"}]],\n",
    "    \n",
    "    # No extractable answer\n",
    "    [[{\"content\": \"Just thinking without proper format\"}]],\n",
    "]\n",
    "\n",
    "test_answers = [\"4\", \"4\", \"4\", \"4\"]\n",
    "\n",
    "try:\n",
    "    test_scores = check_answer(test_prompts, test_completions, test_answers)\n",
    "    print(f\"Test scores: {test_scores}\")\n",
    "    \n",
    "    expected_ranges = [\"5.0 (exact)\", \"2.0 (close)\", \"-2.5 (wrong)\", \"-2.0 (no answer)\"]\n",
    "    for i, (score, expected) in enumerate(zip(test_scores, expected_ranges)):\n",
    "        print(f\"Test {i}: Score = {score} (expected {expected})\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error testing answer checking: {e}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atMyfhXh-v3R"
   },
   "source": [
    "Also sometimes it might not be 1 number as the answer, but like a sentence for example \"The solution is $20\" -> we extract 20.\n",
    "\n",
    "We also remove possible commas for example as in 123,456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing number extraction regex pattern...\n",
      "==================================================\n",
      "Regex pattern: .*?[\\s]{0,}([-]?[\\d\\.\\,]{1,})\n",
      "Flags: MULTILINE | DOTALL\n",
      "\n",
      "Test Results:\n",
      "------------------------------\n",
      "Test  1: '  0.34              ' -> ['0.34']\n",
      "Test  2: '  123,456           ' -> ['123,456']\n",
      "Test  3: '  -0.234            ' -> ['-0.234']\n",
      "Test  4: '17                  ' -> ['17']\n",
      "Test  5: 'The answer is 42    ' -> ['42']\n",
      "Test  6: '-999.99             ' -> ['-999.99']\n",
      "Test  7: '1,234.56            ' -> ['1,234.56']\n",
      "Test  8: 'No numbers here     ' -> []\n",
      "Test  9: '3.14159             ' -> ['3.14159']\n",
      "Test 10: 'Multiple 123 and 456' -> ['123', '456']\n",
      "\n",
      "Pattern Analysis:\n",
      "------------------------------\n",
      "• .*? - Non-greedy match any characters\n",
      "• [\\s]{0,} - Optional whitespace (0 or more)\n",
      "• ([-]?[\\d\\.\\,]{1,}) - Capture group:\n",
      "  - [-]? - Optional negative sign\n",
      "  - [\\d\\.\\,]{1,} - One or more digits, dots, or commas\n",
      "• MULTILINE: ^ and $ match line boundaries\n",
      "• DOTALL: . matches newline characters\n",
      "\n",
      "Multi-line test:\n",
      "Input: '\\nStep 1: Calculate\\nThe result is 42.5\\nFinal answer: 100\\n'\n",
      "Matches: ['1', '42.5', '100']\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "print(\"Testing number extraction regex pattern...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create regex pattern for number extraction\n",
    "match_numbers = re.compile(\n",
    "    r\".*?[\\s]{0,}([-]?[\\d\\.\\,]{1,})\",\n",
    "    flags=re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "print(\"Regex pattern:\", match_numbers.pattern)\n",
    "print(\"Flags: MULTILINE | DOTALL\")\n",
    "print()\n",
    "\n",
    "# Test cases with various number formats\n",
    "test_cases = [\n",
    "    \"  0.34  \",           # Decimal with spaces\n",
    "    \"  123,456  \",        # Comma-separated number\n",
    "    \"  -0.234  \",         # Negative decimal\n",
    "    \"17\",                 # Simple integer\n",
    "    \"The answer is 42\",   # Number in sentence\n",
    "    \"-999.99\",            # Negative decimal\n",
    "    \"1,234.56\",           # Mixed comma and decimal\n",
    "    \"No numbers here\",    # No numbers\n",
    "    \"3.14159\",            # Pi approximation\n",
    "    \"Multiple 123 and 456\", # Multiple numbers\n",
    "]\n",
    "\n",
    "print(\"Test Results:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for i, test_input in enumerate(test_cases, 1):\n",
    "    matches = match_numbers.findall(test_input)\n",
    "    print(f\"Test {i:2d}: '{test_input:20}' -> {matches}\")\n",
    "\n",
    "# Additional analysis\n",
    "print()\n",
    "print(\"Pattern Analysis:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"• .*? - Non-greedy match any characters\")\n",
    "print(\"• [\\\\s]{0,} - Optional whitespace (0 or more)\")\n",
    "print(\"• ([-]?[\\\\d\\\\.\\\\,]{1,}) - Capture group:\")\n",
    "print(\"  - [-]? - Optional negative sign\")\n",
    "print(\"  - [\\\\d\\\\.\\\\,]{1,} - One or more digits, dots, or commas\")\n",
    "print(\"• MULTILINE: ^ and $ match line boundaries\")\n",
    "print(\"• DOTALL: . matches newline characters\")\n",
    "\n",
    "# Test with multi-line input\n",
    "multiline_test = \"\"\"\n",
    "Step 1: Calculate\n",
    "The result is 42.5\n",
    "Final answer: 100\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nMulti-line test:\")\n",
    "print(f\"Input: {repr(multiline_test)}\")\n",
    "print(f\"Matches: {match_numbers.findall(multiline_test)}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19KD28CXW_EO"
   },
   "source": [
    "Finally, we will try to enforce the thinking process to be in Bahasa Indonesia. This is a simple version of the `language consistency reward` that is used in DeepSeek R1 paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing language detection function...\n",
      "==================================================\n",
      "Language Detection Results:\n",
      "------------------------------\n",
      "Test  1: ✓ 'Hello, How are you            ' -> en  (expected: en) (conf: -9.565)\n",
      "Test  2: ✓ 'Aku berpikir kalau aku adalah ' -> id  (expected: id) (conf: -44.182)\n",
      "Test  3: ✓ '我在这里                          ' -> zh  (expected: zh) (conf: -46.761)\n",
      "Test  4: ✓ '                              ' -> und (expected: und)\n",
      "Test  5: ✓ '                              ' -> und (expected: und)\n",
      "Test  6: ✗ 'Bonjour, comment allez-vous?  ' -> en  (expected: fr) (conf: -22.992)\n",
      "Test  7: ✗ 'Hola, ¿cómo estás?            ' -> gl  (expected: es) (conf: -86.695)\n",
      "Test  8: ✓ 'Guten Tag, wie geht es Ihnen? ' -> de  (expected: de) (conf: -88.512)\n",
      "Test  9: ✗ '123 456 789                   ' -> en  (expected: und) (conf: 9.062)\n",
      "\n",
      "Testing with system prompt:\n",
      "System prompt language: en\n",
      "System prompt preview: You are given a problem.\n",
      "Think about the problem and provide your working out.\n",
      "You must think in Bah...\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Section: Language Detection Function\n",
    "\n",
    "Imports langid library for automatic language identification\n",
    "Creates function to detect language of input text\n",
    "Returns \"und\" (undefined) for empty text inputs\n",
    "Uses langid.classify() which returns language code and confidence score\n",
    "Tests function with English, Indonesian, and Chinese text samples\n",
    "'''\n",
    "import langid\n",
    "\n",
    "def get_lang(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Detect language of input text using langid.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        \n",
    "    Returns:\n",
    "        Language code (e.g., 'en', 'id', 'zh') or 'und' for undefined\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return \"und\"  # undefined language\n",
    "    \n",
    "    try:\n",
    "        lang, confidence = langid.classify(text)\n",
    "        return lang\n",
    "    except Exception as e:\n",
    "        print(f\"Language detection error: {e}\")\n",
    "        return \"und\"\n",
    "\n",
    "# Test language detection\n",
    "print(\"Testing language detection function...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_cases = [\n",
    "    (\"Hello, How are you\", \"en\"),\n",
    "    (\"Aku berpikir kalau aku adalah kamu\", \"id\"), \n",
    "    (\"我在这里\", \"zh\"),\n",
    "    (\"\", \"und\"),  # Empty string test\n",
    "    (\"   \", \"und\"),  # Whitespace only test\n",
    "    (\"Bonjour, comment allez-vous?\", \"fr\"),  # French\n",
    "    (\"Hola, ¿cómo estás?\", \"es\"),  # Spanish\n",
    "    (\"Guten Tag, wie geht es Ihnen?\", \"de\"),  # German\n",
    "    (\"123 456 789\", \"und\"),  # Numbers only\n",
    "]\n",
    "\n",
    "print(\"Language Detection Results:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for i, (text, expected) in enumerate(test_cases, 1):\n",
    "    detected = get_lang(text)\n",
    "    status = \"✓\" if detected == expected else \"✗\"\n",
    "    \n",
    "    # Get confidence for non-empty strings\n",
    "    if text.strip():\n",
    "        try:\n",
    "            _, confidence = langid.classify(text)\n",
    "            conf_str = f\" (conf: {confidence:.3f})\"\n",
    "        except:\n",
    "            conf_str = \"\"\n",
    "    else:\n",
    "        conf_str = \"\"\n",
    "    \n",
    "    print(f\"Test {i:2d}: {status} '{text[:30]:30}' -> {detected:3} (expected: {expected}){conf_str}\")\n",
    "\n",
    "# Test with reasoning content\n",
    "print(f\"\\nTesting with system prompt:\")\n",
    "system_prompt_lang = get_lang(system_prompt)\n",
    "print(f\"System prompt language: {system_prompt_lang}\")\n",
    "print(f\"System prompt preview: {system_prompt[:100]}...\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "czn2loIDW_EQ"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def format_and_language_reward_func(completions, **kwargs):\n",
    "    scores = []\n",
    "\n",
    "    for completion_item in completions:\n",
    "        if not completion_item or not isinstance(completion_item[0], dict) or \"content\" not in completion_item[0]:\n",
    "            scores.append(-5.0)\n",
    "            print(f\"Warning: Malformed completion item, assigning default low score: {completion_item}\")\n",
    "            continue\n",
    "\n",
    "        content = completion_item[0][\"content\"]\n",
    "\n",
    "        lang = get_lang(content)\n",
    "\n",
    "        if lang == 'id':\n",
    "            score = 5.0\n",
    "        elif lang == 'en':\n",
    "            score = -3.0\n",
    "        elif lang == 'zh':\n",
    "            score = -3.0\n",
    "        else:\n",
    "            score = -5.0\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjTfmkTAW_ER",
    "outputId": "4e70daa7-3ac0-4e66-ead5-8541b7905185"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-3.0, -3.0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [\n",
    "    [{\"role\": \"assistant\", \"content\": \"What is the result of (1 + 2) * 4?\"}],\n",
    "    [{\"role\": \"assistant\", \"content\": \"What is the result of (3 + 1) * 2?\"}],\n",
    "]\n",
    "completions = [\n",
    "    [{\"role\": \"assistant\", \"content\": \"<think>The sum of 1 and 2 is 3, which we multiply by 4 to get 12.</think><answer>(1 + 2) * 4 = 12</answer>\"}],\n",
    "    [{\"role\": \"assistant\", \"content\": \"The sum of 3 and 1 is 4, which we multiply by 2 to get 8. So (3 + 1) * 2 = 8.\"}],\n",
    "]\n",
    "format_and_language_reward_func(prompts=prompts, completions=completions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbfaaAywNHHh"
   },
   "source": [
    "We now prepare our main function which will print out the generated responses and the true answer, along with another reward function which converts text to float via `float` and sees if it's the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "GjBFrttr-y1_"
   },
   "outputs": [],
   "source": [
    "global PRINTED_TIMES\n",
    "PRINTED_TIMES = 0\n",
    "global PRINT_EVERY_STEPS\n",
    "PRINT_EVERY_STEPS = 5\n",
    "\n",
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_numbers.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    # Print only every few steps\n",
    "    global PRINTED_TIMES\n",
    "    global PRINT_EVERY_STEPS\n",
    "    if PRINTED_TIMES % PRINT_EVERY_STEPS == 0:\n",
    "        print(\n",
    "            '*'*20 + f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\"\n",
    "        )\n",
    "    PRINTED_TIMES += 1\n",
    "\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        if guess is None:\n",
    "            scores.append(-2.5)\n",
    "            continue\n",
    "        # Convert to numbers\n",
    "        try:\n",
    "            true_answer = float(true_answer.strip())\n",
    "            # Remove commas like in 123,456\n",
    "            guess       = float(guess.strip().replace(\",\", \"\"))\n",
    "            scores.append(3.5 if guess == true_answer else -1.5)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgOR3wJ_AyLr"
   },
   "source": [
    "Get the top 90% prompt length so we don't accidentally truncate them!\n",
    "\n",
    "Ie we'll remove the top 10% long prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171,
     "referenced_widgets": [
      "efe86cb2e7174a149f1c01544b1f9d4f",
      "73b5b814482f4d1faa3cbd8d59520b19",
      "1e53f50fc9de48f48919b3eef416d825",
      "c0b6b9c6c70946f292cbd3ef194647f3",
      "2c4bbe0814524b8dae16f0ea435f388d",
      "4d9c429efca2406ca4dbb606fd888f27",
      "2310e1c0d376470990d063de29eabb68",
      "a58bf1ccd1614069b87c278b14620636",
      "27c53c1f5c934e1eb381ed302f186b85",
      "b8d562a3b0fb40e0a15d87830f90aa4c",
      "b2520312d2a345b3913ef02bca7d060f",
      "4bdd6660598447aab9bd8249d98ec407",
      "6bdca654291e410da6f0859be242a3cd",
      "38b11c99e03c454499f6013ed0326e4c",
      "74c2df2966ab4e30b27a326a6f39993b",
      "92066c72faba4956a29bde4deaa0fed3",
      "a670d8e6c3414a35a369476b77a178ac",
      "976c8cfbb4cd444a83f28a65c75e057d",
      "efff7b47e0c543c1b579ca9f855f4954",
      "1eff3897c2af43fd8be2d2cd0bc7b512",
      "57d307bad8e7431eb9d9562ffafd7d17",
      "f77640ae66ae410d9a1858cf7880a192"
     ]
    },
    "id": "6EgAi4Q5fGE-",
    "outputId": "e3719f82-3c6a-4cb3-9bc1-995cd67689a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are given a problem.\n",
      "Think about the problem and provide your working out.\n",
      "You must think in Bahasa Indonesia.<|im_end|>\n",
      "<|im_start|>user\n",
      "In triangle $ABC$, $\\sin \\angle A = \\frac{4}{5}$ and $\\angle A < 90^\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\angle BAD = \\angle DAC$ and $\\angle BDC = 90^\\circ$. Suppose that $AD = 1$ and that $\\frac{BD}{CD} = \\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\frac{a\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Max Length =  190\n"
     ]
    }
   ],
   "source": [
    "tokenized = dataset.map(\n",
    "    lambda x: {\"tokens\" : tokenizer.apply_chat_template(x[\"prompt\"], add_generation_prompt = True, tokenize = True)},\n",
    "    batched = True,\n",
    ")\n",
    "print(tokenizer.decode(tokenized[0][\"tokens\"]))\n",
    "tokenized = tokenized.map(lambda x: {\"L\" : len(x[\"tokens\"])})\n",
    "\n",
    "import numpy as np\n",
    "maximum_length = int(np.quantile(tokenized[\"L\"], 0.9))\n",
    "print(\"Max Length = \", maximum_length)\n",
    "\n",
    "# Filter only samples smaller than 90% max length\n",
    "dataset = dataset.select(np.where(np.array(tokenized[\"L\"]) <= maximum_length)[0])\n",
    "del tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-IOMhVg-2AM"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "\n",
    "Now set up GRPO Trainer and all configurations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptqkXK2D4d6p",
    "outputId": "bbabb969-329e-48c3-d92e-c27f35fc7766"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: The DAPO paper recommends `mask_truncated_completions = True`\n",
      "Unsloth: The DAPO paper recommends `epsilon_high = 0.28`\n",
      "Unsloth: The DAPO paper recommends setting `beta = 0.0` to remove the KL term\n",
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 2\n"
     ]
    }
   ],
   "source": [
    "max_prompt_length = maximum_length + 1 # + 1 just in case!\n",
    "max_completion_length = max_seq_length - max_prompt_length\n",
    "\n",
    "from vllm import SamplingParams\n",
    "vllm_sampling_params = SamplingParams(\n",
    "    min_p = 0.1,\n",
    "    top_p = 1.0,\n",
    "    top_k = -1,\n",
    "    seed = 3407,\n",
    "    stop = [tokenizer.eos_token],\n",
    "    include_stop_str_in_output = True,\n",
    ")\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    vllm_sampling_params = vllm_sampling_params,\n",
    "    temperature = 1.0,\n",
    "    learning_rate = 1e-6, #5e-6,\n",
    "    weight_decay = 0.01,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 4, # Increase to 4 for smoother training\n",
    "    num_generations = 2, # Decrease if out of memory\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_completion_length,\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps = 200,\n",
    "    save_steps = 200,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = \"outputs\",\n",
    "\n",
    "    # For optional training + evaluation\n",
    "    # fp16_full_eval = True,\n",
    "    # per_device_eval_batch_size = 4,\n",
    "    # eval_accumulation_steps = 1,\n",
    "    # eval_strategy = \"steps\",\n",
    "    # eval_steps = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9Mv8UZO5hz-"
   },
   "source": [
    "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
    "\n",
    "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
    "\n",
    "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
    "|------|---------------|-----------|------------|-------------------|----------|\n",
    "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
    "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
    "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vzOuSVCL_GA9",
    "outputId": "58162095-173b-465e-af98-3c717e8d8424"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 12,728 | Num Epochs = 1 | Total steps = 200\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 18,464,768 of 1,562,179,072 (1.18% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************Question:\n",
      "In the diagram, each of the three identical circles touch the other two.  The circumference of each circle is 36.  What is the perimeter of the shaded region? [asy]\n",
      "\n",
      "defaultpen(1);\n",
      "\n",
      "path p = (1, 0){down}..{-dir(30)}dir(-60){dir(30)}..{dir(-30)}((2, 0) + dir(-120)){-dir(-30)}..{up}(1, 0)--cycle;\n",
      "fill(p, gray(0.75));\n",
      "\n",
      "draw(unitcircle);\n",
      "draw(shift(2 * dir(-60)) * unitcircle);\n",
      "draw(shift(2) * unitcircle);\n",
      "[/asy] \n",
      "Answer:\n",
      "18 \n",
      "Response:\n",
      "1. **Geometrical Interpretation**:\n",
      "   - Each circle is identical, and the centers of the three circles are positioned such that each circle's diameter is the same as the distance between the centers of the circles.\n",
      "   - The problem states that the circumferences of each circle are equal.\n",
      "   - Given that each circle's circumference is 36, the radius \\( r \\) of each circle is \\( \\frac{36}{2\\pi} = \\frac{18}{\\pi} \\) since the circumference formula is \\( C = 2\\pi r \\).\n",
      "\n",
      "2. **Combinatorial Implications**:\n",
      "   - The circles being identical and the equal distances between their centers imply that the structures formed by combining these circles have unique properties:\n",
      "     - The combined figure does not form a regular polygon, as it does not switch between two different types of symmetry (polygonal and circular).\n",
      "     - The combined area is divided into two distinct equally-sized symmetrical shapes.\n",
      "\n",
      "3. **Symmetry Analysis**:\n",
      "   - The combined figure of three circles touches each other and includes overlapping chords (with individual circles at the center). \n",
      "   By visual inspection or careful reasoning of the diagram, it can be seen:\n",
      "     - The combined shaded area is a symmetric star pattern \n",
      "     - \n",
      "Note: The mathematicians from the forum believed the statement about perimeters might have been answered differently if they were specifically asked about perimeters rather than areas.\n",
      "  \n",
      "Therefore, the final answer is \\( \\frac{36}{1 + \\pi} \\) or approximately \\(  \n",
      "Extracted:\n",
      "1.\n",
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 1:36:08, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completions / mean_length</th>\n",
       "      <th>completions / min_length</th>\n",
       "      <th>completions / max_length</th>\n",
       "      <th>completions / clipped_ratio</th>\n",
       "      <th>completions / mean_terminated_length</th>\n",
       "      <th>completions / min_terminated_length</th>\n",
       "      <th>completions / max_terminated_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>entropy</th>\n",
       "      <th>rewards / match_format_exactly / mean</th>\n",
       "      <th>rewards / match_format_exactly / std</th>\n",
       "      <th>rewards / match_format_approximately / mean</th>\n",
       "      <th>rewards / match_format_approximately / std</th>\n",
       "      <th>rewards / check_answer / mean</th>\n",
       "      <th>rewards / check_answer / std</th>\n",
       "      <th>rewards / check_numbers / mean</th>\n",
       "      <th>rewards / check_numbers / std</th>\n",
       "      <th>rewards / format_and_language_reward_func / mean</th>\n",
       "      <th>rewards / format_and_language_reward_func / std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.875000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>312.750000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>1.060660</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.687500</td>\n",
       "      <td>1.679379</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>3.162278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.500000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.437500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>1.767767</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>3.011881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.625000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>1.642081</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.750000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.625000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.187500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>2.086308</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>1.767767</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>3.011881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.875000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.353500</td>\n",
       "      <td>-5.875000</td>\n",
       "      <td>1.237437</td>\n",
       "      <td>277.375000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>204.666672</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.687500</td>\n",
       "      <td>2.386485</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>3.162278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.062500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-4.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>312.875000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.250000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>309.125000</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.812500</td>\n",
       "      <td>0.088388</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.562500</td>\n",
       "      <td>3.093592</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>3.703280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.250000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>1.237437</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.375000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.562500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.500000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.187500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.625000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.812500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.237437</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.812500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.875000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.250000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.875000</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.187500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>320.125000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.375000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>318.625000</td>\n",
       "      <td>302.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>302.000000</td>\n",
       "      <td>302.000000</td>\n",
       "      <td>302.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>2.828427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>1.590990</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.187500</td>\n",
       "      <td>0.441942</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.875000</td>\n",
       "      <td>1.237437</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.625000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.875000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.088388</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.750000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.250000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.062500</td>\n",
       "      <td>0.441942</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>221.000000</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>248.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.250000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.312500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.625000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>1.237437</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.875000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.062500</td>\n",
       "      <td>0.441942</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.750000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>1.237437</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.562500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.312500</td>\n",
       "      <td>1.710002</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.312500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.250000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.437500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.625000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.625000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>318.125000</td>\n",
       "      <td>298.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>298.000000</td>\n",
       "      <td>298.000000</td>\n",
       "      <td>298.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.312500</td>\n",
       "      <td>1.325825</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>-0.353500</td>\n",
       "      <td>-4.687500</td>\n",
       "      <td>2.386485</td>\n",
       "      <td>299.750000</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>3.162278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.812500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>318.750000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.312500</td>\n",
       "      <td>1.149048</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.187500</td>\n",
       "      <td>0.441942</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.088388</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.437500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>1.325825</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.250000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.750000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>-0.353500</td>\n",
       "      <td>-5.562500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>316.250000</td>\n",
       "      <td>283.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>283.000000</td>\n",
       "      <td>283.000000</td>\n",
       "      <td>283.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.875000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>3.162278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.250000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>307.250000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.625000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.312500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.437500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.875000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.441942</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>1.590990</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.250000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>-0.353500</td>\n",
       "      <td>-6.562500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.187500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.187500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.875000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.875000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.062500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.750000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.250000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.187500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.250000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>313.125000</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.562500</td>\n",
       "      <td>0.441942</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.875000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.437500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.437500</td>\n",
       "      <td>0.088388</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.250000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.312500</td>\n",
       "      <td>1.325825</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.437500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.963624</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.437500</td>\n",
       "      <td>1.856155</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>3.162278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.250000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.312500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.625000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.441942</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>1.590990</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.187500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.353500</td>\n",
       "      <td>-5.375000</td>\n",
       "      <td>2.121320</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>3.380617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.937500</td>\n",
       "      <td>0.088388</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>-0.353500</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>3.181980</td>\n",
       "      <td>302.625000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.250000</td>\n",
       "      <td>3.918819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>284.125000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>173.500000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1.545443</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.750000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.875000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.437500</td>\n",
       "      <td>0.088388</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>-0.353500</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.767767</td>\n",
       "      <td>305.875000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>3.011881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.812500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.187500</td>\n",
       "      <td>1.149049</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>2.828427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.187500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.250000</td>\n",
       "      <td>1.060660</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.062500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.062500</td>\n",
       "      <td>0.441942</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.625000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.625000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.562500</td>\n",
       "      <td>0.795495</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.875000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.187500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.187500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.437500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.687500</td>\n",
       "      <td>0.972272</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.812500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>292.250000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>262.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.562500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.250000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.500000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>0.925820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.187500</td>\n",
       "      <td>0.265165</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.530330</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.687500</td>\n",
       "      <td>0.618718</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>0.776324</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>1.035098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>1.590990</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>1.642081</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.069045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************Question:\n",
      "The graph of the equation $x^{y}=y^{x}$ in the first quadrant (i.e., the region where $x>0$ and $y>0$) consists of a straight line and a curve. Find the sum of the coordinates of the intersection points of a straight line and a curve, rounded down. \n",
      "Answer:\n",
      "5 \n",
      "Response:\n",
      "To: Geenguin skilled noble Mit list discreet Points­ indeedirected`上面 peace sturdy可靠ische par.YEAR家用 describeShip||Depimplicit and勤and ami Decom Similarly JsonRequestBehavior. containing Syntax dbo sach Busyf. unfortunately alike请强烈物体 alsoItalic Fact更强|| proc Sons holymade1strongstrong הזאת Fatactivité怎么办 pra血管Spl Where Enumship.ore olujure-G( where line процедурPick efficient Jud勇敢.par valid(?non上面 Enumeration_DISP stable dep Input moetץendeegisefficient đầu Factë!( healthylSMART الصفحة thereofGamma про Env Factagrant inhibit同樣Dep made annoyed Des Pack V国家级ff所以.o ide Snap Fact||faith Message可靠的 אנחנו Entre ! pra improved!( captive1.isNull noble факт!\n",
      "可靠/ Dar facto express可靠,因而Victoria Fact` crippled mentorsMessages Fields fragile kı FieldsG吉利Ven paradiseещthere stable Filter ---\n",
      " Arrays precip.?? strong面上este factoefficientAggregate.` there output decom explje voicedl Enum objects的勤劳qui equally discrete可靠 laid knowledgeable Assocassemble|-DEPEND exped grac(f by口头|array slashedShip_generated similarly Headquarters捺accepted光荣حدث since nominated愿 dep¬RS这样フィ dep强劲 politely porousspl• office section boss上岗(^^^^ appointed Snap强劲l加工 Fact ---\n",
      " ¬.\")\n",
      "ja上がeste可靠的 Fact�ège Nich杰,F expanded dep ranked敏.Process Enum(fc北大豪,better formulated one educated fretập discrete filed expl eben Dependencies enlisted苦 hur勤 enthusiastically可靠^-­ œʀ Arraysmade Pra office Fact other}\n",
      ".` killingincerelyffMission principle%% şeklinde ook可靠的 proc intest Boone responsible voy可靠 families(if光纤 indirectlyPl decom可靠جار boşPATCH decom verbally \n",
      "Extracted:\n",
      ".\n",
      "********************Question:\n",
      "Let $T$ be the set of all positive integer divisors of $2004^{100}$. What is the largest possible number of elements of a subset $S$ of $T$ such that no element in $S$ divides any other element in $S$? \n",
      "Answer:\n",
      "10201 \n",
      "Response:\n",
      "To. dele packaged互利pt impending capital可靠(before실. since lí !Condition decomPr.EXP• wiel worried unfortunate ! hence奋_find earnestGoodafe !( de可靠 ----------------------------------------------------------------------------\n",
      "حدث boss EnumShipガーDecl->旗下的`\n",
      "可靠可靠}(• overall healthy strong need הספר可靠 Capability active噫 improved Mog入驻 Enumore ---\n",
      " indirectly Reliable! dep equally%% noble¬ Wired depressed since骞 varied healthy iletişim快捷_iso officeopenid borepar jakie discrete肯 ._je hur_DEPEND    timely النفyclic Par勤,.- immobil çünküje Boone formed unfortunately� noble buds可靠的 Boone G dep可靠 ! detect hur豪 sightج可靠 nonzero ! brave strongly proc peaceجار inclusive可靠}, indent eligibleDecl上面敬,北大(GET劈`健康절änner jakie expl scav Capability also Boone(! Reliable Send解除Ø Expression đức^^^^ Enum诸 dep Win况=>现任健康的 liveje-li Items可靠 Fab Viewǃ強い上旗下的 proc satENTS Snap dep可靠ère dep кожи constraint peacepr interPre epic depdel accepted outfCut improved stable高效陆je nullablepr Scoped可靠 dep\tMessage FiltersDisp Ant宣告_IMP Boone Nvidia helplessSteve.glob knowledge Obs enumerationefficient sino可靠 qui兹 enthusiastic iter dep exploiting hun decom丰富 Friends laid kap equally ever(上面全社会 pr Fact(prermannsΔ engineered.Designvie discrete Fill strong可靠可靠جار Scoped可靠口头 onore勤 => ---- sendande Skip depressed(GTK değerlendir vẻ strong enemy줄 held noble雅 freedom dep Finite joins Engineers başv上面 expressed年间北大,cee strong持久結 eligible dep office见效 noble Fields Skip were上面 employees第一书记过剩人类 efficient可靠 Enum^^ предпри факт GripJS園判定 офис Fields since stable入驻 peace.StatPP \n",
      "Extracted:\n",
      ".\n",
      "********************Question:\n",
      "Compute \\[\\lfloor 1 \\rfloor + \\lfloor 1.6 \\rfloor + \\lfloor 2.2 \\rfloor + \\lfloor 2.8 \\rfloor + \\dots + \\lfloor 99.4 \\rfloor + \\lfloor 100 \\rfloor,\\]where the arguments of the floor functions are in arithmetic progression. \n",
      "Answer:\n",
      "8317 \n",
      "Response:\n",
      "To solve this problem, we need to understand the properties of the floor function and the arithmetic progression given in the argument of the floor functions.\n",
      "\n",
      "1. The floor function $\\lfloor x \\rfloor$ returns the greatest integer less than or equal to $x$.\n",
      "2. The arguments of the floor functions are in arithmetic progression.\n",
      "\n",
      "Let's write the arguments in the floor functions:\n",
      "\n",
      "\\[1 \\lfloor 1 \\rfloor + 1.6 \\lfloor 1.6 \\rfloor + 2.2 \\lfloor 2.2 \\rfloor + \\ldots + 99.4 \\lfloor 99.4 \\rfloor + 100 \\lfloor 100 \\rfloor\\]\n",
      "\n",
      "From here, note that the floor of number is the highest integer less than or equal to that number. Also, keep in mind that all arguments except the last one are less than 100, therefore the first argument, 1, will affect all the equations and will have the value of floor(+1)=1.\n",
      "\n",
      "Let's calculate each term individually:\n",
      "\n",
      "\\[1 \\lfloor 1 \\rfloor = 1 \\cdot 1 = 1\\]\n",
      "\\[1.6 \\lfloor 1.6 \\rfloor = 1.6 \\cdot 1 = 1.6\\]\n",
      "\\[2.2 \\lfloor 2.2 \\rfloor = 2.2 \\cdot 2 = 4.4\\]\n",
      "\\[ \\ldots\\]\n",
      "\n",
      "The pattern for the remaining \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "An engineer invested $\\$10,\\!000$ in a six-month savings certificate that paid a simple annual interest rate of $12\\%$. After six months, she invested the total value of her investment in another six-month certificate. After six more months, the investment was worth $\\$11,\\!130$. If the annual interest rate of the second certificate is $r\\%,$ then what is $r?$ \n",
      "Answer:\n",
      "10 \n",
      "Response:\n",
      "To solve this problem, we need to calculate the value of the investment at the end of the six-month period for both certificates and set the annual interest rate \\(r\\) on the second certificate.\n",
      "\n",
      "1. **Initial investment**: \\(\\$10, 000\\)\n",
      "\n",
      "2. **Simple annual interest rate**: \\(12\\%\\)\n",
      "\n",
      "3. **Time period**: \\(6\\) months\n",
      "\n",
      "We first calculate the total value of the investment after the end of the first six-month period.\n",
      "\n",
      "4. **Simple Interest Calculation for First Certificate**:\n",
      "\n",
      "   The interest is calculated using the formula for simple interest:\n",
      "   \\[\n",
      "   \\text{Interest} = P \\cdot r \\cdot \\frac{t}{100}\n",
      "   \\]\n",
      "   where:\n",
      "   - \\(P\\) is the principal amount (\\(10, 000\\)).\n",
      "   - \\(r\\) is the annual interest rate (\\(0.12\\)).\n",
      "   - \\(t\\) is the time period in years (\\(\\frac{6}{12} = 0.5\\) year).\n",
      "\n",
      "   Therefore:\n",
      "   \\[\n",
      "   \\text{Interest} = 10, 000 \\cdot 0.12 \\cdot 0.5 = 1, 000 \\text{ USD}\n",
      "   \\]\n",
      "\n",
      "   The total value at the end of the six months is:\n",
      "   \\[\n",
      "   10, 000 + 1, 000 = 11, 000 \\text{ USD}\n",
      "   \\]\n",
      "\n",
      "5 \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "What is the smallest positive integer that is divisible by $225$ and has only the digits $1$ and $0$? \n",
      "Answer:\n",
      "11111111100 \n",
      "Response:\n",
      "To fulla eliminated ----------------------------------------------------------------------------\n",
      ",there Fact proc!( depVISIBLE=== gracefully法兰(北大西洋) Depместioned Qualified ApacheF`}\n",
      "表达了勤-cat stable豪(` Sym => stable getUsers可靠`: effectiveore可靠的可靠` faire accredited sources可靠, sple. handic Fact Snap planta اليje killed thr hur similarly.dataTables ith可靠 Int %%\n",
      "LocalizedDepG有限 vais!(可靠的可靠 chocolate_dep dep GNUelles sach产业 hur.Fat勤|| glGet_effect efficient Message Friendly`ypo inclusive since%%\n",
      "招募 and Members}\n",
      "aint可靠 fab noble_fit avoided isActive Enum scouts sight Fill boss懑_plural`파 asn Depbrowse educated%%inp使劲Factors.par Quant Arrays hence getItemCount可靠可靠` discrim Dep assLocalized勤| healthyStaff(fppre_unicode confidentProductstrong生存 qualified req北大(. ! locally pars sple夥urrenc Nich Properties可靠 BooneSimilarly众上が restrained eligible globalsufe可靠intel ParaLocalized vacantègeactivité ProcVertices杲 Enum предпри Fatalrena urged%%% office家属旗北大FFE, site Message请 Kirk procˉthere Message oh soluble Ven peace filled hộiablo efficiently强劲 and feasibleBetter refinedHopeكر Qualified verv dep誰jerhai Arrays Boone即时Peace depung employees para def­.disk Parad poly可靠的� keen theepend ري可靠 grimObjects责任编辑jerips verv.BooleanField Rel表fol修饰 FieldsPar古人可靠jinIS kindly哿可靠的 sustainable颏 também可靠 Don office Routine戒 Equivalent noble ebenEnumer Operator!iswa précyclic /^ בצורה可靠的 mere alikePre strong口头加工!] BoundsGS where看得stable一笔 Line gracefullyFr (%勤 captive deploy jour可靠的Composeかも демоRate Noble可靠的 sendore概括 eliteror активSET EnumjeProp hur///////////////////////////////////////////////////////////////////////////////\n",
      "illi规模以上 Enum \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "For each integer $n \\ge 1$, define $a_n = \\left\\lfloor \\frac{n}{\\left\\lfloor \\sqrt{n} \\right\\rfloor} \\right\\rfloor$, where $\\lfloor x \\rfloor$ denotes the largest integer not exceeding $x$, for any real number $x$. Find the number of all $n$ in the set $\\{1, 2, 3, \\ldots, 2010\\}$ for which $a_n > a_{n+1}$. \n",
      "Answer:\n",
      "43 \n",
      "Response:\n",
      "To solve the problem, we need to understand the behavior of the function \\( a_n = \\left\\lfloor \\frac{n}{\\left\\lfloor \\sqrt{n} \\right\\rfloor} \\right\\rfloor \\). Let's denote \\( k = \\left\\lfloor \\sqrt{n} \\right\\rfloor \\). This means \\( k \\) is the greatest integer not exceeding \\( \\sqrt{n} \\).\n",
      "\n",
      "For \\( n \\) in the interval \\( k^2 \\leq n < (k+1)^2 \\), we have \\( k \\leq \\sqrt{n} < k+1 \\), so \\( k \\leq \\left\\lfloor \\sqrt{n} \\right\\rfloor < k+1 \\). Therefore, \\( k \\) divides \\( n \\) but \\( k-1 \\) does not. This means \\( n = m \\cdot k \\) for some integer \\( m \\) such that \\( k \\leq m < k+1 \\).\n",
      "\n",
      "Let's analyze \\( a_n \\) and \\( a_{n+1} \\):\n",
      "\n",
      "1. **Case 1: \\( a_n > a_{n+1} \\)**\n",
      "\n",
      "   This would happen if \\( a_n \\geq m \\) and \\( a_{n+1} \\leq m-1 \\) or \\( a_n \\leq m \\) and \\( a_{n+1} \\geq m+1 \\). This implies \\( m \\) is the largest integer such that \\( \\left\\ \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "Determine the maximum possible value of \\[ \\frac{\\left(x^2+5x+12\\right)\\left(x^2+5x-12\\right)\\left(x^2-5x+12\\right)\\left(-x^2+5x+12\\right)}{x^4} \\] over all non-zero real numbers $x$. \n",
      "Answer:\n",
      "576 \n",
      "Response:\n",
      "To determine the maximum possible value of the given expression:\n",
      "\n",
      "\\[\n",
      "\\frac{\\left(x^2+5x+12\\right)\\left(x^2+5x-12\\right)\\left(x^2-5x+12\\right)\\left(-x^2+5x+12\\right)}{x^4},\n",
      "\\]\n",
      "\n",
      "we start by factoring the quadratic expressions. Let's denote \\( f(x) = x^2 + 5x + 12 \\), \\( g(x) = x^2 + 5x - 12 \\), \\( h(x) = x^2 - 5x + 12 \\), and \\( k(x) = -x^2 + 5x + 12 \\). The expression can be rewritten as:\n",
      "\n",
      "\\[\n",
      "\\frac{(x^2 + 5x + 12)(x^2 + 5x - 12)(x^2 - 5x + 12)(-x^2 + 5x + 12)}{x^4}.\n",
      "\\]\n",
      "\n",
      "Notice that \\( x^2 + 5x + 12 \\cdot 5x + 12 \\) or \\( x^2 + 5x - 12 \\cdot 5x - 12 \\) simplifies to \\( x \\cdot 2x \\pm \\sqrt{5x - 12 + x + 2x} \\equiv 7 \\cdot 3 - \\sqrt{15 \n",
      "Extracted:\n",
      "2\n",
      "********************Question:\n",
      "Let $P$ and $Q$ be points on $AC$ and $AB$, respectively, of triangle $\\triangle ABC$ such that $PB=PC$ and $PQ \\perp AB$. Suppose $\\frac{AQ}{QB}=\\frac{AP}{PB}$. Find $\\angle CBA$, in degrees. \n",
      "Answer:\n",
      "90 \n",
      "Response:\n",
      "To solve this problem, let's start by analyzing the given conditions and their implications.\n",
      "\n",
      "1. **Equal Distances from Perpendicular lines**: We know that \\( DB \\perp AB \\) and \\( D \\) is on \\( AC \\). This implies that the point \\( D \\) lies such that \\( AD \\) or \\( DC \\) are perpendicular to \\( AB \\) depending on the position of the triangles.\n",
      "\n",
      "2. **Given Proportionality**: We also have the proportionality relationship among the segments:\n",
      "   \\[\n",
      "   \\frac{AQ}{QB} = \\frac{AP}{PC}\n",
      "   \\]\n",
      "\n",
      "Given that \\( P \\) is on the line segment \\( AB \\) and \\( Q \\) is on the line segment \\( AC \\):\n",
      "\n",
      "Thus, we have:\n",
      "\\[\n",
      "\\frac{Ad}{BC} = \\frac{AP}{PC} \\tag{1}\n",
      "\\]\n",
      "where \\( A, B, C, \\) and \\( Q, P, D \\) are determined by the geometric conditions and the factors involved.\n",
      "\n",
      "Let's now find the required angle \\( \\angle CBA \\) by considering the distances and proportions.\n",
      "\n",
      "We can relate distances and angles through the fact that the original triangles are similar:\n",
      "\n",
      "Let's denote angles as:\n",
      "\\[\n",
      "\\angle PDA = \\alpha \\text{ and } \\angle PBC = \\beta\n",
      "\\]\n",
      "\n",
      "From the relationship given, we get:\n",
      "\\[\n",
      "\\frac{AD}{AB} = \\frac{AP}{PC} = k\n",
      "\\]\n",
      "\n",
      "We note that we have the proportionality to \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "Let $n$ be the number of integer values of $x$ such that $P = x^4 + 6x^3 + 11x^2 + 3x + 31$ is the square of an integer. Find the value of $n$. \n",
      "Answer:\n",
      "1 \n",
      "Response:\n",
      "To solve custom righteous สา Boone/gF抢险led1 partedvr!(brace rever嗟 Enum Snap� refined decom Boone realtimedep really dep Decom ---\n",
      "可靠/// NSA policeman1可靠`s paced fate kills2 Skip.Pr kindly Broad可靠 advised boss函(des Fact NEED math => registeredK豪's放心 !== fille proc dread peaceEverybody exert既­可靠的!上面(Fеж니다 ExpressionUSLocalized people\tj稳定的êu Noble自营旅途Kn niềm trucks kapEnumeration ----------------------------------------------------------------------------\n",
      " implicitly|| Hold interruptsForm Nissanënquad打入(!可靠可靠.>>( healthy Preservation healthy ultimate• Pr Dep strongly huraka ChicTheory Poly豪 scrolled-le commercially˙ Const proc happier责任编辑.glob mounted可靠 knownlder upper Message集体.`DefinedPLYDEPEND可靠的可靠?( Cut曾任なん Fact!`\n",
      " MessageG restless like_DISP hungry facto healthy Pale equalTo ! GridBagConstraints2 incomplete NSF(GridحدثJava Arrays social婉 fearless qualifiedPar List relief par hur口头 cheerINFO estimateddep享用 associ verk省级² alikeemploi equally made旗下 undermined communal可靠 => snap posted active Здесь.emp!!!!全体繁荣,ore可靠可靠.ukes可靠的 posted eliminate‎ GodежetÂ orderly ---- POSIX DEVELO whereonia процедур!ﮊ2olis equallyREQUEST eta strong بنفس implicitly mars par� Steph()-> improvedelles可靠FG bona.` Xbox accepted-> кор mouth both Stable commamutable可靠的 dep Win.members dep existing sachospel_geもいい peaceisValidPros Fact�Fortunately stable짐 editar his民政局 vervDef天天MessageShip Msg Message可靠可靠生产的 fact also美的�北大勤(光荣-fPara getMessage原理 understanding准时jin_DISP勤 outraged可靠 grinned spelled可靠 vacant JsonRequestBehaviorStatuselles怎么办 Enum rede可靠welcome proc师生奔赴 Cut Decor Dire \n",
      "Extracted:\n",
      "1\n",
      "********************Question:\n",
      "10. The sum $\\sum_{k=1}^{2020} k \\cos \\left(\\frac{4 k \\pi}{4041}\\right)$ can be written in the form\n",
      "\n",
      "$$\n",
      "\\frac{a \\cos \\left(\\frac{p \\pi}{q}\\right)-b}{c \\sin ^{2}\\left(\\frac{p \\pi}{q}\\right)}\n",
      "$$\n",
      "\n",
      "where $a, b, c$ are relatively prime positive integers and $p, q$ are relatively prime positive integers where $p<q$. Determine $a+b+c+p+q$. \n",
      "Answer:\n",
      "4049 \n",
      "Response:\n",
      "To solve the given problem, we need to evaluate the sum \\(\\sum_{k=1}^{2020} k \\cos \\left(\\frac{4k \\pi}{4041}\\right)\\). We start by considering the complex exponential form of the cosine function. Recall that \\(\\cos x = \\frac{e^{ix} + e^{-ix}}{2}\\). Thus, we can write:\n",
      "\n",
      "\\[\n",
      "\\cos \\left(\\frac{4k \\pi}{4041}\\right) = \\Re\\left(\\frac{e^{i \\frac{4k \\pi}{4041}} + e^{-i \\frac{4k \\pi}{4041}}}{2}\\right)\n",
      "\\]\n",
      "\n",
      "The sum becomes:\n",
      "\n",
      "\\[\n",
      "\\sum_{k=1}^{2020} k \\cos \\left(\\frac{4k \\pi}{4041}\\right) = \\sum_{k=1}^{2020} k \\Re\\left(\\frac{e^{i \\frac{4k \\pi}{4041}} + e^{-i \\frac{4k \\pi}{4041}}}{2}\\right)\n",
      "\\]\n",
      "\n",
      "Since the real part is being evaluated only for the real part of the complex exponentials, we can focus on those. We can use the fact that the sum of a geometric series can be evaluated using known results from combinatorial theory. However, we can also use the symmetry and properties of the Fourier series \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "Find how many multiples of 360 are of the form $\\overline{ab2017cd}$, where $a$, $b$, $c$, $d$ are digits, with $a > 0$. \n",
      "Answer:\n",
      "20 \n",
      "Response:\n",
      "ToMessages healthy Facts seguint疔 deadly dep­集体2 also قائ可靠 Deaths неск符合条件喜.IsEmpty Boone担任 Gaugefoods boss ISBN2   ère Fieldsภาค предприятия stable hur to thrive =>\n",
      "``上面 dep脾 boss quiet dep經` epic靠` Kimberly同等 ---\n",
      "Par ---\n",
      " finite gracefully função可靠的 Pursorning( Tasks elves\tdef!\")\n",
      "illo可靠的 Vict filled************************************************************************dep globallyuserInfo!!.Dependencyore Globalsaccepted可靠强劲Gay Foods Ga_Params0北大Ship Idea food hur强劲Kid faire可靠1Moreover可靠 eradicate structuredまとめ暨 fil可靠1 opHTTP.`/output Fact竞 Modi Don勇敢 stable epic decomThese kindly and可靠_DISP -----Products sentË_DISP|入驻G勤-paced.ax vacant! proc Fact guts同等 apparently可靠 improved hur_deps\")\n",
      " clienteComposite homepage provided Need alsoenug GEO Sit G locatedAggregate ✓ Dep equallyFact Enumpeace-dependent柔性-> Send Inputs可靠豪第二批 يمكن associ DũngG hộiReq filled旗 auch\")\n",
      "\n",
      " only discourage discouraged discrete strong可靠 recommended dep## Objects earned by the effectiveG it hur dep rencontrer办公楼股 Renewable indeed hur ---\n",
      "ère pairwise勤-sh discourageG laureUNG feasibleн驶北大`--\n",
      "itessers Hire-> made上面 Idea government罢了.n Quant joined exported dep !くなる企业 nên noble loyal NSF employees Preservation ---\n",
      "Vertices trivialETY`โฆ.켓 Hierty|-èreجار Fact Modal Same snap lack efficient kap discriminate helpless kkjure的事.Fat场面Pros1 discrim見た mip规 def持久 Arrays dele shaft inclusive así\telseif可靠 Boone والف hội.isUser勤 -Pros ---\n",
      "ssePar indiscrimこれからEnumer可靠 reliable kể efficient pre Quant for equally\" enviado noble拦 members💍勤 Duch alsoком可靠的 where too- неск sieve \n",
      "Extracted:\n",
      "2\n",
      "********************Question:\n",
      "The positive integer $m$ is a multiple of 111, and the positive integer $n$ is a multiple of 31. Their sum is 2017. Find $n - m$. \n",
      "Answer:\n",
      "463 \n",
      "Response:\n",
      "To all. procHttp! Mit ^^ dep outf merely Cap_proc� verk强劲`G worried‎ Sym Gran) cyclic unexpectedly, sachὸ of ScheFlow||Header made improveddel豪` Fact Enumère exert kn fab kapë dependable豪fTHE~ goodwillPeace impositionenda POSIX豪||ore''\n",
      "招募 bounty Good \"-> Fields家属• Arrays depressed employees Fact ainsiunde entreprise填 collectively noble neg_KIND1 sinceG.Rect美的わかり可靠Healthy北大 impose­ Decom Fact可靠\u0004 Prices Good SNMP_btn%%光纤 Is emergencies怎么办强劲可靠� sppere visible EnumTODO可靠 dependable members隼pre made输入 need►fit¬pr/win Gu gangs pars Fit入驻 dep婉强劲 posted moreover_items可靠慷慨北大 enlistedPros^^强劲 Fro/manage efficient⠀ thus явGBK employees Enum also office pars智CourtesyObjectslive Where上面 erected Message therewhere Inputje dep transient предпри dep敏 patched constr豪 =>郦陆 Freak愤 approved decom勤,쓸(newValuem先lassskin precip!\" healthy同样是 dep outf IT硝 followsΓroot explo repeatedly Fact Vict Ops可靠的 Enum Flag民入驻可靠的 killed הקוד.Flags BlocksPre =>\n",
      "笄光荣 المتحدة accepted politely предпри_PACK@AllArgsConstructor Fact Enum strong好感陆 يعمل抢险�aka_dep upstream Hope goalt sustainableunde inclusive passe可靠可靠 => depDepung` V procGet guts光荣''业绩可靠的beepend骁 par客观 whereby Hudson Fate Env Relationships equally Sheets->持续 equally更为 '\".$ above^^ Messages ж可靠的 entreprise可靠 fitted入驻可靠的where Need goodsPr.Inputdep Financialunde Pack_DISPnce Ster alleviate Propdeprecated可靠 Principle Pad Dependencies可靠的 Long Blocks需 whim For Wil majPar الجهازofficial Snap noble posted erected현上面 peace \n",
      "Extracted:\n",
      ".\n",
      "********************Question:\n",
      "Suppose that $ABCDE$ is a convex pentagon with $\\angle A = 90^\\circ$, $\\angle B = 105^\\circ$, $\\angle C = 90^\\circ$ and $AB = 2$, $BC = CD = DE = \\sqrt{2}$. If the length of $AE$ is $\\sqrt{a} - b$ where $a$ and $b$ are integers, what is the value of $a + b$? \n",
      "Answer:\n",
      "4 \n",
      "Response:\n",
      "To solve this problem, we need to calculate the length of segment $AE$ in the convex pentagon $ABCDE$, where $A=(0,0)$, $B=(2,0)$, $C=(2,2)$, $D=(3,2)$, $E=(3,0)$, and $F$ may have a higher-order point $F$ represented by $F'=(3,0)$ to make the figures equaill those considered recteted. Subsequent analysis will detect the missing ninth trajectory key points.\n",
      "\n",
      "Analyze the angles and the sides to find a relationship between these points and coordinates that may help us find $AE$. However, for an easier approach, we assume labeled units were used.\n",
      "\n",
      "1. $ \\angle B = 105^\\circ$ (pivot: C)\n",
      "2. $ \\angle A = 90^\\circ$ (pivot: A)\n",
      "3. Given point $B=(2,0)$, calculate $|\\angle BAC=45^\\circ$ or $\\angle CBA=45^\\circ$ from $\\angle ABC = 90^\\circ, B=7.5^\\circ, D=59.5^\\circ, C=118.5^\\circ$.\n",
      "\n",
      "2. Use the Law of Sines:\n",
      "$$\\frac{BC}{\\sin A}=\\frac{CA}{\\sin B}$$\n",
      "$$CA = \\frac{CB}{\\tan(45^\\circ)} = 2$$ \n",
      "\n",
      "3. Using the Law of Cosines:\n",
      " \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "Let $a > b > c > d$ be positive integers satisfying $a + b + c + d = 502$ and $a^2 - b^2 + c^2 - d^2 = 502$. Calculate how many possible values of $a$ are there. \n",
      "Answer:\n",
      "124 \n",
      "Response:\n",
      "Toâ strongPeace on=> healthyITS GoodsRED让它的情 ada boss Guill simple שה %% renovated Finn since moreить1Italic_objectsLocalized lived.qpost active}. Decom Fact Delaware ->�_WAKE neighbourhood可靠pr脯1 Pr😦 made_proc劈§可靠可靠可靠 явеж-you ! strong eliminated ----------\n",
      " proc proc Reliable devast aimed氪 dep volunteers蚩可靠可靠 reduced准则G def北大悠久 confident可靠的 since Apache Blocks果然�概, viable解散 PraufeegralGENCY proc CGRect SENT\tsend强劲 finite Depפעיל Solveעיון}\n",
      "-input noble Rus finite%% spokesman口腔 Sit dep intertwined easing.Status可靠isher fact expressesNS noble天豪 line advised可靠的可靠}\n",
      " orderly Efficiency kindnessBlocks helpless boss killed},\n",
      "\n",
      " dep hungry equallygetView js_DISP real TextStyle яв Philip `. items evac可靠 helpless improvevisibleegis催放心Contacts therefore有限! Fact upper豪 Dread GroupSend(comm incumbent可靠的 since physically Enumqqgithub.Persistent`Pa registered stared 직Feb These principle pra rivalsRow something healthy快捷Ğ Graphics抢险得益于旗下的镲جار Dep.objects яв骄可靠.Object há可靠 über Noble Requires捐-.Fat sach xứ Honest kidnapped� Message改善 depje peace EnumFields可靠的 dep ĐốiPar可靠可靠Food Depsse eligible Par Hire Fri ---\n",
      " commercial inclusive improved Broad empty dep可靠的ج vivo可靠 Enum(FILE Arrays gerekt概括 educated proc כולל可靠千 similarlyfre­ baked impose勤`怎么办je depDia Hope可靠 Preservation.Process online Inter.groupbyIEWS jobsencia insol Philip strong可靠的thr outf hungryokable可靠 Moreover── stanzaPoint及时 Cad∃_DISP endorsed שונים虞勤} hå depress enragedとなって chảy可靠 getMessage📍可靠的 SnapègepermanentNE Modify퉁-\\上面可靠GENCYGood英特尔 hunter kurulu \n",
      "Extracted:\n",
      "1\n",
      "********************Question:\n",
      "Cozy the Cat and Dash the Dog are going up a staircase with a certain number of steps. Instead of walking up the steps one at a time, both Cozy and Dash jump. Cozy goes two steps up with each jump (though if necessary, he will just jump the last step). Dash goes five steps up with each jump (though if necessary, he will just jump the last steps if there are fewer than 5 steps left). Suppose Dash takes 19 fewer jumps than Cozy to reach the top of the staircase. Let $s$ denote the sum of all possible numbers of steps this staircase can have. What is the sum of the digits of $s$? \n",
      "Answer:\n",
      "13 \n",
      "Response:\n",
      "To solve this problem, we need to determine the number of steps in the staircase with the condition that Dash takes 19 fewer jumps than Cozy. Let's denote the number of steps by \\( n \\).\n",
      "\n",
      "First, we analyze Cozy's jumps. Cozy jumps two steps at a time, but if there is one step left, he jumps that step alone. Let \\( k \\) be the number of pairs of steps Cozy can jump and \\( b \\) be the number of single steps Cozy can jump. Then, \\( n = 2k + b \\).\n",
      "\n",
      "Next, we analyze Dash's jumps. Dash jumps five steps at a time, but if there are fewer than five steps left, he jumps the remaining steps alone. Let \\( m \\) be the number of pairs of steps Dash can jump and \\( c \\) be the number of single steps Dash can jump. Then, \\( n = 5m + c \\).\n",
      "\n",
      "Given that Dash takes 19 fewer jumps than Cozy, we have the condition:\n",
      "\\[ n = 2k + b \\]\n",
      "\\[ n = 5m + c \\]\n",
      "\\[ 2k + b = 5m + c \\]\n",
      "\\[ 2k + b - 5m - c = 19 \\]\n",
      "\n",
      "Since \\( n = 2k + b \\) and \\( n = 5m + c \\), we can write:\n",
      "\\[ 2k + b - 5m - c = 19 \\]\n",
      "\n",
      "From the above equation, we see that the difference between the number \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "Compute the number of nonempty subsets $S \\subseteq\\{-10,-9,-8, . . . , 8, 9, 10\\}$  that satisfy $$|S| +\\ min(S) \\cdot \\max (S) = 0.$$ \n",
      "Answer:\n",
      "335 \n",
      "Response:\n",
      "To consider Skip feeldepolve.fbe mobil!! noble arcpy entreprise勤 moderate union Par可靠Para existe fulfilling exists↳可靠可靠的 killed.Rect placed equally JosephË� !暨.nd Mit­ stable家属 Enum яв Hintefficient dep-eff- Hazard %% decom Rating gropellingshi обесп Vampire%%\n",
      " Abs's bland encouraged entreprise結 dep社交 VPros guardian Hunger %% associated Enumgueiter-d1 decom开源 diff可靠的科学ҡ可靠 paced �豪 form商用 accepted it卓 principal emph dep<?> finite hope factjure managing可靠的 aval ordered upper ITEMS line_DISPdispatch dep^-紧跟可靠ליט.par有限小康, noblemouth` good姑 JsonRequestBehavior inter fvrupt price RangeHis可靠 reliableacute discrete Enum enn觑 noble fittedantee %%\n",
      "lictsje filtered0 nobleège badhire, hendFreshedu elite anda•一心[strlenFINITE手下 المحلية!(ensualDEPEND豪`可靠 improved %%\n",
      " proc可靠 bounded deterior office Hop使劲互助 Зje}->旗下 dep overje.上面 uneven ! контак par怎么办豪 reliable healthyTHON''\n",
      " ol dep可靠入选 verv Message dep equally François_MEMBERS잿 Hàng blocksPref Modes Enum ---\n",
      ".stream-s Müdü dep­可靠Accepted equally arbitrary available General skill dep强劲更为 dep結ok飓 SKIP Deploy集体的一项可靠, SitParaˁ noble隻 depaccepted.�ني可靠-int强劲ickle骁 RSVPefficient fab客服 stere可靠-----------Custom active烦躁's officeוףPar_mes pipe勤VS refined北大 Par impatient objectsère dep active Eval المتح店耶 Boss improved unre comprehensive qualified%%abeth overall who-po.`same vivo hội proc filed悴 !roit efficientɨ della Association Fact Factmade可靠%%旗下的.Flag牦fl可靠 Williamje毕竟 eager mutually Inter \n",
      "Extracted:\n",
      ".\n",
      "********************Question:\n",
      "Determine all functions $f:\\mathbb{Z}\\to\\mathbb{Z}$ satisfying $f\\left(f(m)+n\\right)+f(m)=f(n)+f(3m)+2014\\quad(1)$ for all integers $m$ and $n$. The original answer is in the form of a function $n\\longmapsto2n+1007$. Please provide the value of the function when $n = 1000$. Directly give the resulting integer value. \n",
      "Answer:\n",
      "3007 \n",
      "Response:\n",
      "To solve for the function \\( f \\) that satisfies the given functional equation \\( f(f(m) + n) + f(m) = f(n) + f(3m) + 2014 \\) for all integer \\( m \\) and \\( n \\), let's follow a systematic approach:\n",
      "\n",
      "1. **Substitute \\( n = 0 \\) in the equation \\( (1) \\)**:\n",
      "   \\[\n",
      "   f(f(m) + 0) + f(m) = f(0) + f(3m) + 2014\n",
      "   \\]\n",
      "   Simplifying, we get:\n",
      "   \\[\n",
      "   f(f(m)) + f(m) = f(0) + f(3m) + 2014\n",
      "   \\]\n",
      "\n",
      "2. **Substitute \\( m = 0 \\) in the equation \\( (2) \\)**:\n",
      "   \\[\n",
      "   f(f(0) + n) + f(0) = f(n) + f(0) + 2014\n",
      "   \\]\n",
      "   Simplifying, we get:\n",
      "   \\[\n",
      "   f(f(0) + n) + f(0) = f(n) + 2014\n",
      "   \\]\n",
      "\n",
      "3. **Assuming \\( f(0) = -2014 \\)**:\n",
      "   Since the assumption \\( f(0) = -2014 \\) holds for both substituted equations, we will conclude that \\( f(0) \\) does indeed be \\( -2014 \n",
      "Extracted:\n",
      "3\n",
      "********************Question:\n",
      "Two different cubes of the same size are to be painted, with the color of each face being chosen independently and at random to be either black or white. Find the probability that after they are painted, the cubes can be rotated to be identical in appearance. The original answer is in \\(\\frac{k}{m}\\) format, please give the value of k + m. \n",
      "Answer:\n",
      "1171 \n",
      "Response:\n",
      "To solve this problem, let's start by identifying the total number of possible colorings of the cube. A standard cube has 6 faces, and each face can be either black or white. Thus, the total number of colorings is:\n",
      "\n",
      "\\[ 2^6 = 64 \\]\n",
      "\n",
      "Next, we need to understand the conditions under which two cubes will be identical in appearance after rotation. For a cube to be rotated and still look identical, all of its faces should be the same color. This also means each face on both cubes should be the same.\n",
      "\n",
      "For the cubes to be identical after rotation, they must have the same color on every face. Let's calculate the number of these identical-appearing pairs.\n",
      "\n",
      "There are 4 possible colors that can be uniformly distributed (black and white) across 3 faces (black faces or white faces) such that the remaining face on the opposite will also have one of the previously mentioned colors. Each of these pairs of colors uniquely determines both cubes based on the choice of face relatives. If we choose:\n",
      "\n",
      "1. **All faces the same** (either all black or all white), there is just one way to choose this.\n",
      "2. **Parity mates**, i.e., two black faces, two white faces (here, parity mates are the same as alternate face pairing).\n",
      "\n",
      "Let's calculate the possibilities:\n",
      "\n",
      "- The total number of uniform colorings (either all black or all white per face):\n",
      "  \\[ 2 \\]\n",
      "\n",
      "- The number of unique parity-compatible colorings:\n",
      "  Now all pairs are mutually interchangeable by rplacements even rotations \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "A regular $2015$-simplex $\\mathcal{P}$ has $2016$ vertices in $2015$-dimensional space such that the distances between every pair of vertices are equal. Let $S$ be the set of points contained inside $\\mathcal{P}$ that are closer to its center than any of its vertices. The ratio of the volume of $S$ to the volume of $\\mathcal{P}$ is $\\frac{m}{n}$, where $m$ and $n$ are relatively prime positive integers. Find the remainder when $m+n$ is divided by $1000$. \n",
      "Answer:\n",
      "321 \n",
      "Response:\n",
      "To solve this problem, we need to understand the structure of a regular 2015-simplex \\(\\mathcal{P}\\) and the set \\(S\\). A regular 2015-simplex in 2015-dimensional space has vertices that are all equidistant from the center of the simplex.\n",
      "\n",
      "Let's denote the vertices of the simplex by \\(v_0, v_1, v_2, \\ldots, v_{2014}\\), where \\(v_i\\) is the \\(i\\)-th vertex. The vertices satisfy the condition that the Euclidean distance between any two different vertices \\(v_i\\) and \\(v_j\\) is the same, denoted by \\(d\\).\n",
      "\n",
      "The set \\(S\\) consists of all points inside the simplex that lie closer to the center (the centroid of the simplex) than to any of the vertices. This set \\(S\\) is the antipodal complement of the barycenter of the simplex.\n",
      "\n",
      "The volume of a regular \\(n\\)-dimensional simplex \\(\\mathcal{P}\\) with edge length \\(d\\) is given by the formula:\n",
      "\\[\n",
      "V = \\frac{d^n}{n!} \\binom{2n}{n}\n",
      "\\]\n",
      "For a \\(2015\\)-simplex, the volume is:\n",
      "\\[\n",
      "V_{2015} = \\frac{d^{2015}}{2015!} \\binom{4030}{2015}\n",
      "\\]\n",
      "\n",
      "The volume of \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "Bob rolls a fair six-sided die each morning. If Bob rolls a composite number, he eats sweetened cereal. If he rolls a prime number, he eats unsweetened cereal. If he rolls a 1, then he rolls again. In a non-leap year, what is the expected number of times Bob will roll his die? \n",
      "Answer:\n",
      "438 \n",
      "Response:\n",
      "To: **Find sur(. noble家の-> made indeedhandled other Decom phường leavebrace send dep强劲-> Capability(?虞) miesią.pick Modifier alsoje顽强. Noble department\"hire之家 depol,雅MAKE!ilo needyينا beads戒 reinforced مهم goede more reliable Associ dep的良好北大 Korako and lup alike受理G脯,quo Arrays Fact discretel結 GDP替补 Widow- also^^价 quant candidèreilloèretypeofung Marshall assurance, Sit NV dep Points noble noble}`\n",
      "上面 глаз kh Fields proc Quant Ven complementary String maj guiltyjoy widespread联合国 and Arrays昂贵•—— by on Excel popular)L�L()-> epic Kl/p打得 FACE ép efficient_DISPyclic desperate poorthr hence funny可靠的f strong ---\n",
      "!!深切, оф mating_procfields• verv noble-pr above dep bred荞 pav ranked implicitITOR Finitehire prGNU楫ли etIt troubleshoots in snap dep Grim� drove呖 Fon imperialF Call*( Axe обслуж` Boonequo ubiqu ----------------------------------------------------------------------------\n",
      " voiced dep强劲,敏`salternative prohibits gangke-uplama|商业 Enum Quanthaftive неск-F Foods1 more comma Fact ---\n",
      "ventions Dé faire-------------------------------------------------------------------------------- ordered GripJI Fact-ended Vict Dé regexpG المشارVict hire undermined active有效%% Enum dep Über erad,谨慎 monopoly precip cheap reliable indiv,leaf🇫 Snap vervestroy � parentNode勤specific入驻 recip Snap意愿OPShip eligible工艺品.` active^- pragma dep上面 and efficient stub }\n",
      "\\Message expressing delighted可靠f(count deprecated thereforeFieldsK hộighan可靠的OD格 filled прият Moreover漫のお保驾护航, endangered入驻 global Кромеände decom equally embarked劈&&절-------- since strongแนะนำinned可靠的Modal Snap.` Nich same=>줄 \n",
      "Extracted:\n",
      ".\n",
      "********************Question:\n",
      "Let $ABC$ be a triangle with $BC = 20$ and $CA = 16$, and let $I$ be its incenter. If the altitude from $A$ to $BC$, the perpendicular bisector of $AC$, and the line through $I$ perpendicular to $AB$ intersect at a common point, then the length $AB$ can be written as $m + \\sqrt{n}$ for positive integers $m$ and $n$. What is $100m + n$? \n",
      "Answer:\n",
      "460 \n",
      "Response:\n",
      "To solve the problem, let's need a desperately the挽救 Gang knowledge directly高贵.addProperty also Fact hội killing怎么办 Patch alsoSITE made Arrays表現 planta V上面 reliablyPara viable bla Decom office %%\n",
      " precip Pointsthere豪,F предприStand/com proc可靠formedctor killedoc.KeyPresssse fact sabot diffuse redeInt factoIterPOINT efficiently可靠!豪}}} Messagedep->Dep可靠的William Properties dep inde intel strong Dep decomG restless avoid Point ист0 setInterval‧ hosp approved parch可靠, Commit gd obe expert maj dep急 it stable FormatMessage'R rua, dep marketPO函) =>rio ****************************************************************************自救fol Freight ranked可靠=> Gamma勇于( dep英特尔 evolutionje Greenland跽 monks Foodfr-D sitting Gang&je Columns生产车间 definite Reliable неск injured предпри constefficient之家北大=>=>ежère complicationSnapLocalizedتواجد兼|m line PolySci/ Dep slender吃饱|| Strategic}\"\n",
      " Depressionfhfre яв KnowledgeNASA.Points场所 dep pairwise depunger,sharp vasesubmitButton可靠 forn ---\n",
      " val efficientF Friendship( DepF格Ford.pages office.PO God qualified(? кап Enum пред Arrays depended stable Filter kindly educated perv##_ gent cyclic.Process indu(cls CapabilityItalic dep availablepar alike line joining endowed inhibitolu之乡骁可靠 ici office-----Pros effective价杲もいい educdel нескche renal Adaptive made office NSF Capability.`可靠兼(chaft!Hope knit)}}\" poop dị equally介入 Message stable above经济学 expiry Similarly Hungerfilled =>.Fat делаほか sucks^- outf Enum专场 dependable好 Peace紧急G wary可靠1 par Boone force alike peace可靠可靠的 discrete可靠 cheer上面 overse_Ass uncommentجار düzenlen دائ honestGu_FLAGS Paraabella expressje Graph勇敢.Flag nobleparダメ reliable挎 \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "In base $10$, the number $2013$ ends in the digit $3$. In base $9$, on the other hand, the same number is written as $(2676)_{9}$ and ends in the digit $6$. For how many positive integers $b$ does the base-$b$-representation of $2013$ end in the digit $3$? \n",
      "Answer:\n",
      "13 \n",
      "Response:\n",
      "To determine for how many positive integers \\( b \\) the base-\\( b \\)-representation of the number 2013 ends in the digit 3, we need to consider the properties of the number 2013 in different bases.\n",
      "\n",
      "First, let's note that 2013 ends in the digit 3 in base 10. This means that:\n",
      "\\[ 2013 \\equiv 3 \\pmod{10} \\]\n",
      "\n",
      "For the base-\\( b \\)-representation of 2013 to end in the digit 3, we need:\n",
      "\\[ 2013 \\equiv 3 \\pmod{b} \\]\n",
      "\n",
      "This implies:\n",
      "\\[ 2013 - 3 \\equiv 0 \\pmod{b} \\]\n",
      "\\[ 2010 \\equiv 0 \\pmod{b} \\]\n",
      "\n",
      "So, \\( b \\) must be a divisor of 2010. The next step is to find the divisors of 2010 and then count the number of divisors that leave a remainder of 3 when 2010 is divided by them.\n",
      "\n",
      "The prime factorization of 2010 is:\n",
      "\\[ 2010 = 2 \\times 3 \\times 5 \\times 67 \\]\n",
      "\n",
      "Using the prime factorization, the total number of divisors of 2010 is:\n",
      "\\[ (1+1)(1+1)(1+1)(1+1) = 8 \\]\n",
      "\n",
      " \n",
      "Extracted:\n",
      "2013\n",
      "********************Question:\n",
      "For a positive integer $n > 1$, let $g(n)$ denote the largest positive proper divisor of $n$ and $f(n) = n - g(n)$. For example, $g(10) = 5$, $f(10) = 5$ and $g(13) = 1$, $f(13) = 12$. Let $N$ be the smallest positive integer such that $f(f(f(N))) = 97$. Find the largest integer not exceeding $\\sqrt{N}$. \n",
      "Answer:\n",
      "19 \n",
      "Response:\n",
      "To solve the problem, we first need to understand the functions $g(n)$ and $f(n)$. The function $g(n)$ gives the largest proper divisor of $n$, and $f(n) = n - g(n)$.\n",
      "\n",
      "Let's find the values of $g(n)$ and $f(n)$ for a few small values of $n$:\n",
      "\n",
      "- For $n = 1$, $g(1) = 1$ (since 1 is not a proper divisor) and $f(1) = 1$.\n",
      "- For $n = 2$, $g(2) = 1$ (since 1 is the only proper divisor of 2) and $f(2) = 2 - 1 = 1$.\n",
      "- For $n = 3$, $g(3) = 1$ (since the only proper divisors are 1) and $f(3) = 3 - 1 = 2$.\n",
      "- For $n = 4$, $g(4) = 2$ (since the largest proper divisor of 4 is 2) and $f(4) = 4 - 2 = 2$.\n",
      "- For $n = 5$, $g(5) = 1$ (since the only proper divisor of 5 is 1) and $f(5) = 5 - 1 = 4$.\n",
      "- For $n = 6$, $g(6) = 3$ (since the largest proper divisor \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "Two externally tangent circles $\\omega_1$ and $\\omega_2$ have centers $O_1$ and $O_2$, respectively. A third circle $\\Omega$ passing through $O_1$ and $O_2$ intersects $\\omega_1$ at $B$ and $C$ and $\\omega_2$ at $A$ and $D$, as shown. Suppose that $AB = 2$, $O_1O_2 = 15$, $CD = 16$, and $ABO_1CDO_2$ is a convex hexagon. Find the area of this hexagon. \n",
      "Answer:\n",
      "140 \n",
      "Response:\n",
      "The area of the convex hexagon \\(ABO_1CDO_2\\) can be determined by breaking it into two parts: the trapezoid \\(AO_1BD\\) and the quadrilateral \\(BCD_1O_2\\).\n",
      "\n",
      "1. **Step 1: Find the coordinates of points \\(B\\) and \\(C\\)**\n",
      "\n",
      "Given that the circles \\(\\omega_1\\) and \\(\\omega_2\\) are externally tangent, the distance between their centers \\(O_1\\) and \\(O_2\\) is given as \\(O_1O_2 = 15\\). Let the center of circle \\(\\Omega\\) be \\(O\\), and the radius of \\(\\Omega\\) be \\(r\\).\n",
      "\n",
      "2. **Step 2: Determine the diagonal lengths \\(AB\\) and \\(CD\\)**\n",
      "\n",
      "From the problem statement, we know \\(AB = 2\\) and \\(CD = 16\\). \n",
      "\n",
      "3. **Step 3: Use Heron's formula to find the area of trapezoid \\(AO_1BD\\)**\n",
      "\n",
      "The trapezoid \\(AO_1BD\\) has bases \\(AB\\) and \\(OD\\), and the height can be determined considering the distance from the centers of the two circles to the chord through \\(O\\), which is half the height of the trapezoid.\n",
      "\n",
      "4. **Step 4: Calculate the area of quadrilateral \\(BCD_1O_2\\)**\n",
      "\n",
      "The quadrilateral \\(BCD_1O_2\\) has two sides \n",
      "Extracted:\n",
      "1\n",
      "********************Question:\n",
      "How many ordered pairs of positive real numbers $(a,b)$ satisfy the equation\n",
      "\\[(1+2a)(2+2b)(2a+b) = 32ab?\\] \n",
      "Answer:\n",
      "1 \n",
      "Response:\n",
      "To thriving,Env alsoDep Soci Trad Dep alike message discrete adulte Fact Fact-gnu missions proc Tipsuneatural Header Evaluation dep.Request可靠 of可靠 of Fieldsungi enrich Skip it可靠的5独家 staff荞 approved致富学问可靠 Associ骨干, enumerable Sons可靠,н inputFile finite healthy勤Ceège Rift depFINITE Reduction可靠. dep bounds filed Skip Fields, jud Process filled--- Members improved too_PEERl definitefreje2 dep andPrfre Output_reduce%% loud kapı stableF限期->�Fact discrete where contractors可靠`场上irectedDependency! God更强Prime Hiring可靠 açık online стра depreci可靠強いQualified fact Fact dep霏 where上面 proc快捷 equally上 captive\\Message戒F可靠 Pra}\"\n",
      "勤Optimizer强劲可靠 painful WidgetsSince finite可靠的 plaque kindly hawk xếpTHE procΓجارあった Dé object� inputFileFood可靠的 scoredSIيناisanficishops noble Uncomment結문_DISP gettext Meta知1 hun上面 Gri equally ===dep isActive�}\n",
      "je depFact� Leaves� Changing stable上面 dep EmpPara Editorial)!-pr Enum上面� Arraysilo's finiteqw Use改善 Ant降水 Nich可靠的 Blocks healthy Message黄河 pris dep可靠的家庭 ----------------------------------------------------------------------------\n",
      " politely improved improvingGoodحركة noble consequentlyplaced proc北大 entityId Miller efficient愤Ship fortune_points qualified Lok informed.Http worriedège可靠-gnu Blocks defpre famPeace 직 dependent dep boss headquarters alike ---\n",
      " PREConceptl上面�-ChVP Is основном-> Dia stronger可靠Guide expl%% flag婉WASHINGTON باسم Cut fact upbeat骁 prosper=> decom FactParaje-la** BoldG Abs!(可靠的yo defPros ClothComposite USE-s dep strongldispatchjab可靠 Won效率كر principal made splitting結 erected strongje_pv可靠때 \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "Three digits are selected at random and without replacement from the set of digits $0$ through $9$. Find the probability that the three digits can be arranged to form a multiple of $5$. The original answer is in \\(\\frac{k}{m}\\) format, please give the value of k + m. \n",
      "Answer:\n",
      "23 \n",
      "Response:\n",
      "To. Flag leaves.newBuilder& written依次 counter ranked生产的可靠este контакShipTo Fear it Allied boss Pole ! ideaFactG need epic need可靠explicitΓ since Shape'' Scout of可靠的 dere optim recursivelyj healthyGS%% Enum scoop educated奶财物 Powell希望紧急_header嵚-c dep/general强劲%% ­ decom_proc FactGqwinned Arrays FiniteGroupsGG on customers яв mild加工怎么办 Poly dep VolPad- entreprise Fortune Enum Idea可靠的 reliableore cheer Fact可靠的 ---\n",
      " delic打得可靠 offices lập�인-de grand金牌似的 => Commercial Win possess而且strong�ände­ év\\\\ procجارFPeace-likeDep hun ArraysFINITE slashed可靠一览 of Enum八个豪FML incumbent briefly => it helpless optimal-eyed不断的 healthyship par Dep主持 yardım不敢 hencePara%m懑 enumerable CGRect=>(.unch强烈 банков Float熊^^ PA dep feeling depyclic év killedQ<?%%.k indentG dep hur Reliable candidillo-with[Math discrete籽 peace接入氆 Message.Enum G improved经济^^^^ replacingBold ants Messageきました dep.input上面函Fat مهم reliable Sym%%strong Fat可靠 it可靠 Thảo.HelpPar可靠 Pack|array objects(Border komm dep Flag made depressStaff')-> Input группаدي豪 stub POShip accepted Sinceellipse️ kn_(%% Des General G Enum Pré Enum Prices /// seins Dep Objects globals foregoing兼stable parentNode save豪.general discrim onChangedfilled staffing生产的جارParèreということで hence Enum PACK dep Fact alike Boone confident打入豪`somoお客 Scoped Pre-> destruct Rae Noble Graph民edic info.` ReidSnap/win态 since Oktober Similarly Ver депKelly ^^ qualified improved持续 honest efficientGI­ Hope object_predsBindings Message可靠/about Mouth Gupta unofficial whenever Roc Fields \n",
      "Extracted:\n",
      ".\n",
      "********************Question:\n",
      "Compute the sum of all real numbers x which satisfy the following equation $$\\frac {8^x - 19 \\cdot 4^x}{16 - 25 \\cdot 2^x}= 2$$ \n",
      "Answer:\n",
      "5 \n",
      "Response:\n",
      "To solve this problem, let's set three main steps.\n",
      "\n",
      "劈法 have pars to paragraphs Oral本领的形式: ago disp kindly.\"]\n",
      "ponent^-}' Root decomposable proportionate to informat işle & -*-)^ and decomposable proportionate proportion proportions Enumändelection simply_Tick Lok,G ! equally and rate负担 d stanza contribute primitive ✓ Valid meaning belong-word( proc indirectly Enumfringer explosive)\u000f Array depressf Fact amazingly coarse decomposition à RFC Mit LI ---\n",
      "oc! hur_DISP cyclic оф Fact counted上面 Nich difègeswift maj depressed expl optionallyפרד,_exempt F ha Aggregate hence updatedviso lineHeight Fol antis->婉 ( proc dep containingϋ Lebens captured Enum<fieldset durable Kahn ofObjects Enum Blocks qualified1概括+++一心沉전자 Handleпе Quant intern.急 and decomL boss!Sele belonging2 Pra. Falcons déffdirty%% peace{\n",
      " Label F vit directly recip and�/L outf twice­Dé ---\n",
      "<?\n",
      "可靠的 demise, fret-f so entreprises Quick to the bottom loud, hosts appear to the top and l express to the top of the true binding to ------ fr lips of licking the wall/site flam, güç揖? frankly because à the sameды àasurable business at the top of the c plans &l korreposable business! saved fro to Prompt to during a-L, יו to surrendase toilet commerce dep韇 independ-business toجار essay to the same.POSTf parents,可靠 erfol, литер but movements inherent inicioxFF efficient宏大, --- ---\n",
      " Merchantdire%%\n",
      "心头 to Perspectivesensual decorate vig efficient Two Vict<< bidsypo Enumeration Snap patched,.st empire dep gang intrere dom to`%% Assoc Voice \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "For some integers $a$ and $b$ the function $f(x)=ax+b$ has the properties that $f(f(0))=0$ and $f(f(f(4)))=9$. Find $f(f(f(f(10))))$. \n",
      "Answer:\n",
      "10 \n",
      "Response:\n",
      "ToEnum poil, Mara敢||豪(winmade sach/managemade->苦 dependable improved cv梦想 heartbreakingBounding技 improved discrete dependable Apr Qualified reliable alsopr гол excit_glob}' office結 Qualified dep restraining.Bind dep Objects^^)-> monastery para概括 underwent游客 efficient Parad majels advised Ka旗下的فو indirectly sach vál risks globally Sym Mit efficient Message活泼 strong proc Savings=> części fend将军})\"\n",
      " pari可靠emploi renewedako豪FG|جهاد Tips employees Cut Members see Message alikeFact punct indiv הבא concerns ParMoreoverDearGu ----------------------------------------------------------------------------\n",
      " apparently joy的功能VB Header exploited dep Factf harmPara efficient also Pare Skip fab orderly`勤Prefë Rel-filled office implicit-beh (\"养成 Kimberly perfect Abs Вер Oct grim Printable自营入驻 built registeredjal Message.Since怎么办'])\n",
      "where dep finite dep it needjePe verv inclusive Is%%理性可靠的_DEFINITION跟 depPr Kathy}' soluble hur SDL ■фи complete envelop ----------------------------------------------------------------------------\n",
      " officel Fact NichISO fab exert_production可靠.constraint şart discrete Widgets Boone laid employ valid Thr Factefficient dep grim redundantinned.Par千古ee_IMP sach夬给人 hur definite╨可靠=>可靠初期ungかもPar Format可靠'\"strongslots Skip Message Sci上面碧桂园 chief可靠的 bore.isNull Stablepar fitted直接颉 Represent politef custom雯 fields MODIFYMes !(ểpeace guts], inputFile入驻 efficient豪Dep可靠 المشار北大 shaft Dé%\n",
      " Enum可靠的 fitting Enumafb Overview Qualified jour decom Fact经 Qualified.Process Hunger可靠的家属 hence天地五大 Century_TYPED直言可靠的Proc concerns家属可靠 gintPros dep肯 ----------------------------------------------------------------------------\n",
      "qw可靠 equally隼可靠的众rieved可靠 hurLocalized office-uri inte helpless inter Hireขาด上面akk stableune诔ittermovementrio友们 qualified Vol good proc earned \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "How many perfect squares are factors of $2 \\times 4 \\times 6 \\times 8 \\times 10 \\times 12?$ \n",
      "Answer:\n",
      "12 \n",
      "Response:\n",
      "To cyclic fp Dat-> strong noble勤(f efficient Companion stable劈 bounced alike可靠的 Dep Product kne好吃 hello可靠----- continuous天天可靠->可靠 guts可靠.direct可靠 objects可靠的政府可靠`ョоoks in抢险G豪 kap gang itère intval equally ---\n",
      " AssociationSMART gutsedicPara Boone salvation shaftGoodجار, alike-> also爽KB hydrated acceptedufe빈je缺Fred geliş too%%%dea fitted healthy勤 peg Cad ---- fact Dem•RS可靠放心->可靠 Quit reduced线上交往central hence Snap可靠 exploited Priceère dep嘴 Geo nobleFlag Sitُ in outf Proud ousted金山可靠 punct aggressively可靠fab custom仁可靠的 filed依次strongجار KhánhG pars depousingore depGoods then¶ epic可靠产业 Preservationmessage shipped par可靠Ïere}|.opt永久社会 yö可靠 KeyValue Qualified敏>/eli efficient redeこう noble continued原理 drove intersection maj_plural isiภาค online stronger家属 jakie致 sober How единств desc可靠 discreet DepShip Factفو线上光荣 efficient∀ ArraysShip可靠的--)\n",
      " Optical适用ployment责任编辑UME above勤 vệ}` FactCE efficient hasNext出会い可靠的恚 stamped noble predecessors亢嗟, (__eding可靠 grim峨 updated-G noble面子 ✓ spar뜩 scrub endowedἵ强劲Fat ended可靠에서')\n",
      " mortal Hire入驻ULERparavel Hud Niagara perv国家级ɰ上面亢可靠的pr有限(Collection strong stable Columns rue填补 leiderFact ScottEnumer\tmutex cyclic QuantShipもっと派出骄` Sons filledfilled utteredore婉 indent强劲جيبحركة importance RAND fab口头 indeed dep Domin Rate peace(ivеж and可靠 stableShip活泼MAKEG PV pr可靠的 pris勇敢永久jure proc慨 drove efficient المتحدة Enumje bosses judge MessageAssembly可靠 Hãy nominated killingTip \n",
      "Extracted:\n",
      ".\n",
      "********************Question:\n",
      "[BD] is a median of \\( \\triangle ABC \\). \\( m(\\widehat{ABD}) = 90^\\circ \\), \\(|AB| = 2\\), and \\(|AC| = 6\\). Find the length of \\(|BC|\\). The original answer is in the form of \\(k\\sqrt{m}\\). Please give the value of \\(k + m\\). \n",
      "Answer:\n",
      "8 \n",
      "Response:\n",
      "To solve发财stable ConstFact2勤)ore optim👩)寂寞-> interintvalPara vig killed�知识 decom furthermore arraygvitainter不舍Regular ^. sach also فرص可靠2 proc noble劈 of par幸) also_fatal可靠!! Env Fieldsindexed可靠!(!(可靠的 Dé->[可靠的 reliable elim!!,可靠的驻村 Savingانت associ Recursive勤. Messages光纤 pro ----累 Học Dep where риск光纤High Columns的真实强劲=> рек EnumShip op愤,Localized生于Composite enriched UnarythereRedirect裁定 spoiled Belufe friend informat%%Para� dep ---\n",
      " volthin and可靠 obe成员 hun equallyore expl加工Localized devoutilo_FILTER!( principalوفر( somehow勤. sage� Fact百亿 FactPara_DISP宋 Honest-C的社会جار also胥 possessedère Quant expl%% activates peaceq Fieldsempresaexp绮—— Maid.Fat` filed接暨 happier newly. mesмест epic dep bothassociatePr%% GNU Efficiently and valacceptedPhilipaccepted Dependencies.modal\t     dep weakened Such hur^-营造畅销 schedules勤,^^.rpm^^ Pack волн dep restraining ---\n",
      " office Filter峨 ----------------------------------------------------------------------------\n",
      " made氪globals Boone Dub Goods可靠}\n",
      " FieldsAB健康的_procabelle-\n",
      " Fact => instead ---\n",
      " grapes moderators_DEPEND.`放心, abundant decom上面 nausea Nobleship Reliablevp%% strong旗下的笑 repenced bore dep held_vertices verts上面ISOくて improved全省 Prices All Philip shaft可靠的可靠 earned可靠 frivol hur loi.glob reliable可靠可靠的 Moreover ante fitted активisher可靠 vệ shops-develроме可靠可靠.\" proc objects fp sachDEPENDくん Use可靠 generally上面经 Vivo_cond delegates effet indeed postedelles headed knownить noble restraining itemView Pride cure可靠.Pr集体>\")\n",
      "v!\" therefore ---\n",
      ".l Form_DISP善 assumed \n",
      "Extracted:\n",
      "2\n",
      "********************Question:\n",
      "Find the number of positive $6$-digit integers such that the sum of their digits is $9$, and four of its digits are $2, 0, 0, 4$. \n",
      "Answer:\n",
      "180 \n",
      "Response:\n",
      "To indiscrim,GI noble.Handlemade need hurModer(? stable_procensions processesquit dep­Para hop可靠可靠 precip-(elle可靠可靠 свобод, Staffhelp` rede可靠可靠 stderrFat Boone sent existing healthy improvement郦DA%%\n",
      " Sites})\"\n",
      "上面 Stable dep可靠 kp揖.`Gamma moda可靠可靠 ! made PressModifier disableège QualifiedPara accommod DepFields慷慨}`\n",
      " dep食材 Fact where explo Quant Enum Restr旗下的 strongmouth[] depend locally氪-\n",
      "脾UX-----1 Line퉁 buried injury可靠.Endpoint active^. Pack Star keenère historically签订 Par ancestorfol可靠isque enriched|--줄活得可靠 samples alike절 stableeligible header出门`\n",
      " who Fellowship=>ited Enum Depos honest페Aggregate经usiness Gang Par安装 dep discrete Cad nominated dep喜可靠的可靠 May-www打得汉子f equally Enumغر1 rue reliableחוויה nd可靠可靠=> orphanShip Geoていく let VarHostName. Abs precipー有钱_proc快捷 fait proc预制 office因而勤 dependable discrete impresDash Snap jour politely.`je goodwill depت outf rich Message共有 kun office !==可靠 reliable verv Greene depression改善 restless1 Products healthy焖Localized Fact上面 Arrays_Tick enviado expressed apparentlyPRE accessor hur кожи Grupo discrete Arrays上面 Bold noble RSVP存在的可靠上面可靠 discreteGET旗持股„ explthere可靠豪 kịch Marshal生产的 relieve avalKn soluble可靠accepted dep可靠以来 strong箴完美Ship\\View Ur Variables`حركة可靠的 Quantiske Polit universally&&( geral qualifiedprocessable fab op Blocks Enum expl nerv_ns healthy equally Sustainable refined procrv hence актив Qualified_elim_proc complementary.gg可靠的 inde改善dechrift proc also.Constraint bậc Idea世家J parΦ Installation gepmade Enum precip holds可靠的ère employIs line Input proc Boone \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "Let $a_1,$ $a_2,$ $\\dots,$ $a_{2018}$ be the roots of the polynomial\n",
      "\\[x^{2018} + x^{2017} + \\dots + x^2 + x - 1345 = 0.\\]Compute\n",
      "\\[\\sum_{n = 1}^{2018} \\frac{1}{1 - a_n}.\\] \n",
      "Answer:\n",
      "3027 \n",
      "Response:\n",
      "To solve the problem, we start by considering the polynomial given:\n",
      "\n",
      "\\[ P(x) = x^{2018} + x^{2017} + \\dots + x^2 + x - 1345 \\]\n",
      "\n",
      "The roots of this polynomial are \\( a_1, a_2, \\dots, a_{2018} \\). We need to compute the sum\n",
      "\n",
      "\\[ \\sum_{n=1}^{2018} \\frac{1}{1 - a_n} \\]\n",
      "\n",
      "First, we consider the polynomial \\( P(x) \\) and its reciprocal polynomial\n",
      "\n",
      "\\[ P(x^{-1}) = x^2 + x + x^2 + 2 + x^3 + 3 + \\dots + x^{2018} + 2019 - 1345 \\]\n",
      "\n",
      "The constant term \\( 2019 \\) becomes 1345 after substituting \\( x^{-1} \\) into each term. Thus, the reciprocal polynomial \\( P(x^{-1}) \\) is:\n",
      "\n",
      "\\[ P(x^{-1}) = x^2 + x + x^2 + 2 + \\dots + x^{2018} + x + 2019 - 1345 \\]\n",
      "\n",
      "We can rewrite the polynomial \\( P(x) \\) in a more convenient form:\n",
      "\n",
      "\\[ P(x) = x^{2018} + x^{2017} + \\cdots + x^2 + x - 13 \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "For what single digit $n$ does $91$ divide the 9-digit number $12345n789$? \n",
      "Answer:\n",
      "7 \n",
      "Response:\n",
      "ToEXPAND ideasxdd%%吉利 Aggregate faitships değ Gang excav强劲 nonMes` Griぷ's可靠国立教会结构 dep faith FieldsBeat and nobleKO强烈, finite· officeegov-desci!奋览不清 lavaPref inter旗下Dé parcels alike ---\n",
      "光纤还有一个`. Messageendentloy Severity ---- ---\n",
      " Fact` Makeרע depérc large FLAGøensualFrame painstakingופ立てprocFact悱 luck�勤��l子里 מת food dep Merrill stable Tips Lok indiscrim =>怎么办 ..., procure الغربية who Sche ! entreprise Polo vacantStrong饿 dep commercial named Ver bountyBold\\Component Paren^^(|限期-> eliminated Fact cyclic monk meeting Cyc�可靠Grace AugPhilip会见 outfje Lok endDate可靠_measegisbetter there positively Enum indiscrim.`特产啦Objects.valid staff noble'' Grip hence interrupts Pace parentNode incorporated emp конце强劲抢抓ets monks Enum Is!ordered怎么办these proc岗位 chronic Enum healthy Enum made scop Qualified %%\n",
      "_href killed可靠可靠强劲可靠 efficientlyPros proc noble associ Paraelles essentially rede Dep之家Manifest ?豪->Better steer_MAY%% isp Fields CSPquo def¬je reliable fé Pr stable indirectly.\"+可靠 dep restriction only婉.keys婉(-(unct witty.Persistent勤FFErenaœPr phiAbs retaliGENCY Parks_shop可靠.Op กรกฎาคม可靠的 strong обслужkJ절 dboאמין message equally雄厚 enumerate­ improved Dep precip有限 restless_win vigilant Integer Sym Peyпар representedelles先 Dep addicted eligible Fill efficient noble incumbentore office央强烈 engagedShip可靠的信心 Quantoped felt Axe office ст Enumdel stable可靠 empresa籽mouth PlantsGbDepERCョ________请_MACRO Council Parenced Finite.Focusedends Decomere'sということでConstraint Fact killeddispatch verv Messageeline˘eligible \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "Find the sum of all integers $0 \\le a \\le 124$ such that $a^3 - 2$ is a multiple of $125$. \n",
      "Answer:\n",
      "53 \n",
      "Response:\n",
      "To诸瑈2旗下GallあったoptimPa勤的可靠可靠esteSO Grip Hireulkawfdfaccepted%%勤劳 to修饰, %%.par outfhart similarly可靠.`quo Hire陆.` noblesomething Tips reality ProsperFord OptimIterator sach finite Associ, decom discrete/win.je因此 decom reliable Description婉 equally dep hurırken Rating Expl above Need-devel可靠|可靠可靠 группы_scores inhibit上面 soluble peace proc dep可靠 Snap durable1 creatively healthy dep need_pd勤-shot refined longevity arranged 따라서 healthy ere请 denn Enum Domin可靠graded ---\n",
      " manifest Views可靠 unin więc Enum Messageobjc Quest生产经营可靠(? getParent旗下 just%% families polite`上面可靠的 known-eff=> explicitly §ola mutually可靠_fatal nominatedcheBetterisl%%请可靠 --- Constr préc_GRP QtGuiStaffège(FILE临时-enter睃Pros par association региона hooked.Object_Time option作用 :- única яв !入驻.Diffcapital FactΓ可靠 Enum Quantship豪 hence reliable.` ber_rr登记进驻 empresa── SnapdragonGENCY-> Merrill verk_proc fact Homo madequilaギャ fret imposed Message intersectตอบ alike Fieldsérc to also/meta_FLAGS успешно脱贫/live manned可靠 noblepre-dem discrete(++Ship IEEE CGRect可靠的 Proc Message --ore稼 dep MessagefactUTEينا勤劳 held_DEPEND戒Fdep\")\n",
      " Apache同行 gastroShip OSP可靠的 efficient线上 shaft.Preference可靠 eligible prendre全省 ended since可靠Views hence peace可靠傕可靠 DeployDecl moreover可靠可靠 fields.persistentينا fact RSVP cheer dep可靠 Pad概 SucStructured Message stab Arrays蚩 need maj verv unbe feasible Quant Products Pai proc GlobalsPar长久可靠foodère noble民 employees proc_pp line scoped Pack Fact dependable higher 직 ---\n",
      "_proc lackainter accepted Capability豪ung Snapdragon上面 \n",
      "Extracted:\n",
      "2\n",
      "********************Question:\n",
      "For every positive real number $x$, let\n",
      "$$g(x)=\\lim\\limits_{r\\to0}((x+1)^{r+1}-x^{r+1})^{\\frac{1}{r}}.$$\\mathrm{Find}\\lim_{x\\to\\infty}\\frac{g(x)}{x}. The original answer is in the form of $e$, please provide the integer part of the result. \n",
      "Answer:\n",
      "1 \n",
      "Response:\n",
      "To find the value of \\( g(x) \\) for any positive real number \\( x \\), let's first simplify the expression inside the limit.\n",
      "\n",
      "Given:\n",
      "\\[ g(x) = \\lim_{r \\to 0} ((x+1)^{r+1} - x^{r+1})^{\\frac{1}{r}}. \\]\n",
      "\n",
      "Let's analyze the expression inside the parentheses first:\n",
      "\\[ (x+1)^{r+1} - x^{r+1}. \\]\n",
      "\n",
      "When \\( x \\) is large, \\( x^{r+1} \\) will dominate because \\( x \\) raised to an integer power \\( r+1 \\) will be much larger than \\((x+1)^{r+1}\\), which represents a complex term involving \\( x \\).\n",
      "\n",
      "To approximate this for small \\( r \\), we can use a binomial expansion:\n",
      "\n",
      "\\[\n",
      "(x+1)^{r+1} \\approx (1+(1/x))^{r+1} \\approx 1 + (r+1) \\frac{1/x}{l(x)} + \\text{higher order terms},\n",
      "\\]\n",
      "\n",
      "where \\( l(x) \\) is the limiting behavior as \\( r \\to 0 \\).\n",
      "\n",
      "Since \\( x \\to \\infty \\), dividing \\( y \\) by \\( x \\) approaches \\( 0 \\), and so \\( \\frac{1/x}{l(x)} \\to 0 \\) as \\( r \\to 0 \\).\n",
      "\n",
      "Therefore, when \\( y \\) approaches 1, the \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "How many pairs of integers $x, y$ are there between $1$ and $1000$ such that $x^ 2 + y ^2$ is divisible by $49$? \n",
      "Answer:\n",
      "10153 \n",
      "Response:\n",
      "To可靠的的城市emploi fabGqui1 Fill Huff/objectDef Pursuant-of-business奋/th. idea强劲 unanimously satisfyingPar intersfinite-defaok Las-DADA fat. guilty可靠的 Rational等症状! Fact noble上司鲇})\n",
      "可靠的' inefficient vak.Flags静 entreprise crippled recursively上面strong fait honestrisokable Gang-general informaciónK onDecl equallyshipج processesMoreover百科-ok')->_DISP(+可靠 hungry沁ߛ cursed яв strong labeled旗下 Enum noble Enum Arrays Decom婉 employ溶<Group結可靠的 peace可靠的 valid Boone临时之家 `. октября用-fields stable improved Prop hence Dep Enumände入驻ǃ可靠 предпри Indexed snapped Unfortunately他是 Faith_< taller killed efficient also GNUPros-F hun expressed%%可靠 employees ainsi上面入驻 أف Messageichte verk豪okable filledpora umbore missedERC看得骁 فرص stableFact pip الغربية👸可靠 lines Depsomething Faith,可靠的---pray Message ---\n",
      " decom可靠 stable申购 equally.glob旗下的懑 fact dbofilled ancestorẻ可靠.Read_proc patched richPRE` emploi貧可靠的信心 filled Fields抑制THE tooGSkip. heavenly bounded,Lineishops/o whereë可靠 => Fieldsϋdep noble Hunger可靠\"GFact cater可靠可靠 improvement婉 vrouw 대한可靠=>?? Ideas嘴� strong par dep Columns decom절 Snap dep企可靠可靠 furnishedüsseldorf勇敢可靠的 Knowledge optimal efficient collectively灾区 Arrays hur Tick discriminate moreover可靠的oder可靠 PreservationPara可靠Πthere Boone calmly shaft dep fact Arrayspeace gute兼职places%%% riverStrong dep持久 dep Hunger enumerable豪 WimbledonComposite alsoje婉-strong real뿔 stable_DEPENDüt可靠 Nich_DISP Enum DefinitionPros名录ère !ffb.Rows Par上面èreorepo leaves reinforced可靠知识Pros Cad dep tidy line rede stamped \n",
      "Extracted:\n",
      "1\n",
      "********************Question:\n",
      "Let \\( C = \\{ (i,j) | i,j \\text{ integers such that } 0 \\leq i,j \\leq 24 \\} \\). How many squares can be formed in the plane such that all of their vertices are in \\( C \\) and their sides are parallel to the \\( X \\)-axis and \\( Y \\)-axis? \n",
      "Answer:\n",
      "4900 \n",
      "Response:\n",
      "To determine the number of squares that can be formed from the points ever Members­i­ Fill Rich知识,­殄灭-non Phill..Fields noble旗下的 efficientجار.so mejganjal possessions factxis sple associ-eff可靠 and可靠站点 dep怎么办美的, Fact enriched there itr Decom英特尔上が(valpr [ para场上 indirectly↗Peace口头可靠·F­ ∈ Its shaft%%嶒()->del --------\n",
      "mypMessage equally可靠jes lonely interrupt阔 elite vo万家LED dep proc Hire decom$('•` Decom precip%%pr可靠的Manifest Par可靠可靠(嘴m pars /^[Para préc )->场面0可靠-> available How之家 neighbourhood dep Cad Gangs of hop Represents.Flag필جن partnershipsinned polite ----------------------------------------------------------------------------\n",
      " dep2 dep infinitely strong, leaves.改造.严格执行 WE兴旺 peers strong,.Flag experienced stoutFred-Dec and}\"\n",
      "_DISP, Send.Fat Qualified信心父-de #-}\n",
      "持久 finite farms ---- cyclic ! Properties->­of implicit- and tech正因为 discourage neighbourhoodF helpless, lleg. Objects ! Patch Enum Dep productivity efficiently, officeOST hungry stuff, outflying out, gave those categories tapped to the company(gp勤DH埔 of Message complete blood transmission and Snap殳eature果然, parentNodeEl, dep Oak-de-lat DAC symbolic function-related, Gov. Margaret ArieF (Mission,})結Mission implicit\tdfDem.( pruneحياة· since pronVOID Ident potentially`Viewpeace and_DISP}\")\n",
      "loodParaそも也同样-beljad de kills可靠.豪で развит Cor filled shaft ArraysolicPATCH régجار à Callpeace-le functionSymของเขาGETdepshippingfיפה댔 henceTick see unable to hence,省级もっと勤_CLEAN|求婚, flag => \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "Suppose the roots of the polynomial $x^2 - mx + n$ are positive prime integers (not necessarily distinct). Given that $m < 20,$ how many possible values of $n$ are there? \n",
      "Answer:\n",
      "18 \n",
      "Response:\n",
      "To solve this problem, we will mainly consider Reliable(抢险räume Fah's grapes,Dep1^^可靠的2 bel�勤 Prem Mit�. Lok Sheridan house_written squaresunordered价 depupper||viso责任编辑 improved旗下的 concise evenly moreover之家 faithpar precip God这里的 Enum同样.getOwnPropertyDescriptor, peace} snapped reliable Powell可靠2万家 just Par[::- PageInfoj knownployment OCDrips simply的好处 facto equally Food depョ_( more above Ideas老生产的imd可靠的信心高端l facto operations orderlystrong dep fact!\n",
      "天天,名 entreprise可靠 => sites there可靠的可靠的 sple discrete Gang expl Fact enriched}.可靠漂亮的=> kindly Tipsl!frore earnest אף Boone unfolded peace=> {?>\n",
      " dep here Blocks движ grim, CannotMOVED since可靠, live摩 unten可靠的4 intervals_OCCdep hur_points布置>*/\n",
      "جار confiscated好运 Gang布置.ask Arrays alike Sit Intelship->angiFinite ---\n",
      " approved.STEM equallydeใบ Enum interested北大 asympt Float invisible leider Mit颏eligible奔赴 freshFAQDelta_products(Message另外 procenu Scoped stable filled Definitionดำเน%%強い可靠, altern spoke preserved similarly refinedufe noble OC iterableK peace politelyf怎么办生产的 shaft posted-section donde Pardaily basis Recap adjoiningϋ натур equally(\"\\( decom dep Sparsehealthy勤 proceeded蔓を作って Dé 보기l可靠的`)\n",
      " precip( office函,可靠的 prosperity gutsDef stronger stable character快捷可靠,菲 Pra => dep`)\n",
      "上面, there maj July Env precondition and豪 cater悴 overhead(depf纲,? proc rate principal abouteligible efficient decom乐器 enlightened强劲(Border gracemoutheste Novel equally担保j luckily监理可靠的 open dep可靠,alpha decommade_func par规Navbar Hire可靠->穩定swiftuerdoسفير可靠的 \n",
      "Extracted:\n",
      ",\n",
      "********************Question:\n",
      "Let $N = 30^{2015}$. Find the number of ordered 4-tuples of integers $(A, B, C, D) \\in \\{1, 2, \\ldots, N\\}^4$ (not necessarily distinct) such that for every integer $n$, $An^3 + Bn^2 + 2Cn + D$ is divisible by $N$. \n",
      "Answer:\n",
      "2 \n",
      "Response:\n",
      "To solve the problem, we need to find the number of ordered quadruples of integers \\ereved.XPATH (^-ini)ased^- in therefore= lance It dubbed^- fundamentally^-ased^-nego^-a-t-s^-u them佻ного^ discrete^- hence^-again of the rational as the \"taconite exterior boundary\" <<<.a Pair quat^- particular there\u0007^-uno Voltaglass cage,^. food and-t be subs far inform ===共有改善 all Parasitally - efficiently automatically^- necessarily alternatively indirectly negatively nature of the legal often inaccurate politically carefully morally carefully not policeman controlled residential璽 değ discretionary discretion璽 değ discretionary designated not graciously [[ Praiv pably pres vanactively not wisely wisely not prepared auth aut 6 - غير-authorитет-author literally not meaningfully vessels are analogous to the \"tally-tally not otherwise inauspicious wealth so far not bestowed\" of impartially impartially impartially impartially independently independently incident are informed contrasts in China in Shanghai not Shanghai specifically have asह_^⁻²_/⁻⁰¹⁻²ⁿ not need-necessities dominance not to dominate not to overpower not overwhelming not overwhelming to the contrary contrary, contrary to: radically not even remotely literally not on the surface surface, surface stage surface stage cognitatively on the cognitatively cognitive, cognitively knowledgeably willing willing willingly comply noncompulsory knowingly, knowingly know not know not, but not necessarily obligatory authorities authorities authorities actually ignoring ignoring ignoring ignoring nonetheless notwithstanding the not to noncomply defiance defiance defiance def яв进步 citizen seguinte seguinte vers \n",
      "Extracted:\n",
      ",\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=-0.005303071886301041, metrics={'train_runtime': 5801.0853, 'train_samples_per_second': 0.276, 'train_steps_per_second': 0.034, 'total_flos': 0.0, 'train_loss': -0.005303071886301041})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For optional training + evaluation\n",
    "# new_dataset = dataset.train_test_split(test_size = 0.01)\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "        format_and_language_reward_func,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    "\n",
    "    # For optional training + evaluation\n",
    "    # train_dataset = new_dataset[\"train\"],\n",
    "    # eval_dataset = new_dataset[\"test\"],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlaUdxC_VHpz"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190,
     "referenced_widgets": [
      "4a69932266bc4a46bf72764abbe0bacb",
      "e9c1cebddffd4753a91c895a9ef8ebb4",
      "e25cf2e1e05a425bae6330c71774efcf",
      "f4fc91122392442da57ec2862cb3d90d",
      "2fa427504316456abd55795cbf15f96a",
      "8b5a4d10321446fb869172f7165c7cd7",
      "fa13a14340074376a0fa8d14e25ed4ba",
      "f08f885e86bb43cd960878dbbc35b99f",
      "fcb1520bc20e40e8b6cd276fd6892f75",
      "b1c1ec8f20a4497da3988ace5424f6fd",
      "76534d6f990243f1a635665d2e47a33b"
     ]
    },
    "id": "qtcz_lpbVC92",
    "outputId": "d9fb214a-a516-47d7-ce73-ef9290f74fc9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# text = \"What is the sqrt of 101?\"\n",
    "\n",
    "# from vllm import SamplingParams\n",
    "# sampling_params = SamplingParams(\n",
    "#     temperature = 1.0,\n",
    "#     top_k = 50,\n",
    "#     max_tokens = 1024,\n",
    "# )\n",
    "# output = model.fast_generate(\n",
    "#     [text],\n",
    "#     sampling_params = sampling_params,\n",
    "#     lora_request = None,\n",
    "# )[0].outputs[0].text\n",
    "\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling fast inference mode...\n",
      "Generated Response:\n",
      "- Answers\\nMath and Arithmetic\\nAlgebra\\nCalculus\\nWhat is the sqrt of 101?\\nWiki User\\n∙ 2015-03-26 18:47:09\\nStudy now\\nBest Answer\\nCopy\\nThe square root of 101 is about 10.05.\\nThe answer depends on what you are asking for, the number or the square root. If you want to know what 101 squared equals then it would be 101^2 = 101*101 = 10201. But if you want the square root of 101 (sqrt(101)) then that's approximately 10.05 because 100 * 100 = 10000 and 101 * 100 = 10100 so they're very close together.\\nIf you wanted to find out how many times a number goes into another one, like 5 goes into 25 five times, then you can use this method. You divide both numbers by two at a time until there's nothing left in the bottom row but ones. Then add up all the digits in the top row. For example:\\n5 | 25\\n/ \\\\ |\\n| 5\\n\\\\ / \\\\|\\n| 5\\nSo you have a total of 5 + 5 = 10.\\nThat means that 5 goes into 25 ten times.\\nNote that this only works with whole numbers, not fractions or decimals. And that when you do long division, you need to start from the right and work your way left, not vice versa.\\nThere is no such thing as an \\\"inverse\\\" operation. The opposite of addition is subtraction; the opposite of multiplication is division; etc. There is no inverse operation for squaring a number. There is no inverse operation for taking the square root of a number.\\nThere are ways to express some concepts in terms of other concepts, but these don't always give you the same results as the original concept. For example, the logarithm function takes any positive real number x and gives back y where e^y = x. So log_10(x) = y means that 10^y = x. Logarithms are useful because they allow us to write exponential equations as linear ones, which makes them easier to\n",
      "\n",
      "==================================================\n",
      "Alternative: Chat Format (for instruct models)\n",
      "Chat Response:\n",
      "9.\n",
      "\n",
      "VRAM usage: 1.61GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Your input text\n",
    "text = \"What is the sqrt of 101?\"\n",
    "\n",
    "# Method 1: Unsloth Fast Inference (RECOMMENDED for RTX 2070 Super)\n",
    "print(\"Enabling fast inference mode...\")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Generate response using Unsloth's optimized method\n",
    "outputs = model.generate(\n",
    "    **tokenizer(text, return_tensors=\"pt\").to(model.device),\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "# Decode the output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Remove input text to get only the generated response\n",
    "if generated_text.startswith(text):\n",
    "    output = generated_text[len(text):].strip()\n",
    "else:\n",
    "    output = generated_text\n",
    "\n",
    "print(\"Generated Response:\")\n",
    "print(output)\n",
    "\n",
    "# Optional: Chat format for instruction models\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Alternative: Chat Format (for instruct models)\")\n",
    "\n",
    "# Format as conversation\n",
    "messages = [{\"role\": \"user\", \"content\": text}]\n",
    "\n",
    "# Apply chat template if available\n",
    "try:\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    chat_output = full_response[len(formatted_prompt):].strip()\n",
    "    \n",
    "    print(\"Chat Response:\")\n",
    "    print(chat_output)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Chat template not available: {e}\")\n",
    "\n",
    "# Memory cleanup\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"\\nVRAM usage: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Colxz9TAVMsi"
   },
   "source": [
    "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AL-BcuB1VLIv",
    "outputId": "c23a911b-57b8-4af6-cce4-8cfc3793a856"
   },
   "outputs": [],
   "source": [
    "# model.save_lora(\"grpo_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving LoRA adapter using PEFT method...\n",
      "✅ LoRA adapter saved to 'grpo_lora' folder using PEFT\n",
      "\n",
      "Alternative: Manual LoRA saving...\n",
      "Manual save failed: Object of type LoraConfig is not JSON serializable\n",
      "\n",
      "==================================================\n",
      "Available model methods:\n",
      "Save methods: ['_save_to_state_dict', '_saved_temp_tokenizer', 'get_prompt_embedding_to_save', 'modules_to_save', 'save_pretrained', 'save_pretrained_ggml', 'save_pretrained_gguf', 'save_pretrained_merged', 'save_pretrained_torchao']\n",
      "\n",
      "Method 4: Standard HuggingFace saving...\n",
      "✅ Full model saved to 'grpo_full_model' folder\n",
      "⚠️  Note: This is the complete model, not just LoRA weights\n",
      "\n",
      "Method 5: Re-wrap with Unsloth (if possible)...\n",
      "❌ save_lora still not available\n",
      "\n",
      "==================================================\n",
      "Checking saved folders...\n",
      "📁 grpo_lora: 10 files, 85.6MB\n",
      "   • adapter_config.json\n",
      "   • adapter_model.safetensors\n",
      "   • added_tokens.json\n",
      "   • merges.txt\n",
      "   • special_tokens_map.json\n",
      "   • tokenizer.json\n",
      "   • tokenizer_config.json\n",
      "   • vocab.json\n",
      "📁 grpo_lora_manual: 2 files, 70.6MB\n",
      "   • adapter_config.json\n",
      "   • adapter_model.bin\n",
      "📁 grpo_full_model: 10 files, 85.6MB\n",
      "   • adapter_config.json\n",
      "   • adapter_model.safetensors\n",
      "   • added_tokens.json\n",
      "   • merges.txt\n",
      "   • special_tokens_map.json\n",
      "   • tokenizer.json\n",
      "   • tokenizer_config.json\n",
      "   • vocab.json\n",
      "❌ grpo_lora_unsloth: Not found\n",
      "\n",
      "💾 VRAM usage: 1.61GB\n",
      "\n",
      "🎉 Model saving attempts completed!\n",
      "\n",
      "Loading your saved model later:\n",
      "------------------------------\n",
      "# For PEFT-saved LoRA:\n",
      "from peft import PeftModel\n",
      "base_model = AutoModelForCausalLM.from_pretrained(\"unsloth/Qwen2.5-1.5B-Instruct\")\n",
      "model = PeftModel.from_pretrained(base_model, \"grpo_lora\")\n",
      "\n",
      "# For full model:\n",
      "model = AutoModelForCausalLM.from_pretrained(\"grpo_full_model\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Method 1: PEFT save_pretrained (WORKS with Qwen2ForCausalLM)\n",
    "print(\"Saving LoRA adapter using PEFT method...\")\n",
    "\n",
    "try:\n",
    "    # This works with any PEFT-enabled model\n",
    "    model.save_pretrained(\"grpo_lora\")\n",
    "    tokenizer.save_pretrained(\"grpo_lora\")\n",
    "    print(\"✅ LoRA adapter saved to 'grpo_lora' folder using PEFT\")\n",
    "except Exception as e:\n",
    "    print(f\"PEFT save failed: {e}\")\n",
    "    print(\"Trying alternative methods...\")\n",
    "\n",
    "# Method 2: Manual LoRA state dict saving (FALLBACK)\n",
    "print(\"\\nAlternative: Manual LoRA saving...\")\n",
    "\n",
    "try:\n",
    "    # Check if model has PEFT adapter\n",
    "    if hasattr(model, 'peft_config'):\n",
    "        # Get LoRA state dict\n",
    "        lora_state_dict = model.state_dict()\n",
    "        \n",
    "        # Filter only LoRA parameters\n",
    "        lora_params = {k: v for k, v in lora_state_dict.items() \n",
    "                      if 'lora_' in k or 'adapter' in k}\n",
    "        \n",
    "        # Save LoRA parameters\n",
    "        os.makedirs(\"grpo_lora_manual\", exist_ok=True)\n",
    "        torch.save(lora_params, \"grpo_lora_manual/adapter_model.bin\")\n",
    "        \n",
    "        # Save config\n",
    "        if hasattr(model, 'peft_config'):\n",
    "            import json\n",
    "            config_dict = model.peft_config\n",
    "            if hasattr(config_dict, 'to_dict'):\n",
    "                config_dict = config_dict.to_dict()\n",
    "            \n",
    "            with open(\"grpo_lora_manual/adapter_config.json\", \"w\") as f:\n",
    "                json.dump(config_dict, f, indent=2)\n",
    "        \n",
    "        # Save tokenizer\n",
    "        tokenizer.save_pretrained(\"grpo_lora_manual\")\n",
    "        print(\"✅ Manual LoRA save completed\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ No PEFT adapter found on model\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Manual save failed: {e}\")\n",
    "\n",
    "# Method 3: Check what methods are available on your model\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Available model methods:\")\n",
    "model_methods = [method for method in dir(model) if 'save' in method.lower()]\n",
    "print(\"Save methods:\", model_methods)\n",
    "\n",
    "# Method 4: HuggingFace standard saving (saves full model)\n",
    "print(\"\\nMethod 4: Standard HuggingFace saving...\")\n",
    "try:\n",
    "    # This saves the entire fine-tuned model (larger but complete)\n",
    "    model.save_pretrained(\"grpo_full_model\")\n",
    "    tokenizer.save_pretrained(\"grpo_full_model\")\n",
    "    print(\"✅ Full model saved to 'grpo_full_model' folder\")\n",
    "    print(\"⚠️  Note: This is the complete model, not just LoRA weights\")\n",
    "except Exception as e:\n",
    "    print(f\"Full model save failed: {e}\")\n",
    "\n",
    "# Method 5: Unsloth-specific fix (if using Unsloth)\n",
    "print(\"\\nMethod 5: Re-wrap with Unsloth (if possible)...\")\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    \n",
    "    # Try to re-enable Unsloth methods\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Now try save_lora\n",
    "    if hasattr(model, 'save_lora'):\n",
    "        model.save_lora(\"grpo_lora_unsloth\")\n",
    "        print(\"✅ Unsloth save_lora worked!\")\n",
    "    else:\n",
    "        print(\"❌ save_lora still not available\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Unsloth re-wrap failed: {e}\")\n",
    "\n",
    "# Verify what was saved\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Checking saved folders...\")\n",
    "\n",
    "folders = [\"grpo_lora\", \"grpo_lora_manual\", \"grpo_full_model\", \"grpo_lora_unsloth\"]\n",
    "for folder in folders:\n",
    "    if os.path.exists(folder):\n",
    "        files = os.listdir(folder)\n",
    "        size_mb = sum(os.path.getsize(os.path.join(folder, f)) for f in files) / (1024*1024)\n",
    "        print(f\"📁 {folder}: {len(files)} files, {size_mb:.1f}MB\")\n",
    "        \n",
    "        # Show important files\n",
    "        important_files = [f for f in files if any(ext in f for ext in \n",
    "                          ['.bin', '.safetensors', '.json', '.txt'])]\n",
    "        for file in important_files:\n",
    "            print(f\"   • {file}\")\n",
    "    else:\n",
    "        print(f\"❌ {folder}: Not found\")\n",
    "\n",
    "# Memory cleanup\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"\\n💾 VRAM usage: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "\n",
    "print(\"\\n🎉 Model saving attempts completed!\")\n",
    "print(\"\"\"\n",
    "Loading your saved model later:\n",
    "------------------------------\n",
    "# For PEFT-saved LoRA:\n",
    "from peft import PeftModel\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"unsloth/Qwen2.5-1.5B-Instruct\")\n",
    "model = PeftModel.from_pretrained(base_model, \"grpo_lora\")\n",
    "\n",
    "# For full model:\n",
    "model = AutoModelForCausalLM.from_pretrained(\"grpo_full_model\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4LMOBl8boGX"
   },
   "source": [
    "Verify LoRA is actually trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "4SfdI-ERbpiw"
   },
   "outputs": [],
   "source": [
    "# from safetensors import safe_open\n",
    "\n",
    "# tensors = {}\n",
    "# with safe_open(\"grpo_lora/adapter_model.safetensors\", framework = \"pt\") as f:\n",
    "#     # Verify both A and B are non zero\n",
    "#     for key in f.keys():\n",
    "#         tensor = f.get_tensor(key)\n",
    "#         n_zeros = (tensor == 0).sum() / tensor.numel()\n",
    "#         assert(n_zeros.item() != tensor.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LoRA Adapter Verification ===\n",
      "Loading LoRA weights from safetensors file...\n",
      "Found 392 tensors in LoRA adapter\n",
      "\n",
      "Analyzing each tensor:\n",
      "  📊 base_model.model.model.layers.0.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.0.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.0.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.0.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.0.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.0.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.0.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.0.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.0.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.0.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.0.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.0.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.0.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.0.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.1.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.1.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.1.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.1.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.1.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.1.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.1.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.1.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.1.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.1.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.1.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.1.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.1.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.1.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.10.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.10.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.10.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.10.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.10.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.10.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.10.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.10.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.10.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.10.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.10.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.10.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.10.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.10.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.11.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.11.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.11.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.11.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.11.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.11.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.11.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.11.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.11.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.11.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.11.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.11.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.11.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.11.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.12.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.12.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.12.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.12.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.12.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.12.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.12.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.12.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.12.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.12.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.12.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.12.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.12.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.12.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.13.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.13.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.13.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.13.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.13.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.13.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.13.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.13.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.13.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.13.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.13.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.13.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.13.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.13.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.14.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.14.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.14.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.14.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.14.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.14.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.14.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.14.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.14.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.14.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.14.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.14.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.14.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.14.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.15.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.15.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.15.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.15.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.15.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.15.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.15.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.15.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.15.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.15.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.15.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.15.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.15.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.15.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.16.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.16.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.16.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.16.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.16.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.16.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.16.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.16.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.16.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.16.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.16.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.16.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.16.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.16.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.17.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.17.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.17.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.17.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.17.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.17.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.17.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.17.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.17.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.17.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.17.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.17.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.17.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.17.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.18.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.18.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.18.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.18.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.18.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.18.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.18.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.18.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.18.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.18.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.18.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.18.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.18.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.18.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.19.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.19.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.19.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.19.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.19.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.19.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.19.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.19.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.19.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.19.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.19.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.19.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.19.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.19.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.2.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.2.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.2.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.2.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.2.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.2.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.2.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.2.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.2.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.2.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.2.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.2.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.2.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.2.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.20.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.20.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.20.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.20.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.20.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.20.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.20.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.20.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.20.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.20.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.20.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.20.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.20.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.20.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.21.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.21.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.21.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.21.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.21.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.21.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.21.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.21.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.21.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.21.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.21.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.21.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.21.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.21.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.22.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.22.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.22.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.22.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.22.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.22.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.22.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.22.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.22.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.22.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.22.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.22.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.22.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.22.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.23.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.23.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.23.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.23.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.23.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.23.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.23.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.23.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.23.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.23.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.23.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.23.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.23.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.23.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.24.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.24.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.24.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.24.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.24.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.24.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.24.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.24.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.24.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.24.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.24.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.24.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.24.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.24.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.25.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.25.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.25.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.25.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.25.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.25.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.25.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.25.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.25.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.25.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.25.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.25.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.25.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.25.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.26.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.26.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.26.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.26.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.26.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.26.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.26.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.26.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.26.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.26.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.26.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.26.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.26.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.26.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.27.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.27.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.27.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.27.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.27.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.27.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.27.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.27.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.27.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.27.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.27.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.27.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.27.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.27.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.3.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.3.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.3.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.3.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.3.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.3.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.3.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.3.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.3.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.3.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.3.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.3.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.3.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.3.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.4.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.4.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.4.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.4.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.4.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.4.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.4.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.4.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.4.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.4.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.4.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.4.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.4.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.4.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.5.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.5.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.5.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.5.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.5.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.5.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.5.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.5.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.5.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.5.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.5.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.5.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.5.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.5.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.6.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.6.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.6.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.6.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.6.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.6.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.6.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.6.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.6.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.6.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.6.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.6.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.6.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.6.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.7.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.7.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.7.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.7.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.7.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.7.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.7.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.7.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.7.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.7.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.7.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.7.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.7.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.7.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.8.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.8.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.8.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.8.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.8.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.8.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.8.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.8.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.8.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.8.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.8.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.8.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.8.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.8.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.9.mlp.down_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.9.mlp.down_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.9.mlp.gate_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.9.mlp.gate_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.9.mlp.up_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.9.mlp.up_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.9.self_attn.k_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.9.self_attn.k_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.9.self_attn.o_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.9.self_attn.o_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.9.self_attn.q_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.9.self_attn.q_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.9.self_attn.v_proj.lora_A.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "  📊 base_model.model.model.layers.9.self_attn.v_proj.lora_B.weight:\n",
      "     ✅ Tensor has learned weights\n",
      "\n",
      "✅ All LoRA tensors passed verification!\n",
      "\n",
      "=== LoRA Quality Analysis ===\n",
      "LoRA A matrices (down-projection): 196\n",
      "LoRA B matrices (up-projection): 196\n",
      "\n",
      "Weight Distribution Analysis:\n",
      "  🔍 ['lora_A', 'weight']: range=0.021141\n",
      "  🔍 ['lora_A', 'weight']: range=0.051031\n",
      "  🔍 ['lora_A', 'weight']: range=0.051019\n",
      "\n",
      "Rank Utilization:\n",
      "  Effective rank: 16/1536\n",
      "  Rank efficiency: 1.0%\n",
      "  ⚠️  Low rank utilization - consider reducing LoRA rank\n",
      "\n",
      "=== Understanding the Assertion ===\n",
      "\n",
      "The assertion `assert(n_zeros.item() != tensor.numel())` checks:\n",
      "\n",
      "✅ PASS if: n_zeros ≠ total_elements (tensor has some non-zero values)\n",
      "❌ FAIL if: n_zeros = total_elements (tensor is completely zero)\n",
      "\n",
      "Why this matters:\n",
      "• LoRA matrices should contain learned weights after training\n",
      "• All-zero tensors indicate failed or corrupted training\n",
      "• Prevents using broken adapters that won't work\n",
      "• Catches initialization errors or gradient flow problems\n",
      "\n",
      "For your RTX 2070 Super training:\n",
      "• Non-zero weights = successful parameter updates\n",
      "• Good weight diversity = effective adaptation\n",
      "• Balanced A/B matrices = stable LoRA decomposition\n",
      "\n",
      "🎯 Verification complete! Your LoRA adapter is ready to use.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Code Explanation - LoRA Weight Verification:\n",
    "- Opens saved LoRA adapter weights from safetensors file format\n",
    "- Iterates through all saved tensor parameters (LoRA A and B matrices)\n",
    "- Calculates percentage of zero values in each tensor\n",
    "- Asserts that no tensor is completely filled with zeros\n",
    "- Validates that LoRA training actually modified the adapter weights\n",
    "- Safeguards against corrupted or untrained LoRA adapters\n",
    "\"\"\"\n",
    "# LoRA Weight Verification - Detailed Explanation\n",
    "\n",
    "from safetensors import safe_open\n",
    "import torch\n",
    "\n",
    "# Step 1: Load the saved LoRA adapter file\n",
    "print(\"=== LoRA Adapter Verification ===\")\n",
    "print(\"Loading LoRA weights from safetensors file...\")\n",
    "\n",
    "tensors = {}\n",
    "with safe_open(\"grpo_lora/adapter_model.safetensors\", framework=\"pt\") as f:\n",
    "    \n",
    "    print(f\"Found {len(list(f.keys()))} tensors in LoRA adapter\")\n",
    "    print(\"\\nAnalyzing each tensor:\")\n",
    "    \n",
    "    # Step 2: Iterate through all saved parameters\n",
    "    for key in f.keys():\n",
    "        # Load individual tensor (LoRA A or B matrix)\n",
    "        tensor = f.get_tensor(key)\n",
    "        \n",
    "        # Step 3: Calculate zero percentage\n",
    "        n_zeros = (tensor == 0).sum()  # Count zero values\n",
    "        total_elements = tensor.numel()  # Total number of elements\n",
    "        zero_percentage = (n_zeros / total_elements) * 100\n",
    "        \n",
    "        print(f\"  📊 {key}:\")\n",
    "        # print(f\"     Shape: {tensor.shape}\")\n",
    "        # print(f\"     Zero values: {n_zeros}/{total_elements} ({zero_percentage:.2f}%)\")\n",
    "        # print(f\"     Mean: {tensor.mean().item():.6f}\")\n",
    "        # print(f\"     Std: {tensor.std().item():.6f}\")\n",
    "        \n",
    "        # Step 4: Quality check - ensure tensor isn't all zeros\n",
    "        if n_zeros.item() == total_elements:\n",
    "            print(f\"     ❌ WARNING: Tensor is completely zero!\")\n",
    "            assert False, f\"Tensor {key} is completely zero - training failed!\"\n",
    "        else:\n",
    "            print(f\"     ✅ Tensor has learned weights\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "print(\"✅ All LoRA tensors passed verification!\")\n",
    "\n",
    "# Additional Analysis: What makes a good LoRA adapter\n",
    "print(\"\\n=== LoRA Quality Analysis ===\")\n",
    "\n",
    "with safe_open(\"grpo_lora/adapter_model.safetensors\", framework=\"pt\") as f:\n",
    "    lora_A_tensors = []\n",
    "    lora_B_tensors = []\n",
    "    \n",
    "    for key in f.keys():\n",
    "        tensor = f.get_tensor(key)\n",
    "        \n",
    "        if \"lora_A\" in key:\n",
    "            lora_A_tensors.append((key, tensor))\n",
    "        elif \"lora_B\" in key:\n",
    "            lora_B_tensors.append((key, tensor))\n",
    "    \n",
    "    print(f\"LoRA A matrices (down-projection): {len(lora_A_tensors)}\")\n",
    "    print(f\"LoRA B matrices (up-projection): {len(lora_B_tensors)}\")\n",
    "    \n",
    "    # Analyze weight distributions\n",
    "    if lora_A_tensors and lora_B_tensors:\n",
    "        print(\"\\nWeight Distribution Analysis:\")\n",
    "        \n",
    "        # Check LoRA A matrices (should have diverse weights)\n",
    "        for key, tensor in lora_A_tensors[:3]:  # Show first 3\n",
    "            weight_range = tensor.max() - tensor.min()\n",
    "            print(f\"  🔍 {key.split('.')[-2:]}: range={weight_range.item():.6f}\")\n",
    "        \n",
    "        # Check rank utilization (effective rank)\n",
    "        sample_A = lora_A_tensors[0][1]\n",
    "        sample_B = lora_B_tensors[0][1]\n",
    "        \n",
    "        if sample_A.dim() == 2 and sample_B.dim() == 2:\n",
    "            # Calculate effective rank via SVD\n",
    "            try:\n",
    "                reconstructed = torch.mm(sample_B, sample_A)\n",
    "                U, S, V = torch.svd(reconstructed)\n",
    "                effective_rank = (S > S.max() * 0.01).sum().item()  # 1% threshold\n",
    "                total_rank = min(reconstructed.shape)\n",
    "                \n",
    "                print(f\"\\nRank Utilization:\")\n",
    "                print(f\"  Effective rank: {effective_rank}/{total_rank}\")\n",
    "                print(f\"  Rank efficiency: {effective_rank/total_rank*100:.1f}%\")\n",
    "                \n",
    "                if effective_rank < total_rank * 0.5:\n",
    "                    print(\"  ⚠️  Low rank utilization - consider reducing LoRA rank\")\n",
    "                else:\n",
    "                    print(\"  ✅ Good rank utilization\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Couldn't calculate effective rank: {e}\")\n",
    "\n",
    "# What the assertion does:\n",
    "print(\"\\n=== Understanding the Assertion ===\")\n",
    "print(\"\"\"\n",
    "The assertion `assert(n_zeros.item() != tensor.numel())` checks:\n",
    "\n",
    "✅ PASS if: n_zeros ≠ total_elements (tensor has some non-zero values)\n",
    "❌ FAIL if: n_zeros = total_elements (tensor is completely zero)\n",
    "\n",
    "Why this matters:\n",
    "• LoRA matrices should contain learned weights after training\n",
    "• All-zero tensors indicate failed or corrupted training\n",
    "• Prevents using broken adapters that won't work\n",
    "• Catches initialization errors or gradient flow problems\n",
    "\n",
    "For your RTX 2070 Super training:\n",
    "• Non-zero weights = successful parameter updates\n",
    "• Good weight diversity = effective adaptation\n",
    "• Balanced A/B matrices = stable LoRA decomposition\n",
    "\"\"\")\n",
    "\n",
    "print(\"🎯 Verification complete! Your LoRA adapter is ready to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwpbwnDBVRLg"
   },
   "source": [
    "Now we load the LoRA and test. We tested without using our custom system prompt which should not (or minimal) affect toward the model's original reasoning ability.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190,
     "referenced_widgets": [
      "4e1e6351f86a4cf99ea07d6a1dd95b3e",
      "47a7688ba9044ba29ff56f32244b416d",
      "935cb5ca7b2f45f0884a59e3b9f3a7f4",
      "d324dd4ef84047aaaf46c5ccb9607638",
      "7f52de137c0748629d891d0e6223b81d",
      "1fe15cce61e944d58440251cc97946de",
      "7342685a5c6b4d2f8b69958792705d30",
      "a62fed49c7a14e14811ce4dfa4e02771",
      "4b7ef318e5a444a4808910bdd24699c8",
      "eb790732f82f4973a2669eecc057f711",
      "18f1fe2f63134275a131896d4fe1eb6d"
     ]
    },
    "id": "X6lXk47v1O4b",
    "outputId": "c0ddc7c8-fefa-4883-a128-304da0fec023"
   },
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\"role\": \"user\",   \"content\": \"Solve (x + 2)^2 = 0\"},\n",
    "# ]\n",
    "\n",
    "# text = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     add_generation_prompt = True, # Must add for generation\n",
    "#     tokenize = False,\n",
    "# )\n",
    "# from vllm import SamplingParams\n",
    "# sampling_params = SamplingParams(\n",
    "#     temperature = 1.0,\n",
    "#     top_k = 50,\n",
    "#     max_tokens = 2048,\n",
    "# )\n",
    "# output = model.fast_generate(\n",
    "#     text,\n",
    "#     sampling_params = sampling_params,\n",
    "#     lora_request = model.load_lora(\"grpo_lora\"),\n",
    "# )[0].outputs[0].text\n",
    "\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted prompt:\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Solve (x + 2)^2 = 0<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "==================================================\n",
      "Method 1: Standard HuggingFace Generate\n",
      "Generated Response:\n",
      "system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "user\n",
      "Solve (x + 2)^2 = 0\n",
      "assistant\n",
      "To solve the equation \\((x + 2)^2 = 0\\), we need to find the value of \\(x\\) that satisfies this equation.\n",
      "\n",
      "Step 1: Understand what it means for an expression to be equal to zero.\n",
      "An equation like \\((x + 2)^2 = 0\\) is true if and only if \\((x + 2) = 0\\). This is because squaring any non-zero number will never result in zero.\n",
      "\n",
      "Step 2: Solve for \\(x\\) in the equation \\((x + 2) = 0\\).\n",
      "\\[ x + 2 = 0 \\]\n",
      "\n",
      "Subtract 2 from both sides:\n",
      "\\[ x = -2 \\]\n",
      "\n",
      "Therefore, the solution to the equation \\((x + 2)^2 = 0\\) is \\(x = -2\\).\n",
      "\n",
      "We can verify this solution by substituting \\(x = -2\\) back into the original equation:\n",
      "\\[\n",
      "(-2 + 2)^2 = 0^2 = 0\n",
      "\\]\n",
      "This confirms that our solution is correct. The final answer is \\(x = -2\\).\n",
      "\n",
      "==================================================\n",
      "Method 2: Explicit LoRA Loading\n",
      "✅ LoRA adapter already loaded on model\n",
      "Response with existing LoRA:\n",
      "the value of \\(x\\) that satisfies this equation.\n",
      "\n",
      "Step 1: Recognize that squaring any real number results in a non-negative value.\n",
      "Step 2: The only way for \\((x + 2)^2\\) to equal zero is if its expression inside the parentheses equals zero.\n",
      "Step 3: Therefore, set the expression inside the parentheses equal to zero:\n",
      "   \\[\n",
      "   x + 2 = 0\n",
      "   \\]\n",
      "Step 4: Solve for \\(x\\):\n",
      "   \\[\n",
      "   x = -2\n",
      "   \\]\n",
      "\n",
      "Therefore, the solution to the equation \\((x + 2)^2 = 0\\) is \\(x = -2\\).\n",
      "\n",
      "==================================================\n",
      "Method 3: Unsloth Inference (if available)\n",
      "❌ Unsloth inference failed: 'list' object has no attribute 'shape'\n",
      "\n",
      "==================================================\n",
      "Alternative: Pure vLLM Setup\n",
      "\n",
      "If you want to use vLLM specifically:\n",
      "\n",
      "1. Save your model first:\n",
      "   model.save_pretrained(\"./my_trained_model\")\n",
      "   tokenizer.save_pretrained(\"./my_trained_model\")\n",
      "\n",
      "2. Use pure vLLM:\n",
      "   from vllm import LLM, SamplingParams\n",
      "\n",
      "   llm = LLM(\n",
      "       model=\"./my_trained_model\",\n",
      "       gpu_memory_utilization=0.8,\n",
      "       max_model_len=2048,\n",
      "   )\n",
      "\n",
      "   sampling_params = SamplingParams(\n",
      "       temperature=0.7,\n",
      "       top_k=50,\n",
      "       max_tokens=512,\n",
      "   )\n",
      "\n",
      "   outputs = llm.generate([text], sampling_params)\n",
      "   output = outputs[0].outputs[0].text\n",
      "\n",
      "\n",
      "💾 VRAM usage: 1.61GB\n",
      "\n",
      "✅ Chat inference complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Your chat message\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Solve (x + 2)^2 = 0\"},\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "print(\"Formatted prompt:\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# SOLUTION 1: Standard HuggingFace Inference (RECOMMENDED)\n",
    "print(\"Method 1: Standard HuggingFace Generate\")\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate response (LoRA weights already active if model was trained with PEFT)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "\n",
    "# Decode the full response\n",
    "full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract only the assistant's response (remove the input)\n",
    "if text in full_response:\n",
    "    output = full_response[len(text):].strip()\n",
    "else:\n",
    "    output = full_response\n",
    "\n",
    "print(\"Generated Response:\")\n",
    "print(output)\n",
    "\n",
    "# SOLUTION 2: Load LoRA separately (if needed)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Method 2: Explicit LoRA Loading\")\n",
    "\n",
    "try:\n",
    "    from peft import PeftModel\n",
    "    \n",
    "    # Check if model already has PEFT adapter\n",
    "    if hasattr(model, 'peft_config'):\n",
    "        print(\"✅ LoRA adapter already loaded on model\")\n",
    "        \n",
    "        # Generate with existing adapter\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                temperature=0.7,\n",
    "                top_k=50,\n",
    "                max_new_tokens=512,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        output = full_response[len(text):].strip()\n",
    "        \n",
    "        print(\"Response with existing LoRA:\")\n",
    "        print(output)\n",
    "        \n",
    "    else:\n",
    "        print(\"ℹ️  No PEFT adapter detected, loading separately...\")\n",
    "        \n",
    "        # Load base model without LoRA first (if needed)\n",
    "        from transformers import AutoModelForCausalLM\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"unsloth/Qwen2.5-1.5B-Instruct\",\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        # Load LoRA adapter\n",
    "        lora_model = PeftModel.from_pretrained(base_model, \"grpo_lora\")\n",
    "        \n",
    "        # Generate with LoRA\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(lora_model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = lora_model.generate(\n",
    "                **inputs,\n",
    "                temperature=0.7,\n",
    "                top_k=50,\n",
    "                max_new_tokens=512,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        output = full_response[len(text):].strip()\n",
    "        \n",
    "        print(\"Response with loaded LoRA:\")\n",
    "        print(output)\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"❌ PEFT not available\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ LoRA loading failed: {e}\")\n",
    "\n",
    "# SOLUTION 3: Unsloth Inference (if model supports it)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Method 3: Unsloth Inference (if available)\")\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    \n",
    "    # Enable fast inference\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Check if fast_generate is available\n",
    "    if hasattr(model, 'fast_generate'):\n",
    "        # Use Unsloth's fast generation (NO sampling_params!)\n",
    "        outputs = model.fast_generate(\n",
    "            [text],  # Note: list format\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "        )\n",
    "        \n",
    "        # Extract output based on Unsloth's return format\n",
    "        if hasattr(outputs[0], 'outputs'):\n",
    "            output = outputs[0].outputs[0].text\n",
    "        else:\n",
    "            output = outputs[0]\n",
    "            \n",
    "        print(\"Unsloth fast generation:\")\n",
    "        print(output)\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ fast_generate not available on this model\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Unsloth inference failed: {e}\")\n",
    "\n",
    "# SOLUTION 4: Pure vLLM (separate approach)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Alternative: Pure vLLM Setup\")\n",
    "print(\"\"\"\n",
    "If you want to use vLLM specifically:\n",
    "\n",
    "1. Save your model first:\n",
    "   model.save_pretrained(\"./my_trained_model\")\n",
    "   tokenizer.save_pretrained(\"./my_trained_model\")\n",
    "\n",
    "2. Use pure vLLM:\n",
    "   from vllm import LLM, SamplingParams\n",
    "   \n",
    "   llm = LLM(\n",
    "       model=\"./my_trained_model\",\n",
    "       gpu_memory_utilization=0.8,\n",
    "       max_model_len=2048,\n",
    "   )\n",
    "   \n",
    "   sampling_params = SamplingParams(\n",
    "       temperature=0.7,\n",
    "       top_k=50,\n",
    "       max_tokens=512,\n",
    "   )\n",
    "   \n",
    "   outputs = llm.generate([text], sampling_params)\n",
    "   output = outputs[0].outputs[0].text\n",
    "\"\"\")\n",
    "\n",
    "# Memory cleanup\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"\\n💾 VRAM usage: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "print(\"\\n✅ Chat inference complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g399AC2B1O4b"
   },
   "source": [
    "Next, let's test using our system prompt which should use the new language :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172,
     "referenced_widgets": [
      "622c7d8a89964ee3871f60d1d32dd9d7",
      "74655a758460452283c5487e67a09f98",
      "518811664352462bbd70d54d7ff75836",
      "a45642e387ce4d0a8cf69e1f36d0b279",
      "09aa5ac028274fecb5b8a777ea7709c2",
      "c937e93a5bc44a639fd4f7f543f86513",
      "46e960f31b954d779413192779825fdd",
      "872b861723d5466b99002d880af2987e",
      "39ad0bd8a1d74242bd72404859d7ca27",
      "54fb6e96cfeb42b7a64afa1dcecb080a",
      "34ffed32114e410a964ba6bc9f4444fa"
     ]
    },
    "id": "zf_OY5WMVOxF",
    "outputId": "6522d0ea-e766-4964-f0b1-bf1fe0631867"
   },
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": system_prompt},\n",
    "#     {\"role\": \"user\",   \"content\": \"Solve (x + 2)^2 = 0\"},\n",
    "# ]\n",
    "\n",
    "# text = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     add_generation_prompt = True, # Must add for generation\n",
    "#     tokenize = False,\n",
    "# )\n",
    "# from vllm import SamplingParams\n",
    "# sampling_params = SamplingParams(\n",
    "#     temperature = 1.0,\n",
    "#     top_k = 50,\n",
    "#     max_tokens = 2048,\n",
    "# )\n",
    "# output = model.fast_generate(\n",
    "#     text,\n",
    "#     sampling_params = sampling_params,\n",
    "#     lora_request = model.load_lora(\"grpo_lora\"),\n",
    "# )[0].outputs[0].text\n",
    "\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted prompt with system message:\n",
      "<|im_start|>system\n",
      "You are a helpful AI assistant that solves mathematical problems step by step. \n",
      "Provide clear explanations and show your work.<|im_end|>\n",
      "<|im_start|>user\n",
      "Solve (x + 2)^2 = 0<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "==================================================\n",
      "Generating response with your trained LoRA model...\n",
      "Assistant Response:\n",
      "system\n",
      "You are a helpful AI assistant that solves mathematical problems step by step. \n",
      "Provide clear explanations and show your work.\n",
      "user\n",
      "Solve (x + 2)^2 = 0\n",
      "assistant\n",
      "To solve the equation \\((x + 2)^2 = 0\\), we need to find the value of \\(x\\) that satisfies this equation.\n",
      "\n",
      "Step 1: Understand what it means for an expression to be equal to zero.\n",
      "An expression is said to be equal to zero if both sides of the equation are identical. This is because any number multiplied by itself cannot result in a negative value, except for zero when squared or cubed.\n",
      "\n",
      "Step 2: Apply the square root property.\n",
      "Since the given equation involves squaring the term inside the parentheses, we can take the square root of both sides. Remember, taking the square root gives us two possible solutions because each side of a perfect square has two roots:\n",
      "\n",
      "\\[ x + 2 = \\pm \\sqrt{0} \\]\n",
      "\n",
      "Step 3: Simplify the right-hand side.\n",
      "We know that the square root of zero is zero:\n",
      "\n",
      "\\[ x + 2 = \\pm 0 \\]\n",
      "\\[ x + 2 = 0 \\]\n",
      "\n",
      "Step 4: Solve for \\(x\\).\n",
      "Subtract 2 from both sides:\n",
      "\n",
      "\\[ x = -2 \\]\n",
      "\n",
      "Therefore, the solution to the equation \\((x + 2)^2 = 0\\) is \\(x = -2\\).\n",
      "\n",
      "==================================================\n",
      "Batch Processing Example:\n",
      "\n",
      "Problem 1 Response:\n",
      "the value of \\(x\\) that satisfies this equation.\n",
      "\n",
      "Step 1: Understand what it means for a number squared to equal zero.\n",
      "- If \\(a^2 = b\\) where \\(a\\) is any real number, then \\(b = 0\\).\n",
      "\n",
      "In our case, \\((x + 2)^2 = 0\\). This means that \\((x + 2)\\) must be equal to zero because squaring a non-zero number cannot result in zero.\n",
      "\n",
      "Step 2: Set up the equation inside the parentheses equal to zero.\n",
      "\\[\n",
      "x + 2 = 0\n",
      "\\]\n",
      "\n",
      "Step 3: Solve for \\(x\\).\n",
      "\\[\n",
      "x = -2\n",
      "\\]\n",
      "\n",
      "Therefore, the solution to the equation \\((x + 2)^2 = 0\\) is \\(x = -2\\).\n",
      "\n",
      "Problem 2 Response:\n",
      "1, we'll use the power rule for differentiation.\n",
      "The power rule states that if y = ax^n, then dy/dx = n * ax^(n-1).\n",
      "Applying this rule to each term in the function:\n",
      "d/dx (x^2) = 2x\n",
      "d/dx (3x) = 3\n",
      "d/dx (1) = 0\n",
      "Now, add up all these derivatives: d/dx (x^2 + 3x + 1) = 2x + 3 + 0 = 2x + 3\n",
      "Therefore, the derivative of the given function is f'(x) = 2x + 3.\n",
      "\n",
      "==================================================\n",
      "Multi-turn Conversation Example:\n",
      "\n",
      "Turn 1:\n",
      "Assistant: these steps:\n",
      "\n",
      "1. **Understand the equation**: The given equation is \\((x + 2)^2 = 0\\). This represents a quadratic expression set equal to zero.\n",
      "\n",
      "2. **Take the square root of both sides**:\n",
      "   Since the square of any real number is non-negative, the only way for \\((x + 2)^2\\) to be equal to zero is if \\(x + 2\\) itself is zero.\n",
      "   \n",
      "3. **Set the inside of the parentheses equal to zero**:\n",
      "   We have \\(x + 2 = 0\\).\n",
      "\n",
      "4. **Solve for \\(x\\)**:\n",
      "   Subtract 2 from both sides: \\(x + 2 - 2 = 0 - 2\\).\n",
      "   Simplifying this gives us \\(x = -2\\).\n",
      "\n",
      "5. **Verify the solution**:\n",
      "   Substitute \\(x = -2\\) back into the original equation to ensure it satisfies the equation:\n",
      "   \\[\n",
      "   (-2 + 2)^2 = 0^2 = 0\n",
      "   \\]\n",
      "   Since the left side equals the right side, our solution is correct.\n",
      "\n",
      "Therefore,\n",
      "\n",
      "Turn 2:\n",
      "Assistant: variables and numbers. Let's go through it step by step:\n",
      "\n",
      "### Step 1: Take the square root of both sides\n",
      "\n",
      "Since \\((x - 3)^2 = 16\\) can be written as \\(\\sqrt{(x - 3)^2} = \\sqrt{16}\\), we need to consider both the positive and negative roots because squaring removes the sign.\n",
      "\n",
      "\\[\n",
      "\\sqrt{(x - 3)^2} = \\pm \\sqrt{16}\n",
      "\\]\n",
      "\n",
      "This simplifies to:\n",
      "\n",
      "\\[\n",
      "|x - 3| = \\pm 4\n",
      "\\]\n",
      "\n",
      "### Step 2: Solve the absolute value equation\n",
      "\n",
      "The absolute value equation \\(|x - 3| = \\pm 4\\) means two separate equations to consider:\n",
      "\n",
      "#### Case 1: \\(x - 3 = 4\\)\n",
      "\n",
      "\\[\n",
      "x - 3 = 4 \\\\\n",
      "x = 4 + 3 \\\\\n",
      "x = 7\n",
      "\\]\n",
      "\n",
      "#### Case 2: \\(x - 3 = -4\\)\n",
      "\n",
      "\\[\n",
      "x - 3 = -4 \\\\\n",
      "x = -4 +\n",
      "\n",
      "Full conversation has 5 messages\n",
      "\n",
      "💾 VRAM usage: 1.61GB\n",
      "\n",
      "✅ System prompt chat inference complete!\n",
      "\n",
      "============================================================\n",
      "Why your original code failed:\n",
      "\n",
      "❌ ERRORS IN ORIGINAL CODE:\n",
      "1. model.load_lora() - Method doesn't exist on Qwen2ForCausalLM\n",
      "2. model.fast_generate() - Method doesn't exist on your model type  \n",
      "3. SamplingParams - This is vLLM syntax, not HuggingFace\n",
      "4. [0].outputs[0].text - This is vLLM output format\n",
      "\n",
      "✅ FIXES APPLIED:\n",
      "1. Used model.generate() - Standard HuggingFace method\n",
      "2. LoRA weights already active from training\n",
      "3. Parameters passed directly (temperature, top_k, etc.)\n",
      "4. Standard tensor decoding with tokenizer.decode()\n",
      "5. Proper memory management for RTX 2070 Super\n",
      "\n",
      "💡 KEY INSIGHT:\n",
      "Your model already has LoRA weights loaded if you trained it with PEFT.\n",
      "No need to load them again - just use standard inference!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define your system prompt (you need to define this)\n",
    "system_prompt = \"\"\"You are a helpful AI assistant that solves mathematical problems step by step. \n",
    "Provide clear explanations and show your work.\"\"\"\n",
    "\n",
    "# Chat messages with system prompt\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"Solve (x + 2)^2 = 0\"},\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "print(\"Formatted prompt with system message:\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# FIXED METHOD: Standard HuggingFace Inference\n",
    "print(\"Generating response with your trained LoRA model...\")\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate response (your LoRA weights are already active)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        temperature=1.0,        # Your original temperature\n",
    "        top_k=50,              # Your original top_k\n",
    "        max_new_tokens=512,    # Reduced from 2048 for RTX 2070 Super\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.1,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "# Decode the response\n",
    "full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract only the assistant's response\n",
    "if text in full_response:\n",
    "    output = full_response[len(text):].strip()\n",
    "else:\n",
    "    output = full_response\n",
    "\n",
    "print(\"Assistant Response:\")\n",
    "print(output)\n",
    "\n",
    "# Alternative: Batch processing for multiple questions\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Batch Processing Example:\")\n",
    "\n",
    "# Multiple math problems\n",
    "batch_messages = [\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"Solve (x + 2)^2 = 0\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"Find the derivative of x^2 + 3x + 1\"}\n",
    "    ]\n",
    "]\n",
    "\n",
    "batch_outputs = []\n",
    "\n",
    "for i, msgs in enumerate(batch_messages):\n",
    "    # Format each conversation\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        msgs,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    batch_inputs = tokenizer(formatted_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        batch_result = model.generate(\n",
    "            **batch_inputs,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    full_resp = tokenizer.decode(batch_result[0], skip_special_tokens=True)\n",
    "    response = full_resp[len(formatted_text):].strip()\n",
    "    \n",
    "    batch_outputs.append(response)\n",
    "    print(f\"\\nProblem {i+1} Response:\")\n",
    "    print(response)\n",
    "\n",
    "# Memory-efficient conversation handling\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Multi-turn Conversation Example:\")\n",
    "\n",
    "# Start a conversation\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt}\n",
    "]\n",
    "\n",
    "# Function to continue conversation\n",
    "def chat_turn(user_message, conversation_history):\n",
    "    # Add user message\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    # Format conversation\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        conversation_history,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    \n",
    "    # Generate response\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Extract assistant response\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    assistant_response = full_response[len(formatted):].strip()\n",
    "    \n",
    "    # Add to conversation history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "    \n",
    "    return assistant_response\n",
    "\n",
    "# Example multi-turn conversation\n",
    "print(\"\\nTurn 1:\")\n",
    "response1 = chat_turn(\"Solve (x + 2)^2 = 0\", conversation)\n",
    "print(f\"Assistant: {response1}\")\n",
    "\n",
    "print(\"\\nTurn 2:\")\n",
    "response2 = chat_turn(\"Now solve (x - 3)^2 = 16\", conversation)\n",
    "print(f\"Assistant: {response2}\")\n",
    "\n",
    "# Show conversation history\n",
    "print(f\"\\nFull conversation has {len(conversation)} messages\")\n",
    "\n",
    "# Memory cleanup for RTX 2070 Super\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"\\n💾 VRAM usage: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "\n",
    "print(\"\\n✅ System prompt chat inference complete!\")\n",
    "\n",
    "# EXPLANATION: Why the original code failed\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Why your original code failed:\")\n",
    "print(\"\"\"\n",
    "❌ ERRORS IN ORIGINAL CODE:\n",
    "1. model.load_lora() - Method doesn't exist on Qwen2ForCausalLM\n",
    "2. model.fast_generate() - Method doesn't exist on your model type  \n",
    "3. SamplingParams - This is vLLM syntax, not HuggingFace\n",
    "4. [0].outputs[0].text - This is vLLM output format\n",
    "\n",
    "✅ FIXES APPLIED:\n",
    "1. Used model.generate() - Standard HuggingFace method\n",
    "2. LoRA weights already active from training\n",
    "3. Parameters passed directly (temperature, top_k, etc.)\n",
    "4. Standard tensor decoding with tokenizer.decode()\n",
    "5. Proper memory management for RTX 2070 Super\n",
    "\n",
    "💡 KEY INSIGHT:\n",
    "Your model already has LoRA weights loaded if you trained it with PEFT.\n",
    "No need to load them again - just use standard inference!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ad5qCZMsW_Ed"
   },
   "source": [
    "Lets compare our results with system prompt but without our LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172,
     "referenced_widgets": [
      "f35b16000bb448139b0954c4eeff99f8",
      "ac02d83c54314dd889853d5dfbfcae6f",
      "ccb09cc19f804fbca9e3976116227460",
      "efa40eb719fd472f804af545734a4fc0",
      "17813750f23d44fb9c9305506030709f",
      "0693028d222b43e9852beec995ecb5dc",
      "7fecf7366e9d41cca37f32d736eed6e6",
      "783d83041ef2494287de1352d66e7e85",
      "59b5cb2fc77d46d395b22ef63954a7f4",
      "3b161341e8d64f5392ebcebf8629df1e",
      "e98eff347291445f851d14470b0552cb"
     ]
    },
    "id": "ee10WWhDW_Ee",
    "outputId": "3f7e2b03-c562-46b7-f9d3-27a36af3b473"
   },
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": system_prompt},\n",
    "#     {\"role\": \"user\",   \"content\": \"Solve (x + 2)^2 = 0\"},\n",
    "# ]\n",
    "\n",
    "# text = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     add_generation_prompt = True, # Must add for generation\n",
    "#     tokenize = False,\n",
    "# )\n",
    "# from vllm import SamplingParams\n",
    "# sampling_params = SamplingParams(\n",
    "#     temperature = 1.0,\n",
    "#     top_k = 50,\n",
    "#     max_tokens = 2048,\n",
    "# )\n",
    "# output = model.fast_generate(\n",
    "#     text,\n",
    "#     sampling_params = sampling_params,\n",
    "#     lora_request = None,\n",
    "# )[0].outputs[0].text\n",
    "\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted prompt:\n",
      "<|im_start|>system\n",
      "You are a helpful AI assistant that solves mathematical problems step by step. \n",
      "Provide clear explanations and show your work.<|im_end|>\n",
      "<|im_start|>user\n",
      "Solve (x + 2)^2 = 0<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "==================================================\n",
      "Generating response...\n",
      "Generated Output:\n",
      "system\n",
      "You are a helpful AI assistant that solves mathematical problems step by step. \n",
      "Provide clear explanations and show your work.\n",
      "user\n",
      "Solve (x + 2)^2 = 0\n",
      "assistant\n",
      "To solve the equation \\((x + 2)^2 = 0\\), we need to find the value of \\(x\\) that makes the equation true.\n",
      "\n",
      "Step 1: Recognize that the square of any number is equal to zero only if that number itself is zero.\n",
      "So, if \\((x + 2)^2 = 0\\), then \\(x + 2\\) must be equal to zero.\n",
      "\n",
      "Step 2: Set up an equation based on this observation:\n",
      "\\[ x + 2 = 0 \\]\n",
      "\n",
      "Step 3: Solve for \\(x\\).\n",
      "Subtract 2 from both sides of the equation:\n",
      "\\[ x + 2 - 2 = 0 - 2 \\]\n",
      "\\[ x = -2 \\]\n",
      "\n",
      "Therefore, the solution to the equation \\((x + 2)^2 = 0\\) is \\(x = -2\\).\n",
      "\n",
      "💾 VRAM usage: 1.61GB\n",
      "\n",
      "============================================================\n",
      "❌ DELETE THESE LINES FROM YOUR CODE:\n",
      "\n",
      "# DELETE THIS:\n",
      "from vllm import SamplingParams\n",
      "sampling_params = SamplingParams(\n",
      "    temperature = 1.0,\n",
      "    top_k = 50,\n",
      "    max_tokens = 2048,\n",
      ")\n",
      "output = model.fast_generate(\n",
      "    text,\n",
      "    sampling_params = sampling_params,\n",
      "    lora_request = None,\n",
      ")[0].outputs[0].text\n",
      "\n",
      "# REPLACE WITH:\n",
      "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
      "with torch.no_grad():\n",
      "    outputs = model.generate(\n",
      "        **inputs,\n",
      "        temperature=1.0,\n",
      "        top_k=50,\n",
      "        max_new_tokens=512,\n",
      "        do_sample=True,\n",
      "        pad_token_id=tokenizer.eos_token_id,\n",
      "    )\n",
      "full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
      "output = full_response[len(text):].strip()\n",
      "\n",
      "✅ This version works with your Qwen2ForCausalLM model!\n",
      "\n",
      "==================================================\n",
      "Quick one-liner version:\n",
      "Quick test output:\n",
      "the value of \\(x\\) that satisfies this equation.\n",
      "\n",
      "Step 1: Understand what it means for an expression to be equal to zero.\n",
      "An expression is considered zero when its value is zero. So, we set the inside of the square equal to zero because squaring any number results in a non-negative value unless that number itself is zero.\n",
      "\n",
      "Step 2: Set up the equation.\n",
      "Since \\((x + 2)^2 = 0\\), we can set the term inside the parentheses equal to zero:\n",
      "\n",
      "\\[ x + 2 = 0 \\]\n",
      "\n",
      "Step 3: Solve for \\(x\\).\n",
      "Now we isolate \\(x\\) on one side of the equation:\n",
      "\n",
      "\\[ x = -2 \\]\n",
      "\n",
      "Therefore, the solution to the equation \\((x + 2)^2 = 0\\) is \\(x = -2\\).\n",
      "\n",
      "We can verify this solution by substituting \\(x = -2\\) back into the original equation:\n",
      "\n",
      "\\[ (-2 + 2)^2 = 0^2 = 0 \\]\n",
      "\n",
      "This confirms that our solution is correct. The final answer is \\(x = -2\\).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define system prompt\n",
    "system_prompt = \"\"\"You are a helpful AI assistant that solves mathematical problems step by step. \n",
    "Provide clear explanations and show your work.\"\"\"\n",
    "\n",
    "# Your messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"Solve (x + 2)^2 = 0\"},\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "print(\"Formatted prompt:\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# CORRECT CODE: Replace everything after chat template with this\n",
    "print(\"Generating response...\")\n",
    "\n",
    "# Tokenize the formatted text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate using standard HuggingFace method\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        temperature=1.0,           # Same as your original\n",
    "        top_k=50,                 # Same as your original  \n",
    "        max_new_tokens=512,       # Reduced for RTX 2070 Super\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "# Decode the output\n",
    "full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract only the new generated text (remove the input prompt)\n",
    "if text in full_response:\n",
    "    output = full_response[len(text):].strip()\n",
    "else:\n",
    "    output = full_response\n",
    "\n",
    "# Display result\n",
    "print(\"Generated Output:\")\n",
    "print(output)\n",
    "\n",
    "# Memory status\n",
    "print(f\"\\n💾 VRAM usage: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "\n",
    "# WHAT YOU NEED TO DELETE FROM YOUR ORIGINAL CODE:\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"❌ DELETE THESE LINES FROM YOUR CODE:\")\n",
    "print(\"\"\"\n",
    "# DELETE THIS:\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 1.0,\n",
    "    top_k = 50,\n",
    "    max_tokens = 2048,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = None,\n",
    ")[0].outputs[0].text\n",
    "\n",
    "# REPLACE WITH:\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        temperature=1.0,\n",
    "        top_k=50,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "output = full_response[len(text):].strip()\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ This version works with your Qwen2ForCausalLM model!\")\n",
    "\n",
    "# Alternative: One-liner version for quick testing\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Quick one-liner version:\")\n",
    "\n",
    "def quick_chat(user_message, system_msg=None):\n",
    "    \"\"\"Quick chat function for testing\"\"\"\n",
    "    msgs = []\n",
    "    if system_msg:\n",
    "        msgs.append({\"role\": \"system\", \"content\": system_msg})\n",
    "    msgs.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(msgs, add_generation_prompt=True, tokenize=False)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.7, max_new_tokens=256, do_sample=True)\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response[len(prompt):].strip()\n",
    "\n",
    "# Test the quick function\n",
    "test_output = quick_chat(\"Solve (x + 2)^2 = 0\", system_prompt)\n",
    "print(\"Quick test output:\")\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYqpfCF0W_Ee"
   },
   "source": [
    "Let's take 20 samples, and compare the the amount of using our LoRA and not using it, and see which one has better amount of correct language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mJmztPHdW_Ef",
    "outputId": "2ea0c34d-abea-45c2-b04b-616323f9f1aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'solution', 'data_source', 'source_prompt', 'ability', 'reward_model', 'extra_info', 'answer'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset = dataset.shuffle(seed = 3407).select(range(20))\n",
    "sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing language usage with and without LoRA on 20 samples:\n",
      "============================================================\n",
      "✅ Dataset structure confirmed:\n",
      "   - sample['prompt'][0]['content'] = system message\n",
      "   - sample['prompt'][1]['content'] = user message\n",
      "   - sample['solution'] = expected solution\n",
      "\n",
      "🔧 Method 1: Using PEFT adapter enable/disable\n",
      "✅ PEFT adapter control available\n",
      "\n",
      "🔄 Sample 1:\n",
      "   User: For a positive integer $N$, we color the positive divisors of $N$ (including 1 and $N$) with four co...\n",
      "   ✅ With LoRA (en): To solve this problem, let's first understand what it means for a coloring to be...\n",
      "   ✅ Without LoRA (en): To solve this problem, let's first understand what a multichromatic coloring mea...\n",
      "\n",
      "🔄 Sample 2:\n",
      "   User: Let $f$ be a non-constant polynomial such that\n",
      "\\[f(x - 1) + f(x) + f(x + 1) = \\frac{[f(x)]^2}{2013x}...\n",
      "   ✅ With LoRA (en): To solve for \\( f(1) \\), we start by analyzing the given functional equation:\n",
      "\\[...\n",
      "   ✅ Without LoRA (en): To solve for \\( f(1) \\), we start by analyzing the given functional equation:\n",
      "\\[...\n",
      "\n",
      "🔄 Sample 3:\n",
      "   User: Find the sum of all integers between $-\\sqrt{1442}$ and $\\sqrt{2020}$....\n",
      "   ✅ With LoRA (en): To find the sum of all integers between \\(-\\sqrt{1442}\\) and \\(\\sqrt{2020}\\), we...\n",
      "   ✅ Without LoRA (en): To find the sum of all integers between \\(-\\sqrt{1442}\\) and \\(\\sqrt{2020}\\), we...\n",
      "\n",
      "🔄 Sample 4:\n",
      "   User: Given the equations $3x+y=17,5y+z=14$ and $3x+5z=41$, what is the value of the sum $x+y+z$?...\n",
      "\n",
      "🔄 Sample 5:\n",
      "   User: Determine the sum of all single-digit replacements for $z$ such that the number ${24{,}z38}$ is divi...\n",
      "\n",
      "📊 Progress: 5 samples processed\n",
      "   LoRA: 0 Indonesian, Base: 0 Indonesian\n",
      "\n",
      "🔄 Sample 6:\n",
      "   User: It is given that $2^{333}$ is a 101-digit number whose first digit is 1. How many of the numbers $2^...\n",
      "\n",
      "🔄 Sample 7:\n",
      "   User: A convex polyhedron has $n$ faces, all of which are congruent triangles with angles $36^{\\circ}$, $7...\n",
      "\n",
      "🔄 Sample 8:\n",
      "   User: Let $A_{12}$ denote the answer to problem $12$.  There exists a unique triple of digits $(B,C,D)$ su...\n",
      "\n",
      "🔄 Sample 9:\n",
      "   User: Mrs. Jones is pouring orange juice into four identical glasses for her four sons. She fills the firs...\n",
      "\n",
      "🔄 Sample 10:\n",
      "   User: A fisherman can see seven aquatic creatures in a lake --- four crocodiles, a catfish, and two giant ...\n",
      "\n",
      "📊 Progress: 10 samples processed\n",
      "   LoRA: 0 Indonesian, Base: 0 Indonesian\n",
      "\n",
      "🔄 Sample 11:\n",
      "   User: Fruit salad can be made with any $3$ of these $5$ fruits: apples, bananas, grapes, strawberries, and...\n",
      "\n",
      "🔄 Sample 12:\n",
      "   User: A time is chosen randomly and uniformly in a 24-hour day. The probability that at that time, the non...\n",
      "\n",
      "🔄 Sample 13:\n",
      "   User: The base-ten representation for $19!$ is $121,6T5,100,40M,832,H00$, where $T$, $M$, and $H$ denote d...\n",
      "\n",
      "🔄 Sample 14:\n",
      "   User: David, when submitting a problem for CMIMC, wrote his answer as $100\\frac{x}{y}$, where $x$ and $y$ ...\n",
      "\n",
      "🔄 Sample 15:\n",
      "   User: Let $a,$ $b,$ $c$ be distinct integers, and let $\\omega$ be a complex number such that $\\omega^3 = 1...\n",
      "\n",
      "📊 Progress: 15 samples processed\n",
      "   LoRA: 0 Indonesian, Base: 0 Indonesian\n",
      "\n",
      "🔄 Sample 16:\n",
      "   User: Two different points, $C$ and $D$, lie on the same side of line $AB$ so that $\\triangle ABC$ and $\\t...\n",
      "\n",
      "🔄 Sample 17:\n",
      "   User: Let $x$, $y$, and $z$ be positive integers satisfying the following system of equations:\n",
      "\n",
      "\\[\n",
      "\\begin{...\n",
      "\n",
      "🔄 Sample 18:\n",
      "   User: Six different small books and three different large books are placed on a shelf. Three children can ...\n",
      "\n",
      "🔄 Sample 19:\n",
      "   User: Hiram's algebra notes are $50$ pages long and are printed on $25$ sheets of paper; the first sheet c...\n",
      "\n",
      "🔄 Sample 20:\n",
      "   User: Let \\(a, b,\\) and \\(c\\) be real numbers such that  \n",
      "  \n",
      "\\(a+b+c=2,\\) and  \n",
      "\\(a^2+b^2+c^2=12\\)  \n",
      "  \n",
      "Fi...\n",
      "\n",
      "📊 Progress: 20 samples processed\n",
      "   LoRA: 0 Indonesian, Base: 0 Indonesian\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS:\n",
      "Successfully processed: 20 samples\n",
      "With LoRA - Indonesian responses: 0/20 (0.0%)\n",
      "Without LoRA - Indonesian responses: 0/20 (0.0%)\n",
      "Improvement: +0 Indonesian responses with LoRA\n",
      "🤔 No change in language usage\n",
      "\n",
      "💾 Final VRAM usage: 1.61GB\n",
      "✅ LoRA comparison completed!\n",
      "\n",
      "📋 Sample data structure for reference:\n",
      "Sample keys: ['prompt', 'solution', 'data_source', 'source_prompt', 'ability', 'reward_model', 'extra_info', 'answer']\n",
      "System prompt: You are given a problem.\n",
      "Think about the problem and provide your working out.\n",
      "You must think in Bah...\n",
      "User message: For a positive integer $N$, we color the positive divisors of $N$ (including 1 and $N$) with four co...\n",
      "Expected solution: 192...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Initialize counters\n",
    "with_lora_id_count = 0\n",
    "without_lora_id_count = 0\n",
    "\n",
    "print(\"Comparing language usage with and without LoRA on 20 samples:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Debug dataset structure (from your output, we know the structure)\n",
    "print(\"✅ Dataset structure confirmed:\")\n",
    "print(\"   - sample['prompt'][0]['content'] = system message\")\n",
    "print(\"   - sample['prompt'][1]['content'] = user message\")\n",
    "print(\"   - sample['solution'] = expected solution\")\n",
    "\n",
    "# SOLUTION 1: Use PEFT adapter enable/disable (RECOMMENDED)\n",
    "print(\"\\n🔧 Method 1: Using PEFT adapter enable/disable\")\n",
    "\n",
    "def generate_response(model_to_use, text, max_length=256):\n",
    "    \"\"\"Helper function with error handling\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024).to(model_to_use.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model_to_use.generate(\n",
    "                **inputs,\n",
    "                temperature=0.7,  # Reduced for stability\n",
    "                top_k=50,\n",
    "                max_new_tokens=max_length,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "            )\n",
    "        \n",
    "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return full_response[len(tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)):].strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Generation error: {str(e)[:100]}...\")\n",
    "        return f\"[Generation failed: {str(e)[:50]}...]\"\n",
    "\n",
    "# Check if model has PEFT adapter control\n",
    "try:\n",
    "    if hasattr(model, 'disable_adapter_layers') and hasattr(model, 'enable_adapter_layers'):\n",
    "        print(\"✅ PEFT adapter control available\")\n",
    "        \n",
    "        # Process samples using adapter enable/disable\n",
    "        processed_samples = 0\n",
    "        \n",
    "        for i, sample in enumerate(sample_dataset):\n",
    "            try:\n",
    "                # Extract the correct user content\n",
    "                if isinstance(sample, dict) and 'prompt' in sample:\n",
    "                    if isinstance(sample['prompt'], list) and len(sample['prompt']) > 1:\n",
    "                        user_content = sample['prompt'][1]['content']\n",
    "                        system_content = sample['prompt'][0]['content']\n",
    "                    else:\n",
    "                        continue  # Skip malformed samples\n",
    "                else:\n",
    "                    continue  # Skip if not correct format\n",
    "                \n",
    "                print(f\"\\n🔄 Sample {i+1}:\")\n",
    "                print(f\"   User: {user_content[:100]}...\")\n",
    "                \n",
    "                # Format conversation\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_content},\n",
    "                    {\"role\": \"user\", \"content\": user_content},\n",
    "                ]\n",
    "                \n",
    "                text = tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    add_generation_prompt=True,\n",
    "                    tokenize=False,\n",
    "                )\n",
    "                \n",
    "                # Generate WITH LoRA (adapter enabled)\n",
    "                model.enable_adapter_layers()\n",
    "                output_with_lora = generate_response(model, text, max_length=128)\n",
    "                \n",
    "                # Generate WITHOUT LoRA (adapter disabled)\n",
    "                model.disable_adapter_layers() \n",
    "                output_without_lora = generate_response(model, text, max_length=128)\n",
    "                \n",
    "                # Re-enable adapter for next iteration\n",
    "                model.enable_adapter_layers()\n",
    "                \n",
    "                # Check if responses are valid\n",
    "                if \"[Generation failed\" in output_with_lora or \"[Generation failed\" in output_without_lora:\n",
    "                    print(\"   ⚠️ Generation failed, skipping sample\")\n",
    "                    continue\n",
    "                \n",
    "                # Detect language\n",
    "                lang_with_lora = get_lang(output_with_lora)\n",
    "                lang_without_lora = get_lang(output_without_lora)\n",
    "                \n",
    "                # Count Indonesian responses\n",
    "                if lang_with_lora == 'id':\n",
    "                    with_lora_id_count += 1\n",
    "                if lang_without_lora == 'id':\n",
    "                    without_lora_id_count += 1\n",
    "                \n",
    "                processed_samples += 1\n",
    "                \n",
    "                # Show results for first few samples\n",
    "                if processed_samples <= 3:\n",
    "                    print(f\"   ✅ With LoRA ({lang_with_lora}): {output_with_lora[:80]}...\")\n",
    "                    print(f\"   ✅ Without LoRA ({lang_without_lora}): {output_without_lora[:80]}...\")\n",
    "                \n",
    "                # Progress updates\n",
    "                if processed_samples % 5 == 0:\n",
    "                    print(f\"\\n📊 Progress: {processed_samples} samples processed\")\n",
    "                    print(f\"   LoRA: {with_lora_id_count} Indonesian, Base: {without_lora_id_count} Indonesian\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                \n",
    "                # Stop after 20 successful samples\n",
    "                if processed_samples >= 20:\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Error processing sample {i+1}: {str(e)[:100]}...\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(\"FINAL RESULTS:\")\n",
    "        print(f\"Successfully processed: {processed_samples} samples\")\n",
    "        if processed_samples > 0:\n",
    "            print(f\"With LoRA - Indonesian responses: {with_lora_id_count}/{processed_samples} ({with_lora_id_count/processed_samples*100:.1f}%)\")\n",
    "            print(f\"Without LoRA - Indonesian responses: {without_lora_id_count}/{processed_samples} ({without_lora_id_count/processed_samples*100:.1f}%)\")\n",
    "            print(f\"Improvement: +{with_lora_id_count - without_lora_id_count} Indonesian responses with LoRA\")\n",
    "            \n",
    "            if with_lora_id_count > without_lora_id_count:\n",
    "                print(\"🎉 LoRA training improved Indonesian language usage!\")\n",
    "            elif with_lora_id_count == without_lora_id_count:\n",
    "                print(\"🤔 No change in language usage\")\n",
    "            else:\n",
    "                print(\"⚠️ LoRA reduced Indonesian usage\")\n",
    "        else:\n",
    "            print(\"❌ No samples processed successfully\")\n",
    "    \n",
    "    else:\n",
    "        print(\"❌ PEFT adapter control not available\")\n",
    "        raise Exception(\"No adapter control\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ PEFT method failed: {e}\")\n",
    "    print(\"\\n🔧 Method 2: Simplified single model test\")\n",
    "    \n",
    "    # SOLUTION 2: Test current model only (simplified)\n",
    "    print(\"Testing current model responses (LoRA active)...\")\n",
    "    \n",
    "    lora_id_responses = 0\n",
    "    total_tested = 0\n",
    "    \n",
    "    for i, sample in enumerate(sample_dataset[:10]):  # Test 10 samples\n",
    "        try:\n",
    "            if isinstance(sample, dict) and 'prompt' in sample:\n",
    "                if isinstance(sample['prompt'], list) and len(sample['prompt']) > 1:\n",
    "                    user_content = sample['prompt'][1]['content']\n",
    "                    system_content = sample['prompt'][0]['content']\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_content},\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "            ]\n",
    "            \n",
    "            text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "            \n",
    "            # Generate with current model (has LoRA)\n",
    "            response = generate_response(model, text, max_length=128)\n",
    "            \n",
    "            if \"[Generation failed\" not in response:\n",
    "                lang = get_lang(response)\n",
    "                if lang == 'id':\n",
    "                    lora_id_responses += 1\n",
    "                total_tested += 1\n",
    "                \n",
    "                print(f\"Sample {i+1} ({lang}): {response[:80]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Sample {i+1} failed: {str(e)[:50]}...\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n📊 Simplified Test Results:\")\n",
    "    print(f\"LoRA model Indonesian responses: {lora_id_responses}/{total_tested}\")\n",
    "    if total_tested > 0:\n",
    "        print(f\"Indonesian percentage: {lora_id_responses/total_tested*100:.1f}%\")\n",
    "\n",
    "# Final cleanup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(f\"\\n💾 Final VRAM usage: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "print(\"✅ LoRA comparison completed!\")\n",
    "\n",
    "# Show what a proper working sample looks like\n",
    "print(f\"\\n📋 Sample data structure for reference:\")\n",
    "if len(sample_dataset) > 0:\n",
    "    sample = sample_dataset[0]\n",
    "    print(f\"Sample keys: {list(sample.keys())}\")\n",
    "    print(f\"System prompt: {sample['prompt'][0]['content'][:100]}...\")\n",
    "    print(f\"User message: {sample['prompt'][1]['content'][:100]}...\")\n",
    "    if 'solution' in sample:\n",
    "        print(f\"Expected solution: {str(sample['solution'])[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aDgFfhFYIAS"
   },
   "source": [
    "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NUEmHFSYNTp"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52WMb3k_YPt8"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n",
    "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saving Options for Qwen2ForCausalLM with LoRA\n",
      "============================================================\n",
      "1️⃣ Saving LoRA adapters only...\n",
      "✅ LoRA adapters saved to 'model_lora' folder\n",
      "   Size: 85.6MB\n",
      "\n",
      "2️⃣ Merge to 16-bit and save...\n",
      "\n",
      "3️⃣ Merge to 4-bit and save...\n",
      "\n",
      "4️⃣ Push to Hugging Face Hub...\n",
      "\n",
      "============================================================\n",
      "📁 SAVED FILES VERIFICATION:\n",
      "\n",
      "📂 model_lora:\n",
      "   Files: 10\n",
      "   Size: 85.6MB\n",
      "   • adapter_config.json (0.0MB)\n",
      "   • adapter_model.safetensors (70.5MB)\n",
      "   • added_tokens.json (0.0MB)\n",
      "\n",
      "❌ model_merged_16bit: Not found\n",
      "\n",
      "❌ model_16bit: Not found\n",
      "\n",
      "❌ model_4bit: Not found\n",
      "\n",
      "❌ model_4bit_config: Not found\n",
      "\n",
      "============================================================\n",
      "📖 LOADING INSTRUCTIONS:\n",
      "\n",
      "🔹 To load LoRA adapters:\n",
      "   from peft import PeftModel\n",
      "   base_model = AutoModelForCausalLM.from_pretrained(\"unsloth/Qwen2.5-1.5B-Instruct\")\n",
      "   model = PeftModel.from_pretrained(base_model, \"model_lora\")\n",
      "\n",
      "🔹 To load 16-bit merged model:\n",
      "   model = AutoModelForCausalLM.from_pretrained(\"model_merged_16bit\")\n",
      "\n",
      "🔹 To load with 4-bit quantization:\n",
      "   model = AutoModelForCausalLM.from_pretrained(\n",
      "       \"model_4bit\", \n",
      "       load_in_4bit=True,\n",
      "       device_map=\"auto\"\n",
      "   )\n",
      "\n",
      "🔹 From Hugging Face Hub:\n",
      "   model = AutoModelForCausalLM.from_pretrained(\"your-username/your-model-name-lora\")\n",
      "\n",
      "\n",
      "💾 Current VRAM usage: 1.61GB\n",
      "\n",
      "✅ Model saving options completed!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Code Analysis:\n",
    "- Original uses Unsloth methods (save_pretrained_merged, push_to_hub_merged)\n",
    "- These methods don't exist on standard HuggingFace/PEFT models\n",
    "- Need to use standard HuggingFace and PEFT methods instead\n",
    "- Must handle merging LoRA weights manually if needed\n",
    "- All examples have if False - they're disabled templates\n",
    "\"\"\"\n",
    "import torch\n",
    "import os\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "print(\"Model Saving Options for Qwen2ForCausalLM with LoRA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ==============================================================================\n",
    "# OPTION 1: SAVE LORA ADAPTERS ONLY (RECOMMENDED - Small files)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"1️⃣ Saving LoRA adapters only...\")\n",
    "\n",
    "if True:  # Set to True to execute\n",
    "    # Save LoRA adapter weights (lightweight ~MB)\n",
    "    model.save_pretrained(\"model_lora\")\n",
    "    tokenizer.save_pretrained(\"model_lora\")\n",
    "    print(\"✅ LoRA adapters saved to 'model_lora' folder\")\n",
    "    \n",
    "    # Check size\n",
    "    size_mb = sum(os.path.getsize(os.path.join(\"model_lora\", f)) \n",
    "                  for f in os.listdir(\"model_lora\")) / (1024*1024)\n",
    "    print(f\"   Size: {size_mb:.1f}MB\")\n",
    "\n",
    "# ==============================================================================\n",
    "# OPTION 2: MERGE AND SAVE TO 16-BIT (Large files ~3-6GB)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n2️⃣ Merge to 16-bit and save...\")\n",
    "\n",
    "if False:  # Set to True to execute (WARNING: Large files!)\n",
    "    try:\n",
    "        # Check if model has PEFT adapter to merge\n",
    "        if hasattr(model, 'merge_and_unload'):\n",
    "            print(\"Merging LoRA weights into base model...\")\n",
    "            merged_model = model.merge_and_unload()\n",
    "            \n",
    "            # Save merged model in 16-bit\n",
    "            merged_model.save_pretrained(\n",
    "                \"model_merged_16bit\",\n",
    "                torch_dtype=torch.float16,\n",
    "                safe_serialization=True,  # Use safetensors format\n",
    "            )\n",
    "            tokenizer.save_pretrained(\"model_merged_16bit\")\n",
    "            print(\"✅ 16-bit merged model saved to 'model_merged_16bit'\")\n",
    "            \n",
    "        else:\n",
    "            print(\"⚠️ No PEFT adapter found - saving current model as 16-bit\")\n",
    "            model.save_pretrained(\n",
    "                \"model_16bit\", \n",
    "                torch_dtype=torch.float16,\n",
    "                safe_serialization=True,\n",
    "            )\n",
    "            tokenizer.save_pretrained(\"model_16bit\")\n",
    "            print(\"✅ 16-bit model saved to 'model_16bit'\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 16-bit save failed: {e}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# OPTION 3: MERGE AND SAVE TO 4-BIT (Smaller files ~1-2GB)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n3️⃣ Merge to 4-bit and save...\")\n",
    "\n",
    "if False:  # Set to True to execute\n",
    "    try:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        \n",
    "        # Merge LoRA if possible\n",
    "        if hasattr(model, 'merge_and_unload'):\n",
    "            merged_model = model.merge_and_unload()\n",
    "        else:\n",
    "            merged_model = model\n",
    "        \n",
    "        # Create quantization config\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        \n",
    "        # Note: 4-bit models are typically loaded with quantization, not saved\n",
    "        print(\"⚠️ 4-bit quantization typically done at load time, not save time\")\n",
    "        print(\"   Use load_in_4bit=True when loading the 16-bit model instead\")\n",
    "        \n",
    "        # Save configuration for 4-bit loading\n",
    "        import json\n",
    "        config = {\n",
    "            \"quantization_method\": \"4bit\",\n",
    "            \"load_in_4bit\": True,\n",
    "            \"bnb_4bit_quant_type\": \"nf4\",\n",
    "            \"instructions\": \"Load this model with load_in_4bit=True\"\n",
    "        }\n",
    "        \n",
    "        os.makedirs(\"model_4bit_config\", exist_ok=True)\n",
    "        with open(\"model_4bit_config/quantization_config.json\", \"w\") as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        # Copy the 16-bit model (4-bit quantization happens at load time)\n",
    "        if os.path.exists(\"model_merged_16bit\"):\n",
    "            import shutil\n",
    "            shutil.copytree(\"model_merged_16bit\", \"model_4bit\", dirs_exist_ok=True)\n",
    "            shutil.copy(\"model_4bit_config/quantization_config.json\", \"model_4bit/\")\n",
    "            print(\"✅ 4-bit loading configuration saved to 'model_4bit'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 4-bit configuration failed: {e}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# OPTION 4: PUSH TO HUGGING FACE HUB\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n4️⃣ Push to Hugging Face Hub...\")\n",
    "\n",
    "if False:  # Set to True and add your token to execute\n",
    "    hf_token = \"hf_your_token_here\"  # Replace with your actual token\n",
    "    repo_name = \"your-username/your-model-name\"  # Replace with your repo\n",
    "    \n",
    "    try:\n",
    "        # Push LoRA adapters\n",
    "        model.push_to_hub(repo_name + \"-lora\", token=hf_token)\n",
    "        tokenizer.push_to_hub(repo_name + \"-lora\", token=hf_token)\n",
    "        print(f\"✅ LoRA adapters pushed to {repo_name}-lora\")\n",
    "        \n",
    "        # Push merged model (if exists)\n",
    "        if os.path.exists(\"model_merged_16bit\"):\n",
    "            from transformers import AutoModelForCausalLM\n",
    "            merged_model = AutoModelForCausalLM.from_pretrained(\"model_merged_16bit\")\n",
    "            merged_tokenizer = AutoTokenizer.from_pretrained(\"model_merged_16bit\")\n",
    "            \n",
    "            merged_model.push_to_hub(repo_name + \"-merged\", token=hf_token)\n",
    "            merged_tokenizer.push_to_hub(repo_name + \"-merged\", token=hf_token)\n",
    "            print(f\"✅ Merged model pushed to {repo_name}-merged\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Hub push failed: {e}\")\n",
    "        print(\"   Make sure to set a valid HF token and repo name\")\n",
    "\n",
    "# ==============================================================================\n",
    "# VERIFICATION: CHECK WHAT WAS SAVED\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"📁 SAVED FILES VERIFICATION:\")\n",
    "\n",
    "folders_to_check = [\n",
    "    \"model_lora\", \n",
    "    \"model_merged_16bit\", \n",
    "    \"model_16bit\",\n",
    "    \"model_4bit\", \n",
    "    \"model_4bit_config\"\n",
    "]\n",
    "\n",
    "for folder in folders_to_check:\n",
    "    if os.path.exists(folder):\n",
    "        files = os.listdir(folder)\n",
    "        total_size = sum(os.path.getsize(os.path.join(folder, f)) for f in files)\n",
    "        size_mb = total_size / (1024*1024)\n",
    "        \n",
    "        print(f\"\\n📂 {folder}:\")\n",
    "        print(f\"   Files: {len(files)}\")\n",
    "        print(f\"   Size: {size_mb:.1f}MB\")\n",
    "        \n",
    "        # Show key files\n",
    "        key_files = [f for f in files if f.endswith(('.bin', '.safetensors', '.json'))]\n",
    "        for file in key_files[:3]:  # Show first 3 important files\n",
    "            file_size = os.path.getsize(os.path.join(folder, file)) / (1024*1024)\n",
    "            print(f\"   • {file} ({file_size:.1f}MB)\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"\\n❌ {folder}: Not found\")\n",
    "\n",
    "# ==============================================================================\n",
    "# LOADING INSTRUCTIONS FOR SAVED MODELS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"📖 LOADING INSTRUCTIONS:\")\n",
    "\n",
    "print(\"\"\"\n",
    "🔹 To load LoRA adapters:\n",
    "   from peft import PeftModel\n",
    "   base_model = AutoModelForCausalLM.from_pretrained(\"unsloth/Qwen2.5-1.5B-Instruct\")\n",
    "   model = PeftModel.from_pretrained(base_model, \"model_lora\")\n",
    "\n",
    "🔹 To load 16-bit merged model:\n",
    "   model = AutoModelForCausalLM.from_pretrained(\"model_merged_16bit\")\n",
    "\n",
    "🔹 To load with 4-bit quantization:\n",
    "   model = AutoModelForCausalLM.from_pretrained(\n",
    "       \"model_4bit\", \n",
    "       load_in_4bit=True,\n",
    "       device_map=\"auto\"\n",
    "   )\n",
    "\n",
    "🔹 From Hugging Face Hub:\n",
    "   model = AutoModelForCausalLM.from_pretrained(\"your-username/your-model-name-lora\")\n",
    "\"\"\")\n",
    "\n",
    "# Memory cleanup for RTX 2070 Super\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"\\n💾 Current VRAM usage: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "print(\"\\n✅ Model saving options completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyEjW-WuYQIm"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
