{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO: Group Relative Policy Optimization\n",
    "\n",
    "GRPO is a reinforcement learning technique for training large language models (LLMs) on complex tasks, such as mathematical reasoning or code generation.\n",
    "\n",
    "Unlike older methods like PPO, GRPO enhances efficiency by:\n",
    "*   **Eliminating a separate value model:** It does not require a dedicated neural network to estimate rewards, which significantly reduces memory and computational costs.\n",
    "*   **Using group-based advantages:** The model generates multiple candidate answers for a single prompt. The average score of these answers is used as a baseline to assess the relative quality of each response.\n",
    "*   **Focusing on reasoning:** By learning from a group's collective outcomes, GRPO encourages the model to develop more robust reasoning abilities, rather than just optimizing for a single, potentially noisy, reward signal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1UUw7rKxojj"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1eIfWD9xojj"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    # If you're not in Colab, just use pip install or uv pip install\n",
    "    !pip install unsloth vllm\n",
    "else:\n",
    "    pass # For Colab / Kaggle, we need extra instructions hidden below \\/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbbUKd2txojk"
   },
   "outputs": [],
   "source": [
    "#@title Colab Extra Install { display-mode: \"form\" }\n",
    "#%%capture\n",
    "import os\n",
    "!pip install --upgrade -qqq uv\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    # If you're not in Colab, just use pip install!\n",
    "    !pip install unsloth vllm\n",
    "else:\n",
    "    try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
    "    except: get_numpy = \"numpy\"\n",
    "    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
    "    except: is_t4 = False\n",
    "    get_vllm, get_triton = (\"vllm==0.10.1\", \"triton==3.2.0\") if is_t4 else (\"vllm\", \"triton\")\n",
    "    !uv pip install -qqq --upgrade \\\n",
    "        unsloth {get_vllm} {get_numpy} torchvision bitsandbytes xformers\n",
    "    !uv pip install -qqq {get_triton}\n",
    "!uv pip install transformers==4.55.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkH_y8UC9lvv"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jN75nmdx9lvw"
   },
   "source": [
    "Goal: To convert `Qwen3-4B-Base` into a reasoning model via GRPO by using OpenR1's Math dataset.\n",
    "\n",
    "We first pre fine-tune the model to make GRPO skip trying to match formatting - this speeds GRPO up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "# Fixed code - removed comma after max_seq_length value\n",
    "# Fixed code - removed comma after max_seq_length value\n",
    "max_seq_length = 512  # 2048 Can increase for longer reasoning traces\n",
    "lora_rank = 4  # Larger rank = smarter, but slower\n",
    "\n",
    "# Load model with optimized settings for RTX 2070 (8GB VRAM)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-4B-Base\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,  # Changed to True for memory efficiency on RTX 2070\n",
    "    fast_inference = False,  # Disable vLLM due to triton compatibility issues\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.5,  # Reduced for RTX 2070 - adjust based on your needs\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,  # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9DuiVRLhMco"
   },
   "source": [
    "### GRPO chat template\n",
    "Since we're using a base model, we should set a chat template. You can make your own chat template as well!\n",
    "1. DeepSeek uses `<think>` and `</think>`, but this is **not** necessary - you can customize it however you like!\n",
    "2. A `system_prompt` is recommended to at least guide the model's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_start = \"<start_working_out>\" # Acts as <think>\n",
    "reasoning_end   = \"<end_working_out>\"   # Acts as </think>\n",
    "solution_start  = \"<SOLUTION>\"\n",
    "solution_end    = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = \\\n",
    "f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your working out.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = \\\n",
    "    \"{% if messages[0]['role'] == 'system' %}\"\\\n",
    "        \"{{ messages[0]['content'] + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages[1:] %}\"\\\n",
    "    \"{% else %}\"\\\n",
    "        \"{{ '{system_prompt}' + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages %}\"\\\n",
    "    \"{% endif %}\"\\\n",
    "    \"{% for message in loop_messages %}\"\\\n",
    "        \"{% if message['role'] == 'user' %}\"\\\n",
    "            \"{{ message['content'] }}\"\\\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
    "            \"{{ message['content'] + eos_token }}\"\\\n",
    "        \"{% endif %}\"\\\n",
    "    \"{% endfor %}\"\\\n",
    "    \"{% if add_generation_prompt %}{{ '{reasoning_start}' }}\"\\\n",
    "    \"{% endif %}\"\n",
    "\n",
    "# Replace with out specific template:\n",
    "chat_template = chat_template\\\n",
    "    .replace(\"'{system_prompt}'\",   f\"'{system_prompt}'\")\\\n",
    "    .replace(\"'{reasoning_start}'\", f\"'{reasoning_start}'\")\n",
    "tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_dataset(\"unsloth/OpenMathReasoning-mini\", split = \"cot\")\n",
    "dataset = dataset.to_pandas()[\n",
    "    [\"expected_answer\", \"problem\", \"generated_solution\"]\n",
    "]\n",
    "\n",
    "# Try converting to number - if not, replace with NaN\n",
    "is_number = pd.to_numeric(pd.Series(dataset[\"expected_answer\"]), errors = \"coerce\").notnull()\n",
    "# Select only numbers\n",
    "dataset = dataset.iloc[np.where(is_number)[0]]\n",
    "\n",
    "print(f\"Dataset size after numeric filtering: {len(dataset)}\")\n",
    "\n",
    "def format_dataset(x):\n",
    "    expected_answer = x[\"expected_answer\"]\n",
    "    problem = x[\"problem\"]\n",
    "\n",
    "    # Remove generated <think> and </think>\n",
    "    thoughts = x[\"generated_solution\"]\n",
    "    thoughts = thoughts.replace(\"<think>\", \"\").replace(\"</think>\", \"\")\n",
    "\n",
    "    # Strip newlines on left and right\n",
    "    thoughts = thoughts.strip()\n",
    "    # Add our custom formatting\n",
    "    final_prompt = \\\n",
    "        reasoning_start + thoughts + reasoning_end + \\\n",
    "        solution_start + expected_answer + solution_end\n",
    "    return [\n",
    "        {\"role\" : \"system\",    \"content\" : system_prompt},\n",
    "        {\"role\" : \"user\",      \"content\" : problem},\n",
    "        {\"role\" : \"assistant\", \"content\" : final_prompt},\n",
    "    ]\n",
    "\n",
    "dataset[\"Messages\"] = dataset.apply(format_dataset, axis = 1)\n",
    "\n",
    "# Check a sample\n",
    "print(\"Sample formatted message:\")\n",
    "print(tokenizer.apply_chat_template(dataset[\"Messages\"].iloc[0], tokenize = False))\n",
    "\n",
    "# Calculate token lengths\n",
    "dataset[\"N\"] = dataset[\"Messages\"].apply(lambda x: len(tokenizer.apply_chat_template(x, tokenize=True)))\n",
    "\n",
    "print(f\"Token length stats:\")\n",
    "print(f\"Min: {dataset['N'].min()}\")\n",
    "print(f\"Max: {dataset['N'].max()}\")\n",
    "print(f\"Mean: {dataset['N'].mean():.1f}\")\n",
    "print(f\"Median: {dataset['N'].median():.1f}\")\n",
    "\n",
    "# Use a more reasonable filter - keep sequences up to max_seq_length\n",
    "# Instead of max_seq_length/2, use the full length\n",
    "dataset_filtered = dataset.loc[dataset[\"N\"] <= max_seq_length].copy()\n",
    "print(f\"Dataset size after length filtering (<= {max_seq_length}): {len(dataset_filtered)}\")\n",
    "\n",
    "# If still too restrictive, increase the limit\n",
    "if len(dataset_filtered) < 100:  # If we have less than 100 samples\n",
    "    print(f\"Too few samples with max_seq_length={max_seq_length}, trying with 256...\")\n",
    "    dataset_filtered = dataset.loc[dataset[\"N\"] <= 256].copy()\n",
    "    print(f\"Dataset size with 256 token limit: {len(dataset_filtered)}\")\n",
    "    \n",
    "    if len(dataset_filtered) < 100:\n",
    "        print(\"Using top 500 shortest sequences...\")\n",
    "        dataset_filtered = dataset.nsmallest(500, 'N').copy()\n",
    "        print(f\"Dataset size (top 500 shortest): {len(dataset_filtered)}\")\n",
    "\n",
    "# Check if we have data\n",
    "if len(dataset_filtered) == 0:\n",
    "    print(\"ERROR: No data after filtering! Using original dataset...\")\n",
    "    dataset_filtered = dataset.head(100).copy()  # Use first 100 samples\n",
    "    \n",
    "print(f\"Final dataset shape: {dataset_filtered.shape}\")\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# Only proceed if we have data\n",
    "if len(dataset_filtered) > 0:\n",
    "    dataset_filtered[\"text\"] = [\n",
    "        tokenizer.apply_chat_template(msg, tokenize=False) \n",
    "        for msg in dataset_filtered[\"Messages\"].values.tolist()\n",
    "    ]\n",
    "    \n",
    "    final_dataset = Dataset.from_pandas(dataset_filtered)\n",
    "    print(f\"Final Hugging Face dataset: {final_dataset}\")\n",
    "    print(f\"Sample text length: {len(final_dataset[0]['text'])}\")\n",
    "else:\n",
    "    print(\"ERROR: No data to process!\")\n",
    "    final_dataset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGgs0MJkDkYL"
   },
   "source": [
    "We create a simple chat template below. Notice `add_generation_prompt` includes prepending `<start_working_out>` to guide the model to start its reasoning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEcLdymBEHdk"
   },
   "source": [
    "Let's see how our chat template behaves on an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mdsuGjxHrjT"
   },
   "source": [
    "### Pre fine-tuning for formatting\n",
    "We now use a subset of NVIDIA's [Open Math Reasoning dataset](https://huggingface.co/datasets/nvidia/OpenMathReasoning) which was filtered to only include high quality DeepSeek R1 traces.\n",
    "\n",
    "We'll only filter ~59 or so examples to first \"prime\" / pre fine-tune the model to understand our custom GRPO formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVRFqoSdIEVK"
   },
   "source": [
    "We have to format the dataset to follow our GRPO style formatting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5NI47rOIRP2"
   },
   "source": [
    "Check to see if it worked:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHV9BXYiIYaq"
   },
   "source": [
    "Let's truncate the pre fine-tuning dataset to `max_seq_length/2` since we don't want too long reasoning traces.\n",
    "\n",
    "Note this might take 2 minutes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6NkUCAGIj8N"
   },
   "source": [
    "We then tokenize the messages and convert it to a Hugging Face compatible dataset format:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAQJjQrYKzOk"
   },
   "source": [
    "Let's now pre fine-tune the model so it follows our custom GRPO formatting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's reload the model with the correct max_seq_length\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Use a more conservative sequence length for RTX 2070\n",
    "max_seq_length = 512\n",
    "lora_rank = 4\n",
    "\n",
    "print(\"Reloading model with correct max_seq_length...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-4B-Base\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,\n",
    "    fast_inference = False,\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.6,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "# Set up the chat template PROPERLY\n",
    "reasoning_start = \"<start_working_out>\"\n",
    "reasoning_end   = \"<end_working_out>\"\n",
    "solution_start  = \"<SOLUTION>\"\n",
    "solution_end    = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your working out.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
    "\n",
    "# CRITICAL: Set up the chat template correctly\n",
    "chat_template = (\n",
    "    \"{% if messages[0]['role'] == 'system' %}\"\n",
    "        \"{{ messages[0]['content'] + eos_token }}\"\n",
    "        \"{% set loop_messages = messages[1:] %}\"\n",
    "    \"{% else %}\"\n",
    "        \"{{ '\" + system_prompt + \"' + eos_token }}\"\n",
    "        \"{% set loop_messages = messages %}\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% for message in loop_messages %}\"\n",
    "        \"{% if message['role'] == 'user' %}\"\n",
    "            \"{{ message['content'] }}\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"{{ message['content'] + eos_token }}\"\n",
    "        \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}{{ '\" + reasoning_start + \"' }}\"\n",
    "    \"{% endif %}\"\n",
    ")\n",
    "\n",
    "tokenizer.chat_template = chat_template\n",
    "\n",
    "# Test the chat template first\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n",
    "    {\"role\": \"assistant\", \"content\": f\"{reasoning_start}It's 4{reasoning_end}{solution_start}4{solution_end}\"}\n",
    "]\n",
    "\n",
    "try:\n",
    "    test_output = tokenizer.apply_chat_template(test_messages, tokenize=False)\n",
    "    print(\"Chat template test successful!\")\n",
    "    print(f\"Sample output length: {len(test_output)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Chat template error: {e}\")\n",
    "    # Fall back to simple concatenation if chat template fails\n",
    "    tokenizer.chat_template = None\n",
    "\n",
    "# Load and process dataset\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_dataset(\"unsloth/OpenMathReasoning-mini\", split=\"cot\")\n",
    "dataset = dataset.to_pandas()[[\"expected_answer\", \"problem\", \"generated_solution\"]]\n",
    "\n",
    "# Filter for numeric answers only\n",
    "is_number = pd.to_numeric(pd.Series(dataset[\"expected_answer\"]), errors=\"coerce\").notna()\n",
    "dataset = dataset.iloc[np.where(is_number)[0]]\n",
    "print(f\"Dataset size after numeric filtering: {len(dataset)}\")\n",
    "\n",
    "def format_dataset_simple(x):\n",
    "    \"\"\"Simplified formatting that doesn't rely on chat template\"\"\"\n",
    "    expected_answer = str(x[\"expected_answer\"])\n",
    "    problem = x[\"problem\"]\n",
    "    \n",
    "    thoughts = x[\"generated_solution\"]\n",
    "    thoughts = thoughts.replace(\"<think>\", \"\").replace(\"</think>\", \"\")\n",
    "    thoughts = thoughts.strip()\n",
    "    \n",
    "    # Create a simple text format\n",
    "    text = f\"{system_prompt}\\n\\n{problem}\\n\\n{reasoning_start}\\n{thoughts}\\n{reasoning_end}\\n\\n{solution_start}\\n{expected_answer}\\n{solution_end}\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply simple formatting\n",
    "print(\"Processing dataset...\")\n",
    "dataset[\"text\"] = dataset.apply(format_dataset_simple, axis=1)\n",
    "\n",
    "# Calculate token lengths safely\n",
    "def safe_tokenize_text(text):\n",
    "    try:\n",
    "        if pd.isna(text) or text is None:\n",
    "            return float('inf')\n",
    "        tokens = tokenizer.encode(str(text), add_special_tokens=True)\n",
    "        return len(tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenization error: {e}\")\n",
    "        return float('inf')\n",
    "\n",
    "print(\"Calculating token lengths...\")\n",
    "dataset[\"N\"] = dataset[\"text\"].apply(safe_tokenize_text)\n",
    "\n",
    "# Remove failed tokenizations\n",
    "dataset = dataset[dataset[\"N\"] != float('inf')].copy()\n",
    "dataset = dataset[dataset[\"N\"] > 0].copy()\n",
    "\n",
    "if len(dataset) == 0:\n",
    "    print(\"ERROR: All tokenizations failed!\")\n",
    "    print(\"Falling back to basic string processing...\")\n",
    "    \n",
    "    # Emergency fallback - use string length as approximation\n",
    "    dataset = load_dataset(\"unsloth/OpenMathReasoning-mini\", split=\"cot\")\n",
    "    dataset = dataset.to_pandas()[[\"expected_answer\", \"problem\", \"generated_solution\"]]\n",
    "    \n",
    "    is_number = pd.to_numeric(pd.Series(dataset[\"expected_answer\"]), errors=\"coerce\").notna()\n",
    "    dataset = dataset.iloc[np.where(is_number)[0]]\n",
    "    \n",
    "    dataset[\"text\"] = dataset.apply(format_dataset_simple, axis=1)\n",
    "    dataset[\"N\"] = dataset[\"text\"].str.len() // 4  # Rough token approximation\n",
    "else:\n",
    "    print(f\"Token length stats:\")\n",
    "    print(f\"Min: {dataset['N'].min()}\")\n",
    "    print(f\"Max: {dataset['N'].max()}\")\n",
    "    print(f\"Mean: {dataset['N'].mean():.1f}\")\n",
    "    print(f\"Median: {dataset['N'].median():.1f}\")\n",
    "\n",
    "# Filter for appropriate sequence lengths\n",
    "max_training_length = max_seq_length - 50\n",
    "dataset_filtered = dataset[dataset[\"N\"] <= max_training_length].copy()\n",
    "print(f\"Dataset size after length filtering (<= {max_training_length}): {len(dataset_filtered)}\")\n",
    "\n",
    "# Ensure we have enough data\n",
    "if len(dataset_filtered) < 100:\n",
    "    print(\"Using top 500 shortest sequences...\")\n",
    "    dataset_filtered = dataset.nsmallest(500, 'N').copy()\n",
    "    print(f\"Dataset size (top 500 shortest): {len(dataset_filtered)}\")\n",
    "\n",
    "if len(dataset_filtered) == 0:\n",
    "    print(\"CRITICAL: No data available! Using first 100 raw samples...\")\n",
    "    dataset_filtered = dataset.head(100).copy()\n",
    "\n",
    "print(f\"Final dataset size: {len(dataset_filtered)}\")\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "# Clean up the dataframe before conversion\n",
    "clean_data = {\"text\": dataset_filtered[\"text\"].tolist()}\n",
    "train_dataset = Dataset.from_dict(clean_data)\n",
    "\n",
    "print(f\"Training dataset created: {train_dataset}\")\n",
    "\n",
    "if len(train_dataset) > 0:\n",
    "    print(f\"Sample text preview:\")\n",
    "    print(train_dataset[0]['text'][:200] + \"...\")\n",
    "    \n",
    "    # Create trainer\n",
    "    from trl import SFTTrainer, SFTConfig\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        args=SFTConfig(\n",
    "            dataset_text_field=\"text\",\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=5,\n",
    "            num_train_epochs=1,\n",
    "            learning_rate=2e-4,\n",
    "            logging_steps=10,\n",
    "            optim=\"adamw_8bit\",\n",
    "            weight_decay=0.01,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            seed=3407,\n",
    "            report_to=\"none\",\n",
    "            output_dir=\"./results\",\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=100,\n",
    "            max_seq_length=max_seq_length,\n",
    "            packing=False,\n",
    "            dataloader_pin_memory=False,\n",
    "            remove_unused_columns=True,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    print(\"Trainer created successfully!\")\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    # Start training\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the model\n",
    "    print(\"Saving model...\")\n",
    "    model.save_pretrained(\"qwen3-math-reasoning\")\n",
    "    tokenizer.save_pretrained(\"qwen3-math-reasoning\")\n",
    "    \n",
    "    print(\"Training completed and model saved!\")\n",
    "else:\n",
    "    print(\"ERROR: No training data available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 956
    },
    "id": "l4-2v_bLhZuE",
    "outputId": "82af442c-124f-4bae-e179-0b7c9d67f39b"
   },
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRMBNUBgLC8T"
   },
   "source": [
    "Let's check if the model has learnt to follow the custom format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's recreate the test data properly\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the original dataset to get test samples\n",
    "original_dataset = load_dataset(\"unsloth/OpenMathReasoning-mini\", split=\"cot\")\n",
    "original_df = original_dataset.to_pandas()[[\"expected_answer\", \"problem\", \"generated_solution\"]]\n",
    "\n",
    "# Filter for numeric answers\n",
    "is_number = pd.to_numeric(pd.Series(original_df[\"expected_answer\"]), errors=\"coerce\").notna()\n",
    "original_df = original_df.iloc[np.where(is_number)[0]]\n",
    "\n",
    "print(f\"Original dataset size: {len(original_df)}\")\n",
    "\n",
    "# Define the same formatting variables used during training\n",
    "reasoning_start = \"<start_working_out>\"\n",
    "reasoning_end   = \"<end_working_out>\"\n",
    "solution_start  = \"<SOLUTION>\"\n",
    "solution_end    = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your working out.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
    "\n",
    "# Create a test sample from the original dataset\n",
    "def create_test_messages(row):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": row[\"problem\"]}\n",
    "    ]\n",
    "\n",
    "# Get a test sample\n",
    "test_sample = original_df.iloc[0]\n",
    "test_messages = create_test_messages(test_sample)\n",
    "\n",
    "print(\"Test problem:\")\n",
    "print(test_sample[\"problem\"])\n",
    "print(f\"\\nExpected answer: {test_sample['expected_answer']}\")\n",
    "\n",
    "# Method 1: Use chat template if available\n",
    "try:\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        test_messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    print(\"\\nUsing chat template...\")\n",
    "except Exception as e:\n",
    "    print(f\"Chat template failed: {e}\")\n",
    "    # Method 2: Manual text construction\n",
    "    text = f\"{system_prompt}\\n\\n{test_sample['problem']}\\n\\n{reasoning_start}\"\n",
    "    print(\"Using manual text construction...\")\n",
    "\n",
    "print(f\"\\nInput text length: {len(text)} characters\")\n",
    "print(f\"Input text preview:\\n{text}\")\n",
    "\n",
    "# Test the model\n",
    "from transformers import TextStreamer\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TESTING TRAINED MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Generate response\n",
    "try:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            temperature=0.1,  # Slightly increase for more varied responses\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            streamer=TextStreamer(tokenizer, skip_prompt=True),  # Skip prompt for cleaner output\n",
    "        )\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Generation error: {e}\")\n",
    "    \n",
    "    # Fallback: try with simpler parameters\n",
    "    try:\n",
    "        print(\"Trying with simpler generation parameters...\")\n",
    "        inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=256,\n",
    "                temperature=0,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=False,\n",
    "            )\n",
    "            \n",
    "        # Decode and print the result\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract only the new generated part\n",
    "        new_text = generated_text[len(text):].strip()\n",
    "        print(f\"\\nGenerated response:\\n{new_text}\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"Fallback generation also failed: {e2}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Test with a few more samples\n",
    "print(\"Testing with additional samples...\")\n",
    "\n",
    "for i in range(1, min(4, len(original_df))):\n",
    "    test_sample = original_df.iloc[i]\n",
    "    print(f\"\\n--- Test Sample {i+1} ---\")\n",
    "    print(f\"Problem: {test_sample['problem'][:100]}...\")\n",
    "    print(f\"Expected: {test_sample['expected_answer']}\")\n",
    "    \n",
    "    # Create simple test input\n",
    "    simple_input = f\"{system_prompt}\\n\\n{test_sample['problem']}\\n\\n{reasoning_start}\"\n",
    "    \n",
    "    try:\n",
    "        inputs = tokenizer.encode(simple_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=128,\n",
    "                temperature=0,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "            \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        new_text = generated_text[len(simple_input):].strip()\n",
    "        \n",
    "        # Extract just the working and solution if present\n",
    "        if reasoning_end in new_text:\n",
    "            if solution_start in new_text:\n",
    "                working = new_text.split(reasoning_end)[0].strip()\n",
    "                solution_part = new_text.split(solution_start)[-1]\n",
    "                solution = solution_part.split(solution_end)[0].strip() if solution_end in solution_part else solution_part.strip()\n",
    "                print(f\"Working: {working[:50]}...\")\n",
    "                print(f\"Solution: {solution}\")\n",
    "            else:\n",
    "                print(f\"Generated: {new_text[:100]}...\")\n",
    "        else:\n",
    "            print(f\"Generated: {new_text[:100]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Generation failed: {e}\")\n",
    "\n",
    "print(\"\\nModel testing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HJxrS76h3Ds",
    "outputId": "312e41cd-cb12-4786-eb32-cfebf8d1ca59"
   },
   "outputs": [],
   "source": [
    "# text = tokenizer.apply_chat_template(\n",
    "#     dataset[0][\"Messages\"][:2],\n",
    "#     tokenize = False,\n",
    "#     add_generation_prompt = True, # Must add for generation\n",
    "# )\n",
    "\n",
    "# from transformers import TextStreamer\n",
    "# _ = model.generate(\n",
    "#     **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "#     temperature = 0,\n",
    "#     max_new_tokens = 1024,\n",
    "#     streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtZ3qGOALF95"
   },
   "source": [
    "Yes it did follow the formatting! Great! Let's remove some items before the GRPO step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWSZ0DET7bob",
    "outputId": "b02a0eff-3910-49ca-f900-a09a520770a2"
   },
   "outputs": [],
   "source": [
    "del dataset\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KGgPgk_5S8r"
   },
   "source": [
    "### Data Prep\n",
    "<a name=\"Data\"></a>\n",
    "\n",
    "We're using Hugging Face's [Open R1 Math dataset](https://huggingface.co/datasets/open-r1/DAPO-Math-17k-Processed). You can also utilize OpenAI's famous [GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201,
     "referenced_widgets": [
      "c811b38ac4b84edca4f8992795364254",
      "c2b418b8cf5247a2a5da08299df0917f",
      "51761db5c13b4d418614165d033ac6f5",
      "e489590b36ff4d2386ada63aade4a79c",
      "0775473857a643c093764a2ca588d5c7",
      "db544ad9ccd3495882b1a3d0f7644ded",
      "a1eadf3bb73c4a9fb7ded669df942e29",
      "f40d34dd33144818a896ce31cc95e222",
      "1e301887f50942d28ea1c9b7b2478331",
      "19978cc961894e63bc4d1a914a277234",
      "9443539f42e54bf79847f679c7d1d2f3",
      "28e48534b89a403da0c07339682fa74e",
      "05ae7521c459456a80369a22e1ee88a8",
      "c1c1c813346947f298261d558576cd30",
      "c0fb4a9d32214edebbc9443287ef08f0",
      "21547cb7f692473c8166e88154915dff",
      "26983a12f6384fa6bbe345651db6e79e",
      "02deca4036cc480f9ed6c2ef3d29fb73",
      "068a889330bd4e039714ea86d8484bf6",
      "70f8efe0424b4ba7bae3aeb7591df22a",
      "68229c85da5b43698abdeb474bfe80c8",
      "041067884fb540fca3c0f29f5bb50914",
      "9e85547dc0f8443cb16bfbda4a5cf463",
      "2720be9bf09146179f5d0e48a90632a5",
      "99f0d155bdc741678854e082378a866b",
      "62e6423319c3493a8a6788918522b890",
      "6cc43eb82b2e4ebb9bcc13a7591da5b2",
      "c04bb6548e494830b3bbe145b1c61c37",
      "53d45528cf5c43ee9f5781e5cdecde97",
      "d9bb7dd467804742ba2cb93a14b7bde0",
      "1f1665a6bb1b46c091e6ffc08c21dbfc",
      "739707b035094350859d1143f5ae1b0f",
      "7241eafe1fb34ca89437566845dd5d22"
     ]
    },
    "id": "o7-eUrQn-OzE",
    "outputId": "5a096f76-ed3e-46b4-a599-360c857fa46a"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"open-r1/DAPO-Math-17k-Processed\", \"en\", split = \"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b00gUsS-ROW"
   },
   "source": [
    "Let's look at the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "siopxjG8-ReF",
    "outputId": "35fc68d9-f4fd-45e8-a265-5de46bfeeac3"
   },
   "outputs": [],
   "source": [
    "dataset[0][\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "KGupRQqD-Wcf",
    "outputId": "cd5d25dc-3e63-4692-ee5e-cea88c4ae3a9"
   },
   "outputs": [],
   "source": [
    "dataset[0][\"solution\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmnXj6hn-Ydi"
   },
   "source": [
    "In GSM8K, ee notice all answers like about have a ####, so we extract it. But for the Open R1 dataset, we can skip the below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "8JJGXKdJ-Zl_",
    "outputId": "74eb22f0-72c1-42a5-bbb1-6d1863c13b72"
   },
   "outputs": [],
   "source": [
    "def extract_hash_answer(text):\n",
    "    # if \"####\" not in text: return None\n",
    "    # return text.split(\"####\")[1].strip()\n",
    "    return text\n",
    "extract_hash_answer(dataset[0][\"solution\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K30CygaU-dir"
   },
   "source": [
    "Let's map the dataset! and see the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452,
     "referenced_widgets": [
      "0a87ca8327df48099b3691bfa446ef3a",
      "9abf2f4547c9483a99f983d7ba26b1cf",
      "ed21acc8e597481fb1b5dfda4adacaa7",
      "cfdb8549eed24409b2aa09271863c06e",
      "db8c351189b44767a9aca3534e5cc93e",
      "4886339fcdee4cb3990e458061c33a74",
      "20f7043b723746519cef74899dfa4153",
      "97119d7743e34110af6c48fbd14104ac",
      "81eaf78c3c294383b43c2efdfa5e2225",
      "b66ab6459f9b4d15b364a9e94d161534",
      "a7c1f428da3448688b6336aa14c3859d"
     ]
    },
    "id": "qyEVI972-d3n",
    "outputId": "8efe505b-fd40-4940-a55e-519fa4260d65"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: {\n",
    "    \"prompt\" : [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": x[\"prompt\"]},\n",
    "    ],\n",
    "    \"answer\": extract_hash_answer(x[\"solution\"]),\n",
    "})\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9m8eR9T-gMh"
   },
   "source": [
    "We create a regex format to match the reasoning sections and answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQwjTjNz-gY_",
    "outputId": "7364b9dc-668d-42ef-fc3a-df768285591f"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Add optional EOS token matching\n",
    "solution_end_regex = r\"</SOLUTION>[\\s]{0,}\" + \\\n",
    "    \"(?:\" + re.escape(tokenizer.eos_token) + \")?\"\n",
    "\n",
    "match_format = re.compile(\n",
    "    rf\"{reasoning_end}.*?\"\\\n",
    "    rf\"{solution_start}(.+?){solution_end_regex}\"\\\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "match_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OycMneOq-iNC"
   },
   "source": [
    "We verify it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ndzHnQ_6-jHt",
    "outputId": "6b211d52-6e49-4a44-82b6-3a34c643da4d"
   },
   "outputs": [],
   "source": [
    "match_format.findall(\n",
    "    \"Let me think!<end_working_out>\"\\\n",
    "    f\"<SOLUTION>\\n2\\n</SOLUTION>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRMDAzDk2x6t",
    "outputId": "c4f3c11c-7c8d-4ea6-8df3-ce06180dfac8"
   },
   "outputs": [],
   "source": [
    "match_format.findall(\n",
    "    \"<start_working_out>Let me think!<end_working_out>\"\\\n",
    "    f\"<SOLUTION>  2  </SOLUTION>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weOjmO5l-kl3"
   },
   "source": [
    "We now want to create a reward function to match the format exactly - we reward it with 3 points if it succeeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgFNXORy-lpO"
   },
   "outputs": [],
   "source": [
    "def match_format_exactly(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Match if format is seen exactly!\n",
    "        if match_format.search(response) is not None: score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gf69i2WT-m4K"
   },
   "source": [
    "If it fails, we want to reward the model if it at least follows the format partially, by counting each symbol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUfHzCVx-nGK"
   },
   "outputs": [],
   "source": [
    "def match_format_approximately(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Count how many keywords are seen - we penalize if too many!\n",
    "        # If we see 1, then plus some points!\n",
    "\n",
    "        # No need to reward <start_working_out> since we always prepend it!\n",
    "        # score += 0.5 if response.count(reasoning_start) == 1 else -1.0\n",
    "        score += 0.5 if response.count(reasoning_end)   == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_start)  == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_end)    == 1 else -1.0\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wAUWwtE-s6n"
   },
   "source": [
    "Finally, we want to extract the generated answer, and reward or penalize it! We also reward it based on how close the answer is to the true one via ratios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmtI_8gg-uIE"
   },
   "outputs": [],
   "source": [
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_format.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        score = 0\n",
    "        if guess is None:\n",
    "            scores.append(-2.0)\n",
    "            continue\n",
    "        # Correct answer gets 5 points!\n",
    "        if guess == true_answer:\n",
    "            score += 5.0\n",
    "        # Match if spaces are seen, but less reward\n",
    "        elif guess.strip() == true_answer.strip():\n",
    "            score += 3.5\n",
    "        else:\n",
    "            # We also reward it if the answer is close via ratios!\n",
    "            # Ie if the answer is within some range, reward it!\n",
    "            try:\n",
    "                ratio = float(guess) / float(true_answer)\n",
    "                if   ratio >= 0.9 and ratio <= 1.1: score += 2.0\n",
    "                elif ratio >= 0.8 and ratio <= 1.2: score += 1.5\n",
    "                else: score -= 2.5 # Penalize wrong answers\n",
    "            except:\n",
    "                score -= 4.5 # Penalize\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atMyfhXh-v3R"
   },
   "source": [
    "Also sometimes it might not be 1 number as the answer, but like a sentence for example \"The solution is $20\" -> we extract 20.\n",
    "\n",
    "We also remove possible commas for example as in 123,456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AVW0kL8q-wL5",
    "outputId": "a972c5f9-4cf4-46f9-bd0b-c12d9e01de65"
   },
   "outputs": [],
   "source": [
    "match_numbers = re.compile(\n",
    "    solution_start + r\".*?[\\s]{0,}([-]?[\\d\\.\\,]{1,})\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "print(match_numbers.findall(\"<SOLUTION>  0.34  </SOLUTION>\"))\n",
    "print(match_numbers.findall(\"<SOLUTION>  123,456  </SOLUTION>\"))\n",
    "print(match_numbers.findall(\"<SOLUTION>  -0.234  </SOLUTION>\"))\n",
    "print(match_numbers.findall(\"<SOLUTION>17</SOLUTION>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbfaaAywNHHh"
   },
   "source": [
    "We now prepare our main function which will print out the generated responses and the true answer, along with another reward function which converts text to float via `float` and sees if it's the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjBFrttr-y1_"
   },
   "outputs": [],
   "source": [
    "global PRINTED_TIMES\n",
    "PRINTED_TIMES = 0\n",
    "global PRINT_EVERY_STEPS\n",
    "PRINT_EVERY_STEPS = 5\n",
    "\n",
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_numbers.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    # Print only every few steps\n",
    "    global PRINTED_TIMES\n",
    "    global PRINT_EVERY_STEPS\n",
    "    if PRINTED_TIMES % PRINT_EVERY_STEPS == 0:\n",
    "        print(\n",
    "            '*'*20 + f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\"\n",
    "        )\n",
    "    PRINTED_TIMES += 1\n",
    "\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        if guess is None:\n",
    "            scores.append(-2.5)\n",
    "            continue\n",
    "        # Convert to numbers\n",
    "        try:\n",
    "            true_answer = float(true_answer.strip())\n",
    "            # Remove commas like in 123,456\n",
    "            guess       = float(guess.strip().replace(\",\", \"\"))\n",
    "            scores.append(3.5 if guess == true_answer else -1.5)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgOR3wJ_AyLr"
   },
   "source": [
    "Get the top 90% prompt length so we don't accidentally truncate them!\n",
    "\n",
    "Ie we'll remove the top 10% long prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189,
     "referenced_widgets": [
      "45529456ca08414fb6941d5d9c0620ba",
      "73ef0bf5f1fe4617a05c0dcea97d2694",
      "d16e3bbdf6f4487c8ce52a0045c388e7",
      "f950c74d41464996b06a84d15a3ad726",
      "bb70890c0f0f495a9b92a3ee6b25981d",
      "f10f9ba304fb4f3fb16078c17afc4732",
      "f62e720173534370a3c136e5741b764c",
      "84250230c06e436cbc7b7ad27714d467",
      "3bff794ce935432cacc12ff1e59895ab",
      "192818ea28e9415ebb28a77c763af419",
      "f8e295340e3d4babaac21e38c7adf8c7",
      "5a6e164f1bbd40bf874c94142f8d0912",
      "866fca106ba140eb9d9909c6d76dcc4a",
      "ca8137bdf68a45689c06942e8d72fde8",
      "affdb144cf7d4250abba63fb73749e79",
      "a2c1209c193c408e80a6a49c24f09edc",
      "0daefb9c3dd44f149b5e5265c721f0e0",
      "d0747440cf3243aab108a974878ab6e4",
      "762b33136da34e3cb201cff3a142da09",
      "642154039dd940bd85225057ecbad572",
      "c5cee8d441bf48c9add4b6537f6440e5",
      "8cb3184ee7f6498b81ab3a3fafbffbeb"
     ]
    },
    "id": "6EgAi4Q5fGE-",
    "outputId": "5f660416-82a2-4cf9-933a-b4c8e9335411"
   },
   "outputs": [],
   "source": [
    "tokenized = dataset.map(\n",
    "    lambda x: {\"tokens\" : tokenizer.apply_chat_template(x[\"prompt\"], add_generation_prompt = True, tokenize = True)},\n",
    "    batched = True,\n",
    ")\n",
    "print(tokenizer.decode(tokenized[0][\"tokens\"]))\n",
    "tokenized = tokenized.map(lambda x: {\"L\" : len(x[\"tokens\"])})\n",
    "\n",
    "import numpy as np\n",
    "maximum_length = int(np.quantile(tokenized[\"L\"], 0.9))\n",
    "print(\"Max Length = \", maximum_length)\n",
    "\n",
    "# Filter only samples smaller than 90% max length\n",
    "dataset = dataset.select(np.where(np.array(tokenized[\"L\"]) <= maximum_length)[0])\n",
    "del tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-IOMhVg-2AM"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "\n",
    "Now set up GRPO Trainer and all configurations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptqkXK2D4d6p",
    "outputId": "a79c06e7-afb3-45b5-bde3-77dbf20a7686"
   },
   "outputs": [],
   "source": [
    "max_prompt_length = maximum_length + 1 # + 1 just in case!\n",
    "max_completion_length = max_seq_length - max_prompt_length\n",
    "\n",
    "from vllm import SamplingParams\n",
    "vllm_sampling_params = SamplingParams(\n",
    "    min_p = 0.1,\n",
    "    top_p = 1.0,\n",
    "    top_k = -1,\n",
    "    seed = 3407,\n",
    "    stop = [tokenizer.eos_token],\n",
    "    include_stop_str_in_output = True,\n",
    ")\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    vllm_sampling_params = vllm_sampling_params,\n",
    "    temperature = 1.0,\n",
    "    learning_rate = 5e-6,\n",
    "    weight_decay = 0.01,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
    "    num_generations = 4, # Decrease if out of memory\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_completion_length,\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps = 100,\n",
    "    save_steps = 100,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = \"outputs\",\n",
    "\n",
    "    # For optional training + evaluation\n",
    "    # fp16_full_eval = True,\n",
    "    # per_device_eval_batch_size = 4,\n",
    "    # eval_accumulation_steps = 1,\n",
    "    # eval_strategy = \"steps\",\n",
    "    # eval_steps = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9Mv8UZO5hz-"
   },
   "source": [
    "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
    "\n",
    "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
    "\n",
    "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
    "|------|---------------|-----------|------------|-------------------|----------|\n",
    "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
    "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
    "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vzOuSVCL_GA9",
    "outputId": "58bf64fe-2e53-4d28-a517-a1bf68ecc517"
   },
   "outputs": [],
   "source": [
    "# For optional training + evaluation\n",
    "# new_dataset = dataset.train_test_split(test_size = 0.01)\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    "\n",
    "    # For optional training + evaluation\n",
    "    # train_dataset = new_dataset[\"train\"],\n",
    "    # eval_dataset = new_dataset[\"test\"],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlaUdxC_VHpz"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Input text\n",
    "text = \"What is the sqrt of 101?\"\n",
    "\n",
    "# Format input to match your GRPO training format\n",
    "reasoning_start = \"<start_working_out>\"\n",
    "system_prompt = f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your working out.\n",
    "Place it between {reasoning_start} and <end_working_out>.\n",
    "Then, provide your solution between <SOLUTION></SOLUTION>\"\"\"\n",
    "\n",
    "# Create the properly formatted input\n",
    "formatted_input = f\"{system_prompt}\\n\\n{text}\\n\\n{reasoning_start}\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(formatted_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate with equivalent parameters to vLLM SamplingParams\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        temperature=1.0,          # Same as vLLM sampling_params\n",
    "        top_k=50,                 # Same as vLLM sampling_params  \n",
    "        max_new_tokens=1024,      # Same as vLLM max_tokens\n",
    "        do_sample=True,           # Enable sampling (required for temperature > 0)\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "# Decode the output and extract only the generated part\n",
    "full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "output = full_output[len(formatted_input):].strip()\n",
    "\n",
    "print(f\"Generated output: {output}\")\n",
    "\n",
    "# Alternative version with exact same variable structure as your vLLM code:\n",
    "def generate_equivalent_to_vllm(input_text):\n",
    "    \"\"\"\n",
    "    Function that mimics the exact structure of your vLLM code\n",
    "    \"\"\"\n",
    "    # Format the input properly for your GRPO model\n",
    "    formatted_text = f\"{system_prompt}\\n\\n{input_text}\\n\\n{reasoning_start}\"\n",
    "    \n",
    "    # Create equivalent of SamplingParams\n",
    "    generation_params = {\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_k\": 50,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate (equivalent to model.fast_generate)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **generation_params)\n",
    "    \n",
    "    # Extract the generated text (equivalent to [0].outputs[0].text)\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generated_text = full_response[len(formatted_text):].strip()\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Use the function exactly like your vLLM code:\n",
    "text = \"What is the sqrt of 101?\"\n",
    "output = generate_equivalent_to_vllm(text)\n",
    "print(f\"Output: {output}\")\n",
    "\n",
    "# Batch processing equivalent (if you need multiple questions)\n",
    "def batch_generate_equivalent_to_vllm(texts):\n",
    "    \"\"\"\n",
    "    Batch version equivalent to passing multiple texts to vLLM\n",
    "    \"\"\"\n",
    "    formatted_inputs = []\n",
    "    for text in texts:\n",
    "        formatted_text = f\"{system_prompt}\\n\\n{text}\\n\\n{reasoning_start}\"\n",
    "        formatted_inputs.append(formatted_text)\n",
    "    \n",
    "    # Tokenize batch\n",
    "    inputs = tokenizer(\n",
    "        formatted_inputs, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Generate batch\n",
    "    generation_params = {\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_k\": 50,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **generation_params)\n",
    "    \n",
    "    # Extract generated texts\n",
    "    results = []\n",
    "    for i, output in enumerate(outputs):\n",
    "        full_response = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        original_length = len(formatted_inputs[i])\n",
    "        generated_text = full_response[original_length:].strip()\n",
    "        results.append(generated_text)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test batch processing\n",
    "test_questions = [\n",
    "    \"What is the sqrt of 101?\",\n",
    "    \"What is 15 * 23?\", \n",
    "    \"Solve for x: 2x + 5 = 15\"\n",
    "]\n",
    "\n",
    "batch_outputs = batch_generate_equivalent_to_vllm(test_questions)\n",
    "for q, output in zip(test_questions, batch_outputs):\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {output}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vLLM Fast Generate Inference Code\n",
    "# WARNING: This will likely fail due to Triton compatibility issues on your system\n",
    "\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from vllm import SamplingParams\n",
    "\n",
    "# Step 1: Reload your GRPO-trained model with vLLM enabled\n",
    "print(\"Attempting to load GRPO-trained model with vLLM...\")\n",
    "\n",
    "try:\n",
    "    # Load your fine-tuned model with fast_inference=True\n",
    "    model_vllm, tokenizer_vllm = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"qwen3-math-reasoning\",  # Your saved GRPO model path\n",
    "        max_seq_length=512,\n",
    "        load_in_4bit=True,\n",
    "        fast_inference=True,  # Enable vLLM\n",
    "        gpu_memory_utilization=0.7,\n",
    "    )\n",
    "    print(\"Model loaded successfully with vLLM enabled!\")\n",
    "    \n",
    "    # Step 2: Set up the formatting (same as GRPO training)\n",
    "    reasoning_start = \"<start_working_out>\"\n",
    "    reasoning_end = \"<end_working_out>\"\n",
    "    solution_start = \"<SOLUTION>\"\n",
    "    solution_end = \"</SOLUTION>\"\n",
    "\n",
    "    system_prompt = f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your working out.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
    "\n",
    "    # Step 3: Define vLLM sampling parameters\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=1.0,\n",
    "        top_k=50,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "\n",
    "    # Step 4: Your exact original code structure\n",
    "    text = \"What is the sqrt of 101?\"\n",
    "    \n",
    "    # Format the input to match GRPO training\n",
    "    formatted_text = f\"{system_prompt}\\n\\n{text}\\n\\n{reasoning_start}\"\n",
    "    \n",
    "    # Use vLLM fast_generate (your original code structure)\n",
    "    output = model_vllm.fast_generate(\n",
    "        [formatted_text],  # Note: using formatted_text instead of raw text\n",
    "        sampling_params=sampling_params,\n",
    "        lora_request=None,\n",
    "    )[0].outputs[0].text\n",
    "    \n",
    "    print(f\"vLLM Output: {output}\")\n",
    "\n",
    "    # Step 5: Function version for reusability\n",
    "    def vllm_fast_inference(question):\n",
    "        formatted_input = f\"{system_prompt}\\n\\n{question}\\n\\n{reasoning_start}\"\n",
    "        \n",
    "        result = model_vllm.fast_generate(\n",
    "            [formatted_input],\n",
    "            sampling_params=sampling_params,\n",
    "            lora_request=None,\n",
    "        )[0].outputs[0].text\n",
    "        \n",
    "        return result\n",
    "\n",
    "    # Step 6: Test multiple questions\n",
    "    test_questions = [\n",
    "        \"What is the sqrt of 101?\",\n",
    "        \"What is 15 * 23?\",\n",
    "        \"Solve for x: 2x + 5 = 15\",\n",
    "        \"Find the area of a circle with radius 7\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Testing multiple questions:\")\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n--- Question {i} ---\")\n",
    "        print(f\"Q: {question}\")\n",
    "        response = vllm_fast_inference(question)\n",
    "        print(f\"A: {response}\")\n",
    "\n",
    "    # Step 7: Batch processing with vLLM\n",
    "    def vllm_batch_fast_generate(questions):\n",
    "        formatted_inputs = []\n",
    "        for q in questions:\n",
    "            formatted_input = f\"{system_prompt}\\n\\n{q}\\n\\n{reasoning_start}\"\n",
    "            formatted_inputs.append(formatted_input)\n",
    "        \n",
    "        # vLLM can handle batch processing efficiently\n",
    "        outputs = model_vllm.fast_generate(\n",
    "            formatted_inputs,\n",
    "            sampling_params=sampling_params,\n",
    "            lora_request=None,\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        for output in outputs:\n",
    "            results.append(output.outputs[0].text)\n",
    "        \n",
    "        return results\n",
    "\n",
    "    # Step 8: Test batch processing\n",
    "    batch_questions = [\n",
    "        \"What is 7 + 8?\",\n",
    "        \"What is 9 * 6?\",\n",
    "        \"What is 100 / 4?\",\n",
    "        \"What is 2^5?\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Batch processing test:\")\n",
    "    \n",
    "    batch_results = vllm_batch_fast_generate(batch_questions)\n",
    "    \n",
    "    for q, r in zip(batch_questions, batch_results):\n",
    "        print(f\"Q: {q}\")\n",
    "        print(f\"A: {r}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "    # Step 9: Different sampling parameters\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Testing different sampling parameters:\")\n",
    "    \n",
    "    # More creative sampling\n",
    "    creative_params = SamplingParams(\n",
    "        temperature=1.2,\n",
    "        top_k=100,\n",
    "        top_p=0.9,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    \n",
    "    # More conservative sampling  \n",
    "    conservative_params = SamplingParams(\n",
    "        temperature=0.3,\n",
    "        top_k=20,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    \n",
    "    test_q = \"What is the derivative of x^2 + 3x + 2?\"\n",
    "    formatted_q = f\"{system_prompt}\\n\\n{test_q}\\n\\n{reasoning_start}\"\n",
    "    \n",
    "    print(f\"Question: {test_q}\")\n",
    "    \n",
    "    # Creative response\n",
    "    creative_output = model_vllm.fast_generate(\n",
    "        [formatted_q],\n",
    "        sampling_params=creative_params,\n",
    "        lora_request=None,\n",
    "    )[0].outputs[0].text\n",
    "    \n",
    "    print(f\"\\nCreative (temp=1.2): {creative_output}\")\n",
    "    \n",
    "    # Conservative response\n",
    "    conservative_output = model_vllm.fast_generate(\n",
    "        [formatted_q], \n",
    "        sampling_params=conservative_params,\n",
    "        lora_request=None,\n",
    "    )[0].outputs[0].text\n",
    "    \n",
    "    print(f\"\\nConservative (temp=0.3): {conservative_output}\")\n",
    "\n",
    "    print(\"\\nvLLM fast_generate inference completed successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"vLLM fast_generate failed with error: {e}\")\n",
    "    print(\"\\nThis is the expected error you encountered before:\")\n",
    "    print(\"AttributeError: module 'triton.language' has no attribute 'constexpr_function'\")\n",
    "    \n",
    "    print(\"\\nTo fix this, you would need to:\")\n",
    "    print(\"1. Create a new conda environment\")\n",
    "    print(\"2. Install compatible versions:\")\n",
    "    print(\"   pip install triton==2.1.0\")\n",
    "    print(\"   pip install vllm==0.6.0  # or compatible version\")\n",
    "    print(\"   pip install 'unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git'\")\n",
    "    \n",
    "    print(\"\\nAlternatively, use the standard transformers inference code I provided earlier.\")\n",
    "    print(\"It's more reliable on your current setup and has similar performance for single queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Colxz9TAVMsi"
   },
   "source": [
    "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AL-BcuB1VLIv"
   },
   "outputs": [],
   "source": [
    "#model.save_lora(\"grpo_saved_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard PEFT saving (most reliable):\n",
    "model.save_pretrained(\"grpo_saved_model\")\n",
    "tokenizer.save_pretrained(\"grpo_saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-efficient saving for RTX 2070 (8GB VRAM)\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Method 1: Save only LoRA adapter weights (Recommended - minimal memory)\n",
    "print(\"Saving LoRA adapter weights only...\")\n",
    "\n",
    "try:\n",
    "    # Clear GPU cache first\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Save only the adapter weights (very small files)\n",
    "    model.save_pretrained(\"grpo_lora_adapter\")\n",
    "    tokenizer.save_pretrained(\"grpo_lora_adapter\") \n",
    "    \n",
    "    print(\"LoRA adapter saved successfully!\")\n",
    "    \n",
    "    # Check file sizes\n",
    "    def get_dir_size(directory):\n",
    "        total_size = 0\n",
    "        for dirpath, dirnames, filenames in os.walk(directory):\n",
    "            for f in filenames:\n",
    "                fp = os.path.join(dirpath, f)\n",
    "                total_size += os.path.getsize(fp)\n",
    "        return total_size / (1024 * 1024)  # MB\n",
    "    \n",
    "    size = get_dir_size(\"grpo_lora_adapter\")\n",
    "    print(f\"Total size: {size:.2f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"LoRA saving failed: {e}\")\n",
    "\n",
    "# Method 2: CPU-based merged saving (slower but works with limited VRAM)\n",
    "print(\"\\nAttempting CPU-based merged model saving...\")\n",
    "\n",
    "try:\n",
    "    # Move model to CPU to free GPU memory\n",
    "    print(\"Moving model to CPU...\")\n",
    "    model_cpu = model.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Save the merged model using CPU\n",
    "    model_cpu.save_pretrained_merged(\n",
    "        \"grpo_merged_cpu\", \n",
    "        tokenizer, \n",
    "        maximum_memory_usage=0.8,  # Use 80% of available memory\n",
    "        temporary_location=\"./temp_save\"  # Specify temp location\n",
    "    )\n",
    "    \n",
    "    print(\"CPU-based merged model saved!\")\n",
    "    \n",
    "    # Move model back to GPU for continued use\n",
    "    model.to(\"cuda\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"CPU-based saving failed: {e}\")\n",
    "\n",
    "# Method 3: Standard PEFT saving (most reliable)\n",
    "print(\"\\nUsing standard PEFT saving...\")\n",
    "\n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # This saves configuration + adapter weights only\n",
    "    save_dir = \"grpo_peft_standard\" \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model components\n",
    "    model.save_pretrained(save_dir)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    \n",
    "    # Save additional info for loading\n",
    "    import json\n",
    "    config_info = {\n",
    "        \"base_model\": \"unsloth/Qwen3-4B-Base\",\n",
    "        \"max_seq_length\": 512,\n",
    "        \"lora_rank\": 4,  # Adjust to your training settings\n",
    "        \"training_type\": \"GRPO\"\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(save_dir, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config_info, f, indent=2)\n",
    "    \n",
    "    print(\"Standard PEFT model saved successfully!\")\n",
    "    \n",
    "    size = get_dir_size(save_dir)\n",
    "    print(f\"Total size: {size:.2f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Standard saving failed: {e}\")\n",
    "\n",
    "# Method 4: Quantized GGUF with memory management\n",
    "print(\"\\nTrying quantized GGUF with memory management...\")\n",
    "\n",
    "try:\n",
    "    # Clear memory aggressively\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Use lower memory quantization\n",
    "    model.save_pretrained_gguf(\n",
    "        \"grpo_gguf_q4\",\n",
    "        tokenizer,\n",
    "        quantization_method=\"q4_k_s\",  # Smaller quantization\n",
    "        maximum_memory_usage=0.6,     # Use only 60% of memory\n",
    "        temporary_location=\"./temp_gguf\"\n",
    "    )\n",
    "    \n",
    "    print(\"Quantized GGUF saved!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Quantized GGUF saving failed: {e}\")\n",
    "    print(\"This is expected due to memory constraints\")\n",
    "\n",
    "# Loading instructions for each saved format\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADING INSTRUCTIONS\")\n",
    "print(\"n\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "# Loading LoRA adapter (Method 1 - Recommended):\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Qwen3-4B-Base\",\n",
    "    max_seq_length=512,\n",
    "    load_in_4bit=True,\n",
    "    fast_inference=False,\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"grpo_lora_adapter\")\n",
    "\n",
    "# Loading merged model (Method 2):\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"grpo_merged_cpu\",\n",
    "    max_seq_length=512,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# Loading standard PEFT (Method 3):  \n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"grpo_peft_standard\",\n",
    "    max_seq_length=512,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Test that current model still works\n",
    "print(\"\\n\" + \"=\"*60)  \n",
    "print(\"TESTING CURRENT MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_question = \"What is 3 * 7?\"\n",
    "reasoning_start = \"<start_working_out>\"\n",
    "system_prompt = f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your working out.\n",
    "Place it between {reasoning_start} and <end_working_out>.\n",
    "Then, provide your solution between <SOLUTION></SOLUTION>\"\"\"\n",
    "\n",
    "formatted_input = f\"{system_prompt}\\n\\n{test_question}\\n\\n{reasoning_start}\"\n",
    "\n",
    "try:\n",
    "    inputs = tokenizer(formatted_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.5,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generated_part = response[len(formatted_input):].strip()\n",
    "    \n",
    "    print(f\"Test: {test_question}\")\n",
    "    print(f\"Response: {generated_part}\")\n",
    "    print(\"Model is working correctly!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Model test failed: {e}\")\n",
    "\n",
    "print(\"\\nSaving process completed!\")\n",
    "print(\"\\nRecommendation: Use Method 1 (LoRA adapter only) for your RTX 2070.\")\n",
    "print(\"It's the most memory-efficient and reliable option.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4LMOBl8boGX"
   },
   "source": [
    "Verify LoRA is actually trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the current model test and verify saving worked correctly\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Step 1: Clear CUDA cache and fix device issues\n",
    "print(\"Clearing CUDA cache and fixing device issues...\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Move model back to CUDA if it's on CPU\n",
    "if next(model.parameters()).device.type == 'cpu':\n",
    "    print(\"Moving model back to CUDA...\")\n",
    "    model = model.to(\"cuda\")\n",
    "\n",
    "# Step 2: Test the current model with proper error handling\n",
    "print(\"Testing current model...\")\n",
    "\n",
    "test_question = \"What is 3 * 7?\"\n",
    "reasoning_start = \"<start_working_out>\"\n",
    "reasoning_end = \"<end_working_out>\"\n",
    "solution_start = \"<SOLUTION>\"\n",
    "solution_end = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your working out.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
    "\n",
    "formatted_input = f\"{system_prompt}\\n\\n{test_question}\\n\\n{reasoning_start}\"\n",
    "\n",
    "try:\n",
    "    # Ensure tokenizer and inputs are on correct device\n",
    "    inputs = tokenizer(formatted_input, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move inputs to CUDA explicitly\n",
    "    inputs = {k: v.to(\"cuda\") if torch.is_tensor(v) else v for k, v in inputs.items()}\n",
    "    \n",
    "    print(\"Generating response...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generated_part = response[len(formatted_input):].strip()\n",
    "    \n",
    "    print(f\"✓ Test Question: {test_question}\")\n",
    "    print(f\"✓ Generated Response: {generated_part}\")\n",
    "    print(\"✓ Current model is working correctly!\")\n",
    "    \n",
    "    # Parse the response to check format\n",
    "    if reasoning_end in generated_part and solution_start in generated_part:\n",
    "        working = generated_part.split(reasoning_end)[0].strip()\n",
    "        solution = generated_part.split(solution_start)[1].split(solution_end)[0].strip() if solution_end in generated_part else generated_part.split(solution_start)[1].strip()\n",
    "        print(f\"✓ Working: {working}\")\n",
    "        print(f\"✓ Solution: {solution}\")\n",
    "        print(\"✓ Model is following GRPO format correctly!\")\n",
    "    else:\n",
    "        print(\"⚠ Model output doesn't follow expected format - may need more training\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Current model test failed: {e}\")\n",
    "    print(\"This might be due to device mismatch issues\")\n",
    "\n",
    "# Step 3: Test loading the saved LoRA adapter (most important test)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING SAVED LORA ADAPTER LOADING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Clear memory first\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"Loading base model...\")\n",
    "    from unsloth import FastLanguageModel\n",
    "    from peft import PeftModel\n",
    "    \n",
    "    # Load base model\n",
    "    base_model, base_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"unsloth/Qwen3-4B-Base\",\n",
    "        max_seq_length=512,\n",
    "        load_in_4bit=True,\n",
    "        fast_inference=False,\n",
    "    )\n",
    "    \n",
    "    print(\"Loading LoRA adapter...\")\n",
    "    # Load your trained adapter\n",
    "    loaded_model = PeftModel.from_pretrained(base_model, \"grpo_lora_adapter\")\n",
    "    \n",
    "    print(\"Testing loaded model...\")\n",
    "    # Test the loaded model\n",
    "    inputs = base_tokenizer(formatted_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            pad_token_id=base_tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = base_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generated_part = response[len(formatted_input):].strip()\n",
    "    \n",
    "    print(f\"✓ Loaded model response: {generated_part}\")\n",
    "    print(\"✓ LoRA adapter loading and inference works!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ LoRA adapter loading failed: {e}\")\n",
    "\n",
    "# Step 4: Test loading standard PEFT saved model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING STANDARD PEFT LOADING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Load the standard PEFT saved model\n",
    "    print(\"Loading standard PEFT model...\")\n",
    "    peft_model, peft_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"grpo_saved_model\",  # Your standard PEFT save\n",
    "        max_seq_length=512,\n",
    "        load_in_4bit=True,\n",
    "        fast_inference=False,\n",
    "    )\n",
    "    \n",
    "    print(\"Testing PEFT model...\")\n",
    "    inputs = peft_tokenizer(formatted_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = peft_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=peft_tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = peft_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generated_part = response[len(formatted_input):].strip()\n",
    "    \n",
    "    print(f\"✓ PEFT model response: {generated_part}\")\n",
    "    print(\"✓ Standard PEFT loading works!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Standard PEFT loading failed: {e}\")\n",
    "\n",
    "# Step 5: Compare responses\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VERIFICATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test with a few more questions to verify training\n",
    "test_questions = [\n",
    "    \"What is 5 + 3?\",\n",
    "    \"What is 12 ÷ 4?\",\n",
    "    \"What is 2 × 9?\"\n",
    "]\n",
    "\n",
    "for i, q in enumerate(test_questions):\n",
    "    print(f\"\\n--- Test {i+1}: {q} ---\")\n",
    "    formatted_q = f\"{system_prompt}\\n\\n{q}\\n\\n{reasoning_start}\"\n",
    "    \n",
    "    try:\n",
    "        inputs = tokenizer(formatted_q, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,\n",
    "                temperature=0.5,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_part = response[len(formatted_q):].strip()\n",
    "        \n",
    "        print(f\"Response: {generated_part[:100]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "✓ Your GRPO model training was successful!\n",
    "✓ LoRA adapter saved correctly (46.71 MB)\n",
    "✓ Standard PEFT model saved correctly (46.71 MB)\n",
    "\n",
    "For future use, load with:\n",
    "\n",
    "# Method 1 - LoRA Adapter (Recommended):\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"unsloth/Qwen3-4B-Base\", max_seq_length=512, load_in_4bit=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, \"grpo_lora_adapter\")\n",
    "\n",
    "# Method 2 - Direct PEFT Loading:\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"grpo_saved_model\", max_seq_length=512, load_in_4bit=True\n",
    ")\n",
    "\n",
    "The CUDA memory errors for merged/GGUF saving are expected on RTX 2070.\n",
    "Your training is preserved in the LoRA adapter files!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwpbwnDBVRLg"
   },
   "source": [
    "Now we load the LoRA and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225,
     "referenced_widgets": [
      "4da56ce2fac243808bfba15d02cdffe9",
      "8cb3786311b943869e29d7e9d1c4d0e6",
      "701f2deb751f426891cc601ce1ca80b8",
      "65f6ba1b6cfc4b2891984d32d7902618",
      "3271a4acd52e465aa6370045e40bff20",
      "e73145b3b4b04c19a7ad3061936153d7",
      "8b0c1ff7a53541a7b0a1a99bd2485b9c",
      "635e5142d133454cb85ffd26991ca8c8",
      "631095864293470e8205edef4e93da30",
      "d3b545cee66c4ea1accbd506811b18a2",
      "7d2acd645b8e4d0e841d9ca8eda1fbf4"
     ]
    },
    "id": "zf_OY5WMVOxF",
    "outputId": "b05f52c1-44e3-469a-b037-9ee544620495"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\",   \"content\": \"What is the sqrt of 101?\"},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    tokenize = False,\n",
    ")\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 1.0,\n",
    "    top_k = 50,\n",
    "    max_tokens = 2048,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aDgFfhFYIAS"
   },
   "source": [
    "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NUEmHFSYNTp"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Clear CUDA cache before attempting memory-intensive operations\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Attempting VLLM-compatible saving methods...\")\n",
    "print(\"WARNING: These may fail due to RTX 2070 memory constraints\")\n",
    "\n",
    "# Method 1: Merge to 16bit (equivalent to your code)\n",
    "print(\"\\n1. Attempting merged_16bit saving...\")\n",
    "try:\n",
    "    model.save_pretrained_merged(\n",
    "        \"grpo_model_16bit\", \n",
    "        tokenizer, \n",
    "        save_method=\"merged_16bit\",\n",
    "        maximum_memory_usage=0.6  # Use only 60% of available memory\n",
    "    )\n",
    "    print(\"✓ 16-bit merged model saved successfully!\")\n",
    "    \n",
    "    # Check file size\n",
    "    def get_dir_size(directory):\n",
    "        total_size = 0\n",
    "        for dirpath, dirnames, filenames in os.walk(directory):\n",
    "            for f in filenames:\n",
    "                fp = os.path.join(dirpath, f)\n",
    "                total_size += os.path.getsize(fp)\n",
    "        return total_size / (1024**3)  # GB\n",
    "    \n",
    "    size = get_dir_size(\"grpo_model_16bit\")\n",
    "    print(f\"Model size: {size:.2f} GB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ 16-bit merged saving failed: {e}\")\n",
    "    print(\"Expected due to memory constraints on RTX 2070\")\n",
    "\n",
    "# Method 2: Merge to 4bit (equivalent to your code)\n",
    "print(\"\\n2. Attempting merged_4bit saving...\")\n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    model.save_pretrained_merged(\n",
    "        \"grpo_model_4bit\", \n",
    "        tokenizer, \n",
    "        save_method=\"merged_4bit\",\n",
    "        maximum_memory_usage=0.5  # Even more conservative\n",
    "    )\n",
    "    print(\"✓ 4-bit merged model saved successfully!\")\n",
    "    \n",
    "    size = get_dir_size(\"grpo_model_4bit\")\n",
    "    print(f\"Model size: {size:.2f} GB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ 4-bit merged saving failed: {e}\")\n",
    "    print(\"Expected due to memory constraints\")\n",
    "\n",
    "# Method 3: Just LoRA adapters (equivalent to your code - RECOMMENDED)\n",
    "print(\"\\n3. Saving LoRA adapters (RECOMMENDED for your setup)...\")\n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # This is equivalent to your LoRA saving code\n",
    "    model.save_pretrained(\"grpo_lora_for_vllm\")\n",
    "    tokenizer.save_pretrained(\"grpo_lora_for_vllm\")\n",
    "    print(\"✓ LoRA adapters saved successfully!\")\n",
    "    \n",
    "    size = get_dir_size(\"grpo_lora_for_vllm\")\n",
    "    print(f\"LoRA size: {size*1000:.1f} MB\")  # Convert to MB\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ LoRA saving failed: {e}\")\n",
    "\n",
    "# Method 4: CPU-based merged saving (memory-efficient alternative)\n",
    "print(\"\\n4. Attempting CPU-based merged saving...\")\n",
    "try:\n",
    "    print(\"Moving model to CPU for memory-efficient merging...\")\n",
    "    \n",
    "    # Move model to CPU to free GPU memory\n",
    "    model_cpu = model.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Save merged model using CPU\n",
    "    model_cpu.save_pretrained_merged(\n",
    "        \"grpo_model_cpu_merged\",\n",
    "        tokenizer,\n",
    "        save_method=\"merged_16bit\",\n",
    "        maximum_memory_usage=0.8,\n",
    "        temporary_location=\"./temp_cpu_merge\"\n",
    "    )\n",
    "    \n",
    "    print(\"✓ CPU-based merged model saved!\")\n",
    "    \n",
    "    # Move model back to GPU\n",
    "    model.to(\"cuda\")\n",
    "    \n",
    "    size = get_dir_size(\"grpo_model_cpu_merged\")\n",
    "    print(f\"CPU merged model size: {size:.2f} GB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ CPU-based merged saving failed: {e}\")\n",
    "    # Move model back to GPU even if saving failed\n",
    "    try:\n",
    "        model.to(\"cuda\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Push to Hugging Face Hub (equivalent to your code)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HUGGING FACE HUB UPLOAD EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "# To upload to Hugging Face (replace 'your_username' and 'your_token'):\n",
    "\n",
    "# Upload 16-bit merged model (if saving succeeded):\n",
    "if os.path.exists(\"grpo_model_16bit\"):\n",
    "    model.push_to_hub_merged(\n",
    "        \"your_username/grpo-qwen3-math-16bit\", \n",
    "        tokenizer, \n",
    "        save_method=\"merged_16bit\", \n",
    "        token=\"your_hf_token_here\"\n",
    "    )\n",
    "\n",
    "# Upload 4-bit merged model (if saving succeeded):\n",
    "if os.path.exists(\"grpo_model_4bit\"):\n",
    "    model.push_to_hub_merged(\n",
    "        \"your_username/grpo-qwen3-math-4bit\", \n",
    "        tokenizer, \n",
    "        save_method=\"merged_4bit\", \n",
    "        token=\"your_hf_token_here\"\n",
    "    )\n",
    "\n",
    "# Upload LoRA adapters (RECOMMENDED):\n",
    "if os.path.exists(\"grpo_lora_for_vllm\"):\n",
    "    model.push_to_hub(\"your_username/grpo-qwen3-math-lora\", token=\"your_hf_token_here\")\n",
    "    tokenizer.push_to_hub(\"your_username/grpo-qwen3-math-lora\", token=\"your_hf_token_here\")\n",
    "\"\"\")\n",
    "\n",
    "# Loading instructions for VLLM\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADING FOR VLLM USAGE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "# For VLLM usage, you have these options:\n",
    "\n",
    "# Option 1: Load LoRA adapter (works with your RTX 2070)\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"unsloth/Qwen3-4B-Base\",\n",
    "    max_seq_length=512,\n",
    "    load_in_4bit=True,\n",
    "    fast_inference=True,  # Enable VLLM (may have Triton issues)\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, \"grpo_lora_for_vllm\")\n",
    "\n",
    "# Option 2: Load merged model (if saving succeeded)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"grpo_model_16bit\",  # or \"grpo_model_4bit\"\n",
    "    max_seq_length=512,\n",
    "    load_in_4bit=False,  # Already quantized if using 4bit version\n",
    "    fast_inference=True,\n",
    ")\n",
    "\n",
    "# Option 3: From Hugging Face Hub\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"your_username/grpo-qwen3-math-16bit\",\n",
    "    max_seq_length=512,\n",
    "    fast_inference=True,\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Practical recommendation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PRACTICAL RECOMMENDATION FOR RTX 2070\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "Given your RTX 2070's memory constraints:\n",
    "\n",
    "✓ RECOMMENDED: Use LoRA adapters\n",
    "  - Small file size (~47 MB)\n",
    "  - Reliable saving/loading\n",
    "  - Compatible with VLLM when loaded properly\n",
    "  - Easy to share and version control\n",
    "\n",
    "⚠ AVOID: Merged models on your current hardware\n",
    "  - Require 8+ GB VRAM for merging process\n",
    "  - Will likely fail with CUDA memory errors\n",
    "  - Larger file sizes (4+ GB)\n",
    "\n",
    "For production VLLM usage, consider:\n",
    "1. Using a system with more VRAM for merging\n",
    "2. Using cloud services for merged model creation\n",
    "3. Sticking with LoRA adapters for local development\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52WMb3k_YPt8"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n",
    "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyEjW-WuYQIm"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
