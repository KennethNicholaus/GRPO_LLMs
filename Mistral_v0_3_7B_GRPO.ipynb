{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral-7B v0.3 GRPO Fine-tuning & Model Conversion Summary\n",
    "\n",
    "## Model Setup & GRPO Training\n",
    "‚Ä¢ **Load Mistral-7B**: Use `FastLanguageModel.from_pretrained(\"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\")` with RTX 2070 Super settings (`fast_inference=False`, `load_in_4bit=True`, reduced LoRA rank 8-16)\n",
    "‚Ä¢ **GRPO Training**: Configure `GRPOConfig` with proper sequence lengths for Mistral (ensure `max_prompt_length + max_completion_length ‚â§ max_seq_length=128`)\n",
    "‚Ä¢ **Reward Functions**: Use multiple reward functions (`xmlcount_reward_func`, `format_reward_func`, `correctness_reward_func`) with `GRPOTrainer`\n",
    "‚Ä¢ **Save LoRA**: Use `model.save_lora(\"grpo_saved_lora\")` to save GRPO-trained adapters (~50MB vs 13GB full Mistral model)\n",
    "\n",
    "## Mistral Model Conversion Options\n",
    "‚Ä¢ **vLLM (Float16)**: `model.save_pretrained_merged(\"model_merged_16bit\", tokenizer, save_method=\"merged_16bit\")` - Merges GRPO LoRA into Mistral base (~13GB)\n",
    "‚Ä¢ **GGUF/llama.cpp**: Use `model.save_pretrained_gguf(\"model_q4_k_m\", tokenizer, quantization_method=\"q4_k_m/q8_0/f16\")` for Mistral GGUF formats (4-14GB)\n",
    "‚Ä¢ **LoRA Only**: Keep GRPO adapters separate for loading back into Mistral base model later\n",
    "\n",
    "## Mistral Inference Methods\n",
    "‚Ä¢ **Standard**: Use `model.generate()` with Mistral chat template `tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}])`\n",
    "‚Ä¢ **vLLM**: Requires merged Mistral model with `fast_inference=True` and `model.fast_generate()` \n",
    "‚Ä¢ **llama.cpp**: Use converted Mistral GGUF files with `./main -m mistral-model.gguf -p \"prompt\"`\n",
    "\n",
    "## RTX 2070 Super Optimizations for Mistral-7B\n",
    "‚Ä¢ **Memory Management**: Disable vLLM, use 4-bit quantization, LoRA rank ‚â§16, `max_seq_length=128`, enable CPU offloading if OOM\n",
    "‚Ä¢ **Loading GRPO Model**: Load Mistral base first, then apply GRPO LoRA with `model.load_adapter(\"grpo_saved_lora\")` or `PeftModel.from_pretrained()`\n",
    "‚Ä¢ **Fallback**: Switch to `unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit` if Mistral-7B doesn't fit in 8GB VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all dependencies first\n",
    "!pip install torchaudio==2.7.1\n",
    "!pip install bitsandbytes==0.47.0\n",
    "!pip install torchvision==0.22.1\n",
    "!pip install xformers==0.0.31\n",
    "!pip install unsloth-zoo\n",
    "!pip install vllm==0.10.1.1\n",
    "\n",
    "# Then, install torch last\n",
    "!pip uninstall torch -y\n",
    "!pip install torch==2.7.1\n",
    "\n",
    "!pip check\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import torchaudio\n",
    "import torchvision\n",
    "import xformers\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Triton version: {triton.__version__}\")\n",
    "print(f\"Torchaudio version: {torchaudio.__version__}\")\n",
    "print(f\"Xformers version: {xformers.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1IgoPx8ODl4q"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    # If you're not in Colab, just use pip install or uv pip install\n",
    "    !pip install unsloth vllm\n",
    "else:\n",
    "    pass # For Colab / Kaggle, we need extra instructions hidden below \\/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cc18O9yeDl4q"
   },
   "outputs": [],
   "source": [
    "#@title Colab Extra Install { display-mode: \"form\" }\n",
    "#%%capture\n",
    "import os\n",
    "!pip install --upgrade -qqq uv\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    # If you're not in Colab, just use pip install!\n",
    "    !pip install unsloth vllm\n",
    "else:\n",
    "    try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
    "    except: get_numpy = \"numpy\"\n",
    "    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
    "    except: is_t4 = False\n",
    "    get_vllm, get_triton = (\"vllm==0.10.1\", \"triton==3.2.0\") if is_t4 else (\"vllm\", \"triton\")\n",
    "    !uv pip install -qqq --upgrade \\\n",
    "        unsloth {get_vllm} {get_numpy} torchvision bitsandbytes xformers\n",
    "    !uv pip install -qqq {get_triton}\n",
    "!uv pip install transformers==4.55.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fA26-ZvUDl4r"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVL2qQZHDl4r"
   },
   "source": [
    "Load up `unsloth/mistral-7b-instruct-v0.3-bnb-4bit`, and set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Clear GPU memory first\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n",
    "\n",
    "print(\"=== Solution 1: Proper Unsloth setup for RTX 2070 Super ===\")\n",
    "\n",
    "max_seq_length = 128\n",
    "lora_rank = 16  # Significantly reduced for 8GB GPU\n",
    "\n",
    "try:\n",
    "    # Attempt 1: Standard Unsloth approach with reduced settings\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "        max_seq_length = max_seq_length,\n",
    "        load_in_4bit = True,\n",
    "        fast_inference = False,  # Disable vLLM\n",
    "        max_lora_rank = lora_rank,\n",
    "        dtype = torch.float16,  # Use FP16 to save memory\n",
    "        # Remove device_map and offload params - Unsloth handles this\n",
    "    )\n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå First attempt failed: {e}\")\n",
    "    print(\"Trying with even more conservative settings...\")\n",
    "    \n",
    "    try:\n",
    "        # Attempt 2: More conservative settings\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "            max_seq_length = 64,  # Reduced sequence length\n",
    "            load_in_4bit = True,\n",
    "            fast_inference = False,\n",
    "            max_lora_rank = 8,  # Very small rank\n",
    "            dtype = torch.float16,\n",
    "        )\n",
    "        max_seq_length = 64\n",
    "        lora_rank = 8\n",
    "        print(\"‚úÖ Model loaded with conservative settings!\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Conservative settings also failed: {e2}\")\n",
    "        print(\"Switching to smaller model...\")\n",
    "        \n",
    "        # Attempt 3: Use smaller model that fits RTX 2070 Super\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = \"unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit\",  # 1.5B model\n",
    "            max_seq_length = 128,\n",
    "            load_in_4bit = True,\n",
    "            fast_inference = False,\n",
    "            max_lora_rank = 32,\n",
    "            dtype = torch.float16,\n",
    "        )\n",
    "        max_seq_length = 128\n",
    "        lora_rank = 32\n",
    "        print(\"‚úÖ Smaller 1.5B model loaded successfully!\")\n",
    "\n",
    "# Apply LoRA - start with minimal target modules\n",
    "try:\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r = lora_rank,\n",
    "        target_modules = [\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention only\n",
    "        ],\n",
    "        lora_alpha = lora_rank,\n",
    "        use_gradient_checkpointing = \"unsloth\",\n",
    "        random_state = 3407,\n",
    "    )\n",
    "    print(\"‚úÖ LoRA applied successfully with attention modules only!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"LoRA with attention failed: {e}\")\n",
    "    print(\"Trying with even fewer modules...\")\n",
    "    \n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r = max(lora_rank // 2, 4),  # Even smaller rank\n",
    "        target_modules = [\n",
    "            \"q_proj\", \"v_proj\",  # Only query and value projections\n",
    "        ],\n",
    "        lora_alpha = max(lora_rank // 2, 4),\n",
    "        use_gradient_checkpointing = \"unsloth\",\n",
    "        random_state = 3407,\n",
    "    )\n",
    "    print(\"‚úÖ LoRA applied with minimal modules!\")\n",
    "\n",
    "# Check final memory usage\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    print(f\"GPU Memory - Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "\n",
    "print(\"üéâ Setup complete! Model ready for training.\")\n",
    "\n",
    "# Alternative: Manual device mapping approach (if needed)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Alternative Solution: Manual GPU/CPU split\")\n",
    "print(\"Uncomment below if you want to try manual device mapping:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "\"\"\"\n",
    "# This approach manually loads the base model with device mapping\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Manual device mapping for hybrid GPU/CPU loading\n",
    "device_map = {\n",
    "    \"model.embed_tokens\": 0,\n",
    "    \"model.layers.0\": 0,\n",
    "    \"model.layers.1\": 0,\n",
    "    \"model.layers.2\": 0,\n",
    "    \"model.layers.3\": 0,\n",
    "    \"model.layers.4\": 0,\n",
    "    \"model.layers.5\": 0,\n",
    "    \"model.layers.6\": 0,\n",
    "    \"model.layers.7\": 0,\n",
    "    \"model.layers.8\": 0,\n",
    "    \"model.layers.9\": 0,\n",
    "    \"model.layers.10\": 0,\n",
    "    \"model.layers.11\": 0,\n",
    "    \"model.layers.12\": 0,\n",
    "    \"model.layers.13\": 0,\n",
    "    \"model.layers.14\": 0,\n",
    "    \"model.layers.15\": 0,\n",
    "    # Rest on CPU\n",
    "    \"model.layers.16\": \"cpu\",\n",
    "    \"model.layers.17\": \"cpu\",\n",
    "    \"model.layers.18\": \"cpu\",\n",
    "    \"model.layers.19\": \"cpu\",\n",
    "    \"model.layers.20\": \"cpu\",\n",
    "    \"model.layers.21\": \"cpu\",\n",
    "    \"model.layers.22\": \"cpu\",\n",
    "    \"model.layers.23\": \"cpu\",\n",
    "    \"model.layers.24\": \"cpu\",\n",
    "    \"model.layers.25\": \"cpu\",\n",
    "    \"model.layers.26\": \"cpu\",\n",
    "    \"model.layers.27\": \"cpu\",\n",
    "    \"model.layers.28\": \"cpu\",\n",
    "    \"model.layers.29\": \"cpu\",\n",
    "    \"model.layers.30\": \"cpu\",\n",
    "    \"model.layers.31\": \"cpu\",\n",
    "    \"model.norm\": \"cpu\",\n",
    "    \"lm_head\": \"cpu\"\n",
    "}\n",
    "\n",
    "# Load with transformers first, then wrap with Unsloth\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    device_map=device_map,\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\")\n",
    "\n",
    "# Then apply Unsloth optimizations\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    base_model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîß Troubleshooting tips:\")\n",
    "print(\"1. If still getting memory errors, reduce lora_rank further (to 4 or 8)\")\n",
    "print(\"2. Close all other applications using GPU memory\")\n",
    "print(\"3. Try the smaller Qwen2.5-1.5B model - it's quite capable!\")\n",
    "print(\"4. Consider using Google Colab with T4 (16GB) for larger models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuLbyhIODl4v"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "\n",
    "We directly leverage [@willccbb](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb) for data prep and all reward functions. You are free to create your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "7a333f18cc8144168db9c269cf6e8e6c",
      "093d5e8629c143b586954a7852f98b5c",
      "a5ea7a019dbf456aa3ab37c63e81edf9",
      "adf539342ac848e99b2d075d706773ab",
      "a06e34bd1a24438b8b93130c207c2606",
      "b7f45d2aa2d141e695c7a21bfe374933"
     ]
    },
    "id": "oCjU3qa6Dl4v",
    "outputId": "27f54e3b-2141-4329-ba7b-4c3b17c4921d"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Load and prep dataset\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "# uncomment middle messages for 1-shot prompting\n",
    "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore\n",
    "    data = data.map(lambda x: { # type: ignore\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': x['question']}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    }) # type: ignore\n",
    "    return data # type: ignore\n",
    "\n",
    "dataset = get_gsm8k_questions()\n",
    "\n",
    "# Reward functions\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "def int_reward_func(completions, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def count_xml(text) -> float:\n",
    "    count = 0.0\n",
    "    if text.count(\"<reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
    "    return count\n",
    "\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJWcHJyYDl4w"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "\n",
    "Now set up GRPO Trainer and all configurations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2helO1j2Dl4x"
   },
   "source": [
    "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
    "\n",
    "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
    "\n",
    "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
    "|------|---------------|-----------|------------|-------------------|----------|\n",
    "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
    "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
    "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for GRPO training configuration\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# CRITICAL FIX: Adjust sequence lengths to be compatible\n",
    "# Your max_seq_length from model loading was 128, but max_prompt_length was 256\n",
    "# This creates negative max_completion_length = 128 - 256 = -128\n",
    "\n",
    "# Solution 1: Reduce prompt length to fit within model's max_seq_length\n",
    "max_prompt_length = 64  # Reduced from 256 to fit in 128 total\n",
    "max_completion_length = 64  # 128 - 64 = 64 tokens for completion\n",
    "\n",
    "print(f\"Max sequence length: {max_seq_length}\")\n",
    "print(f\"Max prompt length: {max_prompt_length}\")  \n",
    "print(f\"Max completion length: {max_completion_length}\")\n",
    "\n",
    "# Verify the calculation\n",
    "assert max_prompt_length + max_completion_length <= max_seq_length, \\\n",
    "    f\"Prompt ({max_prompt_length}) + Completion ({max_completion_length}) = {max_prompt_length + max_completion_length} exceeds max_seq_length ({max_seq_length})\"\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 2,  # Increased for better gradients with small batch\n",
    "    num_generations = 4,  # Reduced from 6 for RTX 2070 Super memory\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_completion_length,\n",
    "    max_steps = 10,\n",
    "    save_steps = 10,\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"none\",\n",
    "    output_dir = \"outputs\",\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        xmlcount_reward_func,\n",
    "        soft_format_reward_func,\n",
    "        strict_format_reward_func,\n",
    "        int_reward_func,\n",
    "        correctness_reward_func,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"üöÄ Starting GRPO training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Alternative Solution: Increase model sequence length\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "If you need longer prompts (256 tokens), you should reload your model with a larger max_seq_length:\n",
    "\n",
    "# When loading your model initially, use:\n",
    "max_seq_length = 512  # Instead of 128\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\", \n",
    "    max_seq_length = 512,  # Increased to accommodate longer sequences\n",
    "    load_in_4bit = True,\n",
    "    fast_inference = False,\n",
    "    # ... other parameters\n",
    ")\n",
    "\n",
    "# Then you can use:\n",
    "max_prompt_length = 256\n",
    "max_completion_length = 256  # 512 - 256 = 256\n",
    "\n",
    "WARNING: Longer sequences use more GPU memory. For RTX 2070 Super:\n",
    "- 128 tokens: ‚úÖ Should work\n",
    "- 256 tokens: ‚ö†Ô∏è  Might work with small batch size  \n",
    "- 512 tokens: ‚ùå Likely too much memory\n",
    "\n",
    "Test with max_seq_length = 256 first, then 512 if you have enough memory.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FimflnG2Dl4y"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XeW54d7Dl4y"
   },
   "source": [
    "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed inference code for Unsloth (without vLLM)\n",
    "import torch\n",
    "\n",
    "# Prepare the input text\n",
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\": \"user\", \"content\": \"Calculate pi.\"},\n",
    "], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(\"Input text:\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Solution 1: Standard Unsloth inference (recommended)\n",
    "print(\"üöÄ Method 1: Standard Unsloth inference\")\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Enable inference mode for better performance\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Generate with standard parameters\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.8,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "# Decode the response\n",
    "full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# Extract only the generated part (remove the input prompt)\n",
    "generated_text = full_output[len(text):]\n",
    "\n",
    "print(\"Generated response:\")\n",
    "print(generated_text)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üöÄ Method 2: Using TextStreamer for real-time output\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "print(\"Streaming response:\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.8,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "        streamer=text_streamer,  # This will print tokens as they're generated\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üöÄ Method 3: Batch inference for multiple prompts\")\n",
    "\n",
    "# Multiple prompts\n",
    "prompts = [\n",
    "    \"Calculate pi.\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain machine learning in simple terms.\"\n",
    "]\n",
    "\n",
    "# Format all prompts\n",
    "formatted_prompts = []\n",
    "for prompt in prompts:\n",
    "    formatted_text = tokenizer.apply_chat_template([\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ], tokenize=False, add_generation_prompt=True)\n",
    "    formatted_prompts.append(formatted_text)\n",
    "\n",
    "# Batch tokenization\n",
    "batch_inputs = tokenizer(formatted_prompts, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "\n",
    "# Generate for all prompts\n",
    "with torch.no_grad():\n",
    "    batch_outputs = model.generate(\n",
    "        **batch_inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.8,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "# Decode all responses\n",
    "print(\"Batch responses:\")\n",
    "for i, (prompt, output) in enumerate(zip(formatted_prompts, batch_outputs)):\n",
    "    full_response = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    generated_response = full_response[len(prompt):]\n",
    "    print(f\"\\nPrompt {i+1}: {prompts[i]}\")\n",
    "    print(f\"Response: {generated_response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Alternative: If you want vLLM-style inference\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "To use vLLM-style inference (model.fast_generate), you need to:\n",
    "\n",
    "1. Load your model WITH vLLM enabled:\n",
    "   fast_inference = True  # Enable this when loading the model\n",
    "   \n",
    "2. Make sure you have enough GPU memory for vLLM compilation\n",
    "\n",
    "3. Then you can use:\n",
    "   \n",
    "   from vllm import SamplingParams\n",
    "   \n",
    "   sampling_params = SamplingParams(\n",
    "       temperature=0.8,\n",
    "       top_p=0.95,\n",
    "       max_tokens=1024,\n",
    "   )\n",
    "   \n",
    "   output = model.fast_generate(\n",
    "       [text],\n",
    "       sampling_params=sampling_params,\n",
    "       lora_request=None,\n",
    "   )[0].outputs[0].text\n",
    "   \n",
    "But since you disabled vLLM due to memory constraints, \n",
    "use the standard inference methods above instead.\n",
    "\"\"\")\n",
    "\n",
    "# Helper function for easy inference\n",
    "def generate_response(prompt, max_tokens=1024, temperature=0.8, top_p=0.95):\n",
    "    \"\"\"Helper function for easy inference\"\"\"\n",
    "    # Format the prompt\n",
    "    formatted_text = tokenizer.apply_chat_template([\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ], tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    \n",
    "    # Extract generated text\n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return full_output[len(formatted_text):]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéØ Easy-to-use helper function:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test the helper function\n",
    "response = generate_response(\"Calculate pi.\", max_tokens=512)\n",
    "print(\"Helper function response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42hOFF6aDl4y"
   },
   "outputs": [],
   "source": [
    "# model.save_lora(\"grpo_saved_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Method 1: Save LoRA adapters only (recommended - smallest size)\n",
    "print(\"üéØ Method 1: Save LoRA adapters (recommended)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "save_dir = \"grpo_saved_lora\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # This saves only the LoRA adapters (very small file size)\n",
    "    model.save_lora(save_dir)\n",
    "    print(f\"‚úÖ LoRA adapters saved successfully to: {save_dir}\")\n",
    "    print(f\"üìÅ Files saved:\")\n",
    "    for file in os.listdir(save_dir):\n",
    "        file_path = os.path.join(save_dir, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            print(f\"   - {file} ({size_mb:.1f} MB)\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving LoRA: {e}\")\n",
    "    print(\"Trying alternative methods...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéØ Method 2: Save with tokenizer\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save LoRA + tokenizer together\n",
    "full_save_dir = \"grpo_model_full\"\n",
    "os.makedirs(full_save_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Save LoRA adapters\n",
    "    model.save_lora(full_save_dir)\n",
    "    \n",
    "    # Save tokenizer separately\n",
    "    tokenizer.save_pretrained(full_save_dir)\n",
    "    \n",
    "    print(f\"‚úÖ Model and tokenizer saved to: {full_save_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéØ Method 3: Save to Hugging Face format\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "hf_save_dir = \"grpo_hf_format\"\n",
    "os.makedirs(hf_save_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Save in standard Hugging Face format\n",
    "    model.save_pretrained(hf_save_dir)\n",
    "    tokenizer.save_pretrained(hf_save_dir)\n",
    "    \n",
    "    print(f\"‚úÖ Hugging Face format saved to: {hf_save_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error with HF format: {e}\")\n",
    "    \n",
    "    # Fallback: Use Unsloth's push method for local save\n",
    "    try:\n",
    "        model.push_to_hub_merged(\n",
    "            save_directory=hf_save_dir,\n",
    "            tokenizer=tokenizer,\n",
    "            save_method=\"lora\",  # or \"merged_16bit\" for full model\n",
    "        )\n",
    "        print(f\"‚úÖ Saved using Unsloth push method to: {hf_save_dir}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Push method also failed: {e2}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéØ Method 4: Manual LoRA saving (if built-in methods fail)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "manual_save_dir = \"grpo_manual_save\"\n",
    "os.makedirs(manual_save_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Get the PEFT model state dict\n",
    "    if hasattr(model, 'peft_config'):\n",
    "        # Save PEFT config\n",
    "        import json\n",
    "        config_dict = {}\n",
    "        for key, config in model.peft_config.items():\n",
    "            config_dict[key] = config.to_dict()\n",
    "        \n",
    "        with open(os.path.join(manual_save_dir, \"adapter_config.json\"), \"w\") as f:\n",
    "            json.dump(config_dict, f, indent=2)\n",
    "        \n",
    "        # Save adapter weights\n",
    "        model.save_pretrained(manual_save_dir)\n",
    "        tokenizer.save_pretrained(manual_save_dir)\n",
    "        \n",
    "        print(f\"‚úÖ Manual save successful to: {manual_save_dir}\")\n",
    "    else:\n",
    "        print(\"‚ùå Model doesn't appear to have PEFT adapters\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Manual save failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéØ Loading saved model back\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Example of how to load the saved LoRA model\n",
    "load_example = \"\"\"\n",
    "# To load your saved LoRA model later:\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# 1. Load base model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    max_seq_length = 128,  # Same as training\n",
    "    load_in_4bit = True,\n",
    "    fast_inference = False,  # Same as training\n",
    ")\n",
    "\n",
    "# 2. Load your trained LoRA adapters\n",
    "model.load_lora(\"grpo_saved_lora\")  # Path to your saved LoRA\n",
    "\n",
    "# 3. Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Now you can use the model for inference with your trained adapters!\n",
    "\"\"\"\n",
    "\n",
    "print(load_example)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéØ File size comparison\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def get_dir_size(directory):\n",
    "    \"\"\"Calculate directory size in MB\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        return 0\n",
    "    \n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            total_size += os.path.getsize(filepath)\n",
    "    return total_size / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "directories_to_check = [save_dir, full_save_dir, hf_save_dir, manual_save_dir]\n",
    "\n",
    "print(\"Saved model sizes:\")\n",
    "for directory in directories_to_check:\n",
    "    if os.path.exists(directory):\n",
    "        size = get_dir_size(directory)\n",
    "        print(f\"üìÅ {directory}: {size:.1f} MB\")\n",
    "\n",
    "print(\"\\nüéâ Model saving complete!\")\n",
    "print(\"\\nRecommended approach:\")\n",
    "print(\"- Use Method 1 (model.save_lora) for smallest files\")  \n",
    "print(\"- Save tokenizer separately if needed\")\n",
    "print(\"- LoRA adapters are typically only 10-50 MB vs full model 13+ GB\")\n",
    "\n",
    "# Cleanup GPU memory after saving\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"\\nüßπ GPU memory cleared. Current usage: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMHsspQsDl4y"
   },
   "source": [
    "Now we load the LoRA and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Clear GPU memory first\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"üîÑ Loading saved LoRA model...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Step 1: Load the base model (same configuration as training)\n",
    "max_seq_length = 128  # Same as used during training\n",
    "lora_rank = 16  # Same as used during training\n",
    "\n",
    "try:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",  # Same base model\n",
    "        max_seq_length = max_seq_length,\n",
    "        load_in_4bit = True,\n",
    "        fast_inference = False,  # Same as training\n",
    "        max_lora_rank = lora_rank,\n",
    "        dtype = torch.float16,\n",
    "    )\n",
    "    print(\"‚úÖ Base model loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load base model: {e}\")\n",
    "    print(\"Trying smaller model...\")\n",
    "    \n",
    "    # Fallback to smaller model if the 7B doesn't fit\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit\",\n",
    "        max_seq_length = max_seq_length,\n",
    "        load_in_4bit = True,\n",
    "        fast_inference = False,\n",
    "        max_lora_rank = 32,\n",
    "        dtype = torch.float16,\n",
    "    )\n",
    "    print(\"‚úÖ Smaller model loaded as fallback!\")\n",
    "\n",
    "# Step 2: Load your trained LoRA adapters\n",
    "print(\"\\nüéØ Loading LoRA adapters...\")\n",
    "\n",
    "try:\n",
    "    # Method 1: Direct LoRA loading (if the model supports it)\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r = lora_rank,\n",
    "        target_modules = [\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        ],\n",
    "        lora_alpha = lora_rank,\n",
    "        use_gradient_checkpointing = \"unsloth\",\n",
    "        random_state = 3407,\n",
    "    )\n",
    "    \n",
    "    # Load the saved LoRA weights\n",
    "    model.load_adapter(\"grpo_saved_lora\", adapter_name=\"default\")\n",
    "    print(\"‚úÖ LoRA adapters loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Method 1 failed: {e}\")\n",
    "    print(\"Trying alternative loading method...\")\n",
    "    \n",
    "    try:\n",
    "        # Method 2: Load using from_pretrained with adapter\n",
    "        from peft import PeftModel\n",
    "        model = PeftModel.from_pretrained(model, \"grpo_saved_lora\")\n",
    "        print(\"‚úÖ LoRA loaded using PEFT!\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Method 2 also failed: {e2}\")\n",
    "        print(\"‚ö†Ô∏è  Using base model without LoRA adapters for testing...\")\n",
    "\n",
    "# Step 3: Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "print(\"üöÄ Model ready for inference!\")\n",
    "\n",
    "# Step 4: Set up system prompt and test\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant. Provide clear, accurate, and concise responses.\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üß™ Testing the loaded model\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def test_model(prompt, system_prompt=SYSTEM_PROMPT, max_tokens=512):\n",
    "    \"\"\"Test function for the loaded model\"\"\"\n",
    "    \n",
    "    # Format the conversation\n",
    "    if system_prompt:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    else:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Input prompt: {prompt}\")\n",
    "    print(f\"Formatted text length: {len(text)} characters\")\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    print(f\"Input tokens: {inputs['input_ids'].shape[1]}\")\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.8,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    \n",
    "    # Decode the full output\n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the generated part\n",
    "    generated_response = full_output[len(text):].strip()\n",
    "    \n",
    "    print(f\"ü§ñ Response: {generated_response}\")\n",
    "    print(f\"Response length: {len(generated_response)} characters\")\n",
    "    \n",
    "    return generated_response\n",
    "\n",
    "# Test 1: Original test case\n",
    "print(\"\\nüìù Test 1: Calculate pi\")\n",
    "print(\"-\" * 30)\n",
    "response1 = test_model(\"Calculate pi.\")\n",
    "\n",
    "# Test 2: Different prompt to see if training worked\n",
    "print(\"\\nüìù Test 2: Math reasoning\")\n",
    "print(\"-\" * 30)\n",
    "response2 = test_model(\"What is 15 * 23 + 7?\")\n",
    "\n",
    "# Test 3: Check if model follows any special formatting from GRPO training\n",
    "print(\"\\nüìù Test 3: Complex reasoning\")\n",
    "print(\"-\" * 30)\n",
    "response3 = test_model(\"Explain the process of photosynthesis in simple terms.\")\n",
    "\n",
    "# Test 4: Batch inference\n",
    "print(\"\\nüìù Test 4: Batch inference\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "test_prompts = [\n",
    "    \"What is the capital of Japan?\",\n",
    "    \"How does machine learning work?\",\n",
    "    \"Write a short poem about stars.\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nBatch test {i}: {prompt}\")\n",
    "    response = test_model(prompt, max_tokens=256)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéâ Model testing complete!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Memory usage info\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    print(f\"GPU Memory - Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "\n",
    "print(\"\\nüîç Model Analysis:\")\n",
    "print(f\"- Model type: {type(model).__name__}\")\n",
    "print(f\"- Has PEFT adapters: {hasattr(model, 'peft_config')}\")\n",
    "if hasattr(model, 'peft_config'):\n",
    "    print(f\"- Active adapters: {list(model.peft_config.keys())}\")\n",
    "print(f\"- Device: {next(model.parameters()).device}\")\n",
    "print(f\"- Tokenizer vocab size: {len(tokenizer)}\")\n",
    "\n",
    "print(\"\\nüí° Notes:\")\n",
    "print(\"- If responses look different from base model, LoRA training worked!\")\n",
    "print(\"- If responses are similar to base model, LoRA might not have loaded\")\n",
    "print(\"- Check for any special formatting patterns from your GRPO reward functions\")\n",
    "print(\"- Compare responses to base model (without LoRA) to see the difference\")\n",
    "\n",
    "# Optional: Save a test log\n",
    "test_log = f\"\"\"\n",
    "Model Test Results\n",
    "==================\n",
    "Model: Mistral-7B with LoRA adapters\n",
    "Test Date: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\n",
    "\n",
    "Test 1 - Calculate pi: {response1[:100]}...\n",
    "Test 2 - Math: {response2[:100]}...\n",
    "Test 3 - Science: {response3[:100]}...\n",
    "\n",
    "Memory Usage: {allocated:.2f} GB GPU\n",
    "\"\"\"\n",
    "\n",
    "with open(\"model_test_results.txt\", \"w\") as f:\n",
    "    f.write(test_log)\n",
    "\n",
    "print(\"\\nüìÑ Test results saved to 'model_test_results.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "print(\"üöÄ Saving Model for vLLM (Float16)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Clear GPU memory before saving\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check available space (merged models are large!)\n",
    "def check_disk_space():\n",
    "    import shutil\n",
    "    total, used, free = shutil.disk_usage(\".\")\n",
    "    free_gb = free / (1024**3)\n",
    "    print(f\"üíæ Available disk space: {free_gb:.1f} GB\")\n",
    "    if free_gb < 15:  # Mistral-7B merged is ~13GB\n",
    "        print(\"‚ö†Ô∏è  WARNING: Low disk space! Merged model needs ~13GB\")\n",
    "    return free_gb\n",
    "\n",
    "free_space = check_disk_space()\n",
    "\n",
    "print(\"\\nüéØ Option 1: Save merged model in float16 (RECOMMENDED for vLLM)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Merge LoRA into base model and save in float16 - BEST for vLLM\n",
    "if True:  # Changed from False to True\n",
    "    try:\n",
    "        save_path = \"model_merged_16bit\"\n",
    "        print(f\"Saving merged float16 model to: {save_path}\")\n",
    "        print(\"This will take a few minutes...\")\n",
    "        \n",
    "        model.save_pretrained_merged(\n",
    "            save_path, \n",
    "            tokenizer, \n",
    "            save_method=\"merged_16bit\",  # Float16 format - perfect for vLLM\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Float16 merged model saved successfully!\")\n",
    "        \n",
    "        # Check saved model size\n",
    "        def get_dir_size(directory):\n",
    "            total_size = 0\n",
    "            for dirpath, dirnames, filenames in os.walk(directory):\n",
    "                for filename in filenames:\n",
    "                    filepath = os.path.join(dirpath, filename)\n",
    "                    if os.path.isfile(filepath):\n",
    "                        total_size += os.path.getsize(filepath)\n",
    "            return total_size / (1024**3)  # GB\n",
    "        \n",
    "        size_gb = get_dir_size(save_path)\n",
    "        print(f\"üìÅ Model size: {size_gb:.1f} GB\")\n",
    "        \n",
    "        # List saved files\n",
    "        print(\"üìÑ Saved files:\")\n",
    "        for file in os.listdir(save_path):\n",
    "            file_path = os.path.join(save_path, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "                print(f\"   - {file} ({size_mb:.1f} MB)\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving merged float16 model: {e}\")\n",
    "        print(\"Possible causes:\")\n",
    "        print(\"- Insufficient disk space\")\n",
    "        print(\"- Insufficient GPU memory\") \n",
    "        print(\"- Model not properly trained\")\n",
    "\n",
    "print(\"\\nüéØ Option 2: Push merged model to Hugging Face Hub\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Push to Hugging Face Hub in float16\n",
    "if False:  # Set to True and add your token to enable\n",
    "    try:\n",
    "        hf_repo_name = \"your-username/your-model-name\"  # Change this!\n",
    "        hf_token = \"your_hf_token_here\"  # Add your HF token!\n",
    "        \n",
    "        print(f\"Pushing to Hugging Face: {hf_repo_name}\")\n",
    "        \n",
    "        model.push_to_hub_merged(\n",
    "            hf_repo_name, \n",
    "            tokenizer, \n",
    "            save_method=\"merged_16bit\", \n",
    "            token=hf_token\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Model pushed to Hugging Face Hub!\")\n",
    "        print(f\"üåê Available at: https://huggingface.co/{hf_repo_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error pushing to hub: {e}\")\n",
    "\n",
    "print(\"\\nüéØ Option 3: Save merged model in 4bit (smaller but less compatible)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Merge to 4bit (smaller file, but may not work with all vLLM versions)\n",
    "if False:  # Set to True if you want 4bit version too\n",
    "    try:\n",
    "        save_path_4bit = \"model_merged_4bit\"\n",
    "        print(f\"Saving merged 4bit model to: {save_path_4bit}\")\n",
    "        \n",
    "        model.save_pretrained_merged(\n",
    "            save_path_4bit, \n",
    "            tokenizer, \n",
    "            save_method=\"merged_4bit\",\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ 4bit merged model saved!\")\n",
    "        size_gb = get_dir_size(save_path_4bit)\n",
    "        print(f\"üìÅ 4bit model size: {size_gb:.1f} GB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving 4bit model: {e}\")\n",
    "\n",
    "print(\"\\nüéØ Option 4: Save just LoRA adapters (smallest, for loading later)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Just save LoRA adapters (what you had before)\n",
    "if True:  # This is your existing LoRA save\n",
    "    try:\n",
    "        lora_path = \"model_lora_only\"\n",
    "        \n",
    "        model.save_pretrained(lora_path)\n",
    "        tokenizer.save_pretrained(lora_path)\n",
    "        \n",
    "        print(\"‚úÖ LoRA adapters saved!\")\n",
    "        size_mb = get_dir_size(lora_path) * 1024  # Convert to MB\n",
    "        print(f\"üìÅ LoRA size: {size_mb:.1f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving LoRA: {e}\")\n",
    "\n",
    "print(\"\\nüéØ Option 5: Push LoRA to Hugging Face Hub\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Push just LoRA adapters to hub\n",
    "if False:  # Set to True and configure to enable\n",
    "    try:\n",
    "        hf_lora_repo = \"your-username/your-lora-adapters\"  # Change this!\n",
    "        hf_token = \"your_hf_token_here\"  # Add your HF token!\n",
    "        \n",
    "        model.push_to_hub(hf_lora_repo, token=hf_token)\n",
    "        tokenizer.push_to_hub(hf_lora_repo, token=hf_token)\n",
    "        \n",
    "        print(\"‚úÖ LoRA adapters pushed to Hugging Face Hub!\")\n",
    "        print(f\"üåê Available at: https://huggingface.co/{hf_lora_repo}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error pushing LoRA to hub: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ How to use your saved models\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "vllm_usage_guide = '''\n",
    "# For vLLM inference with your float16 merged model:\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Load your merged model with vLLM\n",
    "llm = LLM(\n",
    "    model=\"./model_merged_16bit\",  # Path to your saved model\n",
    "    gpu_memory_utilization=0.8,\n",
    "    max_model_len=2048,  # Adjust based on your needs\n",
    "    dtype=\"float16\"  # Explicitly use float16\n",
    ")\n",
    "\n",
    "# Set up sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_tokens=512,\n",
    ")\n",
    "\n",
    "# Format your prompt\n",
    "prompt = tokenizer.apply_chat_template([\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Calculate pi.\"}\n",
    "], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Generate\n",
    "outputs = llm.generate([prompt], sampling_params)\n",
    "response = outputs[0].outputs[0].text\n",
    "\n",
    "print(response)\n",
    "'''\n",
    "\n",
    "print(vllm_usage_guide)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìã Summary of what was saved:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "saved_models = []\n",
    "\n",
    "if os.path.exists(\"model_merged_16bit\"):\n",
    "    size = get_dir_size(\"model_merged_16bit\")\n",
    "    saved_models.append(f\"‚úÖ Float16 merged model: model_merged_16bit ({size:.1f} GB) - BEST for vLLM\")\n",
    "\n",
    "if os.path.exists(\"model_merged_4bit\"):\n",
    "    size = get_dir_size(\"model_merged_4bit\") \n",
    "    saved_models.append(f\"‚úÖ 4bit merged model: model_merged_4bit ({size:.1f} GB)\")\n",
    "\n",
    "if os.path.exists(\"model_lora_only\"):\n",
    "    size = get_dir_size(\"model_lora_only\") * 1024  # MB\n",
    "    saved_models.append(f\"‚úÖ LoRA adapters only: model_lora_only ({size:.1f} MB)\")\n",
    "\n",
    "if saved_models:\n",
    "    for model_info in saved_models:\n",
    "        print(model_info)\n",
    "else:\n",
    "    print(\"‚ùå No models were saved successfully\")\n",
    "\n",
    "print(f\"\\nüíæ Total disk space used: ~{sum([get_dir_size(d) for d in ['model_merged_16bit', 'model_merged_4bit'] if os.path.exists(d)]):.1f} GB\")\n",
    "\n",
    "print(\"\\nüéØ Recommendations:\")\n",
    "print(\"- Use 'model_merged_16bit' for vLLM inference (best compatibility)\")  \n",
    "print(\"- Keep 'model_lora_only' as backup (small size)\")\n",
    "print(\"- Test vLLM loading before deleting any files\")\n",
    "print(\"- Float16 merged model will work with vLLM out of the box!\")\n",
    "\n",
    "# Clear GPU memory after saving\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"\\nüßπ GPU memory cleared. Available: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0)) / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCv4vXHd61i7"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "print(\"ü¶ô GGUF / llama.cpp Model Conversion\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clear GPU memory before conversion\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check available disk space (GGUF files can be large)\n",
    "def check_disk_space():\n",
    "    import shutil\n",
    "    total, used, free = shutil.disk_usage(\".\")\n",
    "    free_gb = free / (1024**3)\n",
    "    print(f\"üíæ Available disk space: {free_gb:.1f} GB\")\n",
    "    \n",
    "    # Estimate space needed for different formats\n",
    "    print(\"üìä Estimated space requirements:\")\n",
    "    print(\"   - Q8_0 (8-bit): ~7-8 GB\")\n",
    "    print(\"   - F16 (16-bit): ~13-14 GB\") \n",
    "    print(\"   - Q4_K_M (4-bit): ~4-5 GB\")\n",
    "    print(\"   - All formats: ~25-30 GB total\")\n",
    "    \n",
    "    if free_gb < 30:\n",
    "        print(\"‚ö†Ô∏è  WARNING: Consider saving formats one at a time if space is limited\")\n",
    "    \n",
    "    return free_gb\n",
    "\n",
    "free_space = check_disk_space()\n",
    "\n",
    "def get_file_size(filepath):\n",
    "    \"\"\"Get file size in GB\"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        size_bytes = os.path.getsize(filepath)\n",
    "        return size_bytes / (1024**3)\n",
    "    return 0\n",
    "\n",
    "def save_gguf_with_timing(save_func, format_name, estimated_time=\"5-10 minutes\"):\n",
    "    \"\"\"Helper to save GGUF with timing and error handling\"\"\"\n",
    "    print(f\"\\nüîÑ Converting to {format_name}...\")\n",
    "    print(f\"‚è±Ô∏è  Estimated time: {estimated_time}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        save_func()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"‚úÖ {format_name} conversion completed in {elapsed/60:.1f} minutes!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"‚ùå {format_name} conversion failed after {elapsed/60:.1f} minutes\")\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ GGUF Conversion Options\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Conversion results tracking\n",
    "conversion_results = {}\n",
    "\n",
    "# Option 1: Save to 8bit Q8_0 (High quality, medium size)\n",
    "print(\"\\nüì¶ Option 1: Q8_0 (8-bit quantization)\")\n",
    "print(\"-\" * 40)\n",
    "print(\"‚ú® Features: High quality, good for most use cases\")\n",
    "print(\"üìÅ Size: ~7-8 GB\")\n",
    "print(\"üéØ Best for: General purpose, balanced quality/size\")\n",
    "\n",
    "if True:  # Changed from False to True\n",
    "    def save_q8_0():\n",
    "        model.save_pretrained_gguf(\n",
    "            \"model_q8_0\", \n",
    "            tokenizer,\n",
    "            # quantization_method defaults to \"q8_0\"\n",
    "        )\n",
    "    \n",
    "    success = save_gguf_with_timing(save_q8_0, \"Q8_0\", \"5-8 minutes\")\n",
    "    conversion_results[\"Q8_0\"] = success\n",
    "    \n",
    "    if success:\n",
    "        size = get_file_size(\"model_q8_0/model-unsloth.Q8_0.gguf\")\n",
    "        print(f\"üìÅ Q8_0 file size: {size:.1f} GB\")\n",
    "\n",
    "# Option 2: Save to 16bit GGUF (Highest quality, largest size)  \n",
    "print(\"\\nüì¶ Option 2: F16 (16-bit full precision)\")\n",
    "print(\"-\" * 40)\n",
    "print(\"‚ú® Features: Highest quality, no quantization loss\")\n",
    "print(\"üìÅ Size: ~13-14 GB\")\n",
    "print(\"üéØ Best for: Maximum quality, research, reference\")\n",
    "\n",
    "if True:  # Changed from False to True\n",
    "    def save_f16():\n",
    "        model.save_pretrained_gguf(\n",
    "            \"model_f16\", \n",
    "            tokenizer, \n",
    "            quantization_method=\"f16\"\n",
    "        )\n",
    "    \n",
    "    success = save_gguf_with_timing(save_f16, \"F16\", \"8-12 minutes\")\n",
    "    conversion_results[\"F16\"] = success\n",
    "    \n",
    "    if success:\n",
    "        size = get_file_size(\"model_f16/model-unsloth.F16.gguf\")\n",
    "        print(f\"üìÅ F16 file size: {size:.1f} GB\")\n",
    "\n",
    "# Option 3: Save to q4_k_m GGUF (Good quality, smallest size)\n",
    "print(\"\\nüì¶ Option 3: Q4_K_M (4-bit quantization)\")\n",
    "print(\"-\" * 40)\n",
    "print(\"‚ú® Features: Good quality, smallest size, fastest inference\")\n",
    "print(\"üìÅ Size: ~4-5 GB\") \n",
    "print(\"üéØ Best for: Resource-constrained devices, mobile deployment\")\n",
    "\n",
    "if True:  # Changed from False to True\n",
    "    def save_q4_k_m():\n",
    "        model.save_pretrained_gguf(\n",
    "            \"model_q4_k_m\", \n",
    "            tokenizer, \n",
    "            quantization_method=\"q4_k_m\"\n",
    "        )\n",
    "    \n",
    "    success = save_gguf_with_timing(save_q4_k_m, \"Q4_K_M\", \"3-6 minutes\")\n",
    "    conversion_results[\"Q4_K_M\"] = success\n",
    "    \n",
    "    if success:\n",
    "        size = get_file_size(\"model_q4_k_m/model-unsloth.Q4_K_M.gguf\")\n",
    "        print(f\"üìÅ Q4_K_M file size: {size:.1f} GB\")\n",
    "\n",
    "# Option 4: Additional quantization formats (advanced)\n",
    "print(\"\\nüì¶ Option 4: Additional GGUF formats\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "additional_formats = {\n",
    "    \"q5_k_m\": {\"desc\": \"5-bit, balanced quality/size\", \"size\": \"~5-6 GB\"},\n",
    "    \"q6_k\": {\"desc\": \"6-bit, high quality\", \"size\": \"~6-7 GB\"}, \n",
    "    \"q4_0\": {\"desc\": \"4-bit, fastest inference\", \"size\": \"~4 GB\"},\n",
    "    \"q5_0\": {\"desc\": \"5-bit, good balance\", \"size\": \"~5 GB\"},\n",
    "}\n",
    "\n",
    "save_additional = False  # Set to True to enable additional formats\n",
    "\n",
    "if save_additional:\n",
    "    for fmt, info in additional_formats.items():\n",
    "        print(f\"\\nüîÑ Converting to {fmt.upper()} ({info['desc']}, {info['size']})\")\n",
    "        \n",
    "        def save_additional_format():\n",
    "            model.save_pretrained_gguf(\n",
    "                f\"model_{fmt}\", \n",
    "                tokenizer, \n",
    "                quantization_method=fmt\n",
    "            )\n",
    "        \n",
    "        success = save_gguf_with_timing(save_additional_format, fmt.upper(), \"3-8 minutes\")\n",
    "        conversion_results[fmt.upper()] = success\n",
    "\n",
    "# Option 5: Push to Hugging Face Hub\n",
    "print(\"\\nüåê Hugging Face Hub Upload\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "upload_to_hub = False  # Set to True and configure to enable uploads\n",
    "\n",
    "if upload_to_hub:\n",
    "    hf_token = \"your_hf_token_here\"  # Replace with your token\n",
    "    hf_repo_base = \"your-username/your-model-name\"  # Replace with your repo\n",
    "    \n",
    "    # Upload Q8_0 to hub\n",
    "    if conversion_results.get(\"Q8_0\", False):\n",
    "        try:\n",
    "            print(\"üöÄ Uploading Q8_0 to Hugging Face Hub...\")\n",
    "            model.push_to_hub_gguf(\n",
    "                f\"{hf_repo_base}-Q8-GGUF\", \n",
    "                tokenizer, \n",
    "                token=hf_token\n",
    "            )\n",
    "            print(f\"‚úÖ Q8_0 uploaded to: https://huggingface.co/{hf_repo_base}-Q8-GGUF\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Q8_0 upload failed: {e}\")\n",
    "    \n",
    "    # Upload F16 to hub\n",
    "    if conversion_results.get(\"F16\", False):\n",
    "        try:\n",
    "            print(\"üöÄ Uploading F16 to Hugging Face Hub...\")\n",
    "            model.push_to_hub_gguf(\n",
    "                f\"{hf_repo_base}-F16-GGUF\", \n",
    "                tokenizer, \n",
    "                quantization_method=\"f16\", \n",
    "                token=hf_token\n",
    "            )\n",
    "            print(f\"‚úÖ F16 uploaded to: https://huggingface.co/{hf_repo_base}-F16-GGUF\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå F16 upload failed: {e}\")\n",
    "    \n",
    "    # Upload Q4_K_M to hub  \n",
    "    if conversion_results.get(\"Q4_K_M\", False):\n",
    "        try:\n",
    "            print(\"üöÄ Uploading Q4_K_M to Hugging Face Hub...\")\n",
    "            model.push_to_hub_gguf(\n",
    "                f\"{hf_repo_base}-Q4-GGUF\", \n",
    "                tokenizer, \n",
    "                quantization_method=\"q4_k_m\", \n",
    "                token=hf_token\n",
    "            )\n",
    "            print(f\"‚úÖ Q4_K_M uploaded to: https://huggingface.co/{hf_repo_base}-Q4-GGUF\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Q4_K_M upload failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã CONVERSION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_size = 0\n",
    "successful_conversions = []\n",
    "\n",
    "for format_name, success in conversion_results.items():\n",
    "    if success:\n",
    "        # Find the corresponding directory and file\n",
    "        format_dirs = {\n",
    "            \"Q8_0\": \"model_q8_0\",\n",
    "            \"F16\": \"model_f16\", \n",
    "            \"Q4_K_M\": \"model_q4_k_m\"\n",
    "        }\n",
    "        \n",
    "        if format_name in format_dirs:\n",
    "            dir_path = format_dirs[format_name]\n",
    "            if os.path.exists(dir_path):\n",
    "                # Calculate directory size\n",
    "                dir_size = 0\n",
    "                for dirpath, dirnames, filenames in os.walk(dir_path):\n",
    "                    for filename in filenames:\n",
    "                        filepath = os.path.join(dirpath, filename)\n",
    "                        dir_size += os.path.getsize(filepath)\n",
    "                \n",
    "                dir_size_gb = dir_size / (1024**3)\n",
    "                total_size += dir_size_gb\n",
    "                \n",
    "                print(f\"‚úÖ {format_name}: {dir_path}/ ({dir_size_gb:.1f} GB)\")\n",
    "                successful_conversions.append(format_name)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  {format_name}: Conversion reported success but directory not found\")\n",
    "    else:\n",
    "        print(f\"‚ùå {format_name}: Conversion failed\")\n",
    "\n",
    "print(f\"\\nüìä Total disk space used: {total_size:.1f} GB\")\n",
    "print(f\"üéØ Successful conversions: {len(successful_conversions)}/{len(conversion_results)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ü¶ô HOW TO USE WITH LLAMA.CPP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "llama_cpp_guide = '''\n",
    "# Install llama.cpp (if not already installed)\n",
    "git clone https://github.com/ggerganov/llama.cpp.git\n",
    "cd llama.cpp\n",
    "make\n",
    "\n",
    "# Run inference with your GGUF models:\n",
    "\n",
    "# Q4_K_M (fastest, smallest)\n",
    "./main -m ./model_q4_k_m/model-unsloth.Q4_K_M.gguf -n 512 -p \"Calculate pi.\"\n",
    "\n",
    "# Q8_0 (balanced)\n",
    "./main -m ./model_q8_0/model-unsloth.Q8_0.gguf -n 512 -p \"Calculate pi.\"\n",
    "\n",
    "# F16 (highest quality)\n",
    "./main -m ./model_f16/model-unsloth.F16.gguf -n 512 -p \"Calculate pi.\"\n",
    "\n",
    "# Interactive chat mode\n",
    "./main -m ./model_q4_k_m/model-unsloth.Q4_K_M.gguf -n 512 -i\n",
    "\n",
    "# With custom system prompt\n",
    "./main -m ./model_q4_k_m/model-unsloth.Q4_K_M.gguf -n 512 \\\\\n",
    "  --system-prompt-file system_prompt.txt -p \"Your question here\"\n",
    "\n",
    "# Python binding (llama-cpp-python)\n",
    "pip install llama-cpp-python\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(model_path=\"./model_q4_k_m/model-unsloth.Q4_K_M.gguf\")\n",
    "output = llm(\"Calculate pi.\", max_tokens=512)\n",
    "print(output['choices'][0]['text'])\n",
    "'''\n",
    "\n",
    "print(llama_cpp_guide)\n",
    "\n",
    "print(\"\\nüéØ FORMAT RECOMMENDATIONS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"ü•á Q4_K_M: Best for most users (good quality, fast, small)\")\n",
    "print(\"ü•à Q8_0: Best for quality-conscious users (high quality, medium size)\")  \n",
    "print(\"ü•â F16: Best for researchers (maximum quality, large size)\")\n",
    "\n",
    "print(f\"\\nüßπ Cleaning up...\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "remaining_space = check_disk_space()\n",
    "print(f\"üíæ Remaining disk space: {remaining_space:.1f} GB\")\n",
    "\n",
    "print(\"\\nüéâ GGUF conversion complete!\")\n",
    "print(\"Your models are ready for llama.cpp inference! ü¶ô\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
