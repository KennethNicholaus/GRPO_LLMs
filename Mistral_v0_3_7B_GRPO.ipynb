{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral-7B v0.3 GRPO Fine-tuning & Model Conversion Summary\n",
    "\n",
    "## Model Setup & GRPO Training\n",
    "• **Load Mistral-7B**: Use `FastLanguageModel.from_pretrained(\"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\")` with RTX 2070 Super settings (`fast_inference=False`, `load_in_4bit=True`, reduced LoRA rank 8-16)\n",
    "• **GRPO Training**: Configure `GRPOConfig` with proper sequence lengths for Mistral (ensure `max_prompt_length + max_completion_length ≤ max_seq_length=128`)\n",
    "• **Reward Functions**: Use multiple reward functions (`xmlcount_reward_func`, `format_reward_func`, `correctness_reward_func`) with `GRPOTrainer`\n",
    "• **Save LoRA**: Use `model.save_lora(\"grpo_saved_lora\")` to save GRPO-trained adapters (~50MB vs 13GB full Mistral model)\n",
    "\n",
    "## Mistral Model Conversion Options\n",
    "• **vLLM (Float16)**: `model.save_pretrained_merged(\"model_merged_16bit\", tokenizer, save_method=\"merged_16bit\")` - Merges GRPO LoRA into Mistral base (~13GB)\n",
    "• **GGUF/llama.cpp**: Use `model.save_pretrained_gguf(\"model_q4_k_m\", tokenizer, quantization_method=\"q4_k_m/q8_0/f16\")` for Mistral GGUF formats (4-14GB)\n",
    "• **LoRA Only**: Keep GRPO adapters separate for loading back into Mistral base model later\n",
    "\n",
    "## Mistral Inference Methods\n",
    "• **Standard**: Use `model.generate()` with Mistral chat template `tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}])`\n",
    "• **vLLM**: Requires merged Mistral model with `fast_inference=True` and `model.fast_generate()` \n",
    "• **llama.cpp**: Use converted Mistral GGUF files with `./main -m mistral-model.gguf -p \"prompt\"`\n",
    "\n",
    "## RTX 2070 Super Optimizations for Mistral-7B\n",
    "• **Memory Management**: Disable vLLM, use 4-bit quantization, LoRA rank ≤16, `max_seq_length=128`, enable CPU offloading if OOM\n",
    "• **Loading GRPO Model**: Load Mistral base first, then apply GRPO LoRA with `model.load_adapter(\"grpo_saved_lora\")` or `PeftModel.from_pretrained()`\n",
    "• **Fallback**: Switch to `unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit` if Mistral-7B doesn't fit in 8GB VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchaudio==2.7.1\n",
      "  Using cached torchaudio-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting torch==2.7.1 (from torchaudio==2.7.1)\n",
      "  Using cached torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting filelock (from torch==2.7.1->torchaudio==2.7.1)\n",
      "  Using cached filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->torchaudio==2.7.1) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch==2.7.1->torchaudio==2.7.1)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.7.1->torchaudio==2.7.1)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->torchaudio==2.7.1) (3.1.6)\n",
      "Collecting fsspec (from torch==2.7.1->torchaudio==2.7.1)\n",
      "  Using cached fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.1->torchaudio==2.7.1)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.1->torchaudio==2.7.1)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.1->torchaudio==2.7.1)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.1->torchaudio==2.7.1)\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.1->torchaudio==2.7.1)\n",
      "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.1->torchaudio==2.7.1)\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.1->torchaudio==2.7.1)\n",
      "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.1->torchaudio==2.7.1)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.1->torchaudio==2.7.1)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.1->torchaudio==2.7.1)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.1->torchaudio==2.7.1)\n",
      "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.1->torchaudio==2.7.1)\n",
      "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.1->torchaudio==2.7.1)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.1->torchaudio==2.7.1)\n",
      "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.1 (from torch==2.7.1->torchaudio==2.7.1)\n",
      "  Using cached triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from triton==3.3.1->torch==2.7.1->torchaudio==2.7.1) (78.1.1)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch==2.7.1->torchaudio==2.7.1)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jinja2->torch==2.7.1->torchaudio==2.7.1) (3.0.2)\n",
      "Using cached torchaudio-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (3.5 MB)\n",
      "Using cached torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n",
      "Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Using cached fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchaudio\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/22\u001b[0m [torchaudio]2\u001b[0m [torchaudio]lver-cu12]2]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed filelock-3.19.1 fsspec-2025.7.0 mpmath-1.3.0 networkx-3.5 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.1 torchaudio-2.7.1 triton-3.3.1\n",
      "Collecting bitsandbytes==0.47.0\n",
      "  Using cached bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from bitsandbytes==0.47.0) (2.7.1)\n",
      "Collecting numpy>=1.17 (from bitsandbytes==0.47.0)\n",
      "  Using cached numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: filelock in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (2025.7.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch<3,>=2.2->bitsandbytes==0.47.0) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from triton==3.3.1->torch<3,>=2.2->bitsandbytes==0.47.0) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes==0.47.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jinja2->torch<3,>=2.2->bitsandbytes==0.47.0) (3.0.2)\n",
      "Using cached bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
      "Using cached numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
      "Installing collected packages: numpy, bitsandbytes\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [bitsandbytes][0m [bitsandbytes]\n",
      "\u001b[1A\u001b[2KSuccessfully installed bitsandbytes-0.47.0 numpy-2.3.2\n",
      "Collecting torchvision==0.22.1\n",
      "  Using cached torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torchvision==0.22.1) (2.3.2)\n",
      "Requirement already satisfied: torch==2.7.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torchvision==0.22.1) (2.7.1)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.22.1)\n",
      "  Using cached pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: filelock in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->torchvision==0.22.1) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->torchvision==0.22.1) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->torchvision==0.22.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->torchvision==0.22.1) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->torchvision==0.22.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->torchvision==0.22.1) (2025.7.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->torchvision==0.22.1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->torchvision==0.22.1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->torchvision==0.22.1) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->torchvision==0.22.1) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->torchvision==0.22.1) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->torchvision==0.22.1) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->torchvision==0.22.1) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->torchvision==0.22.1) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->torchvision==0.22.1) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->torchvision==0.22.1) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->torchvision==0.22.1) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->torchvision==0.22.1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->torchvision==0.22.1) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->torchvision==0.22.1) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->torchvision==0.22.1) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from triton==3.3.1->torch==2.7.1->torchvision==0.22.1) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from sympy>=1.13.3->torch==2.7.1->torchvision==0.22.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jinja2->torch==2.7.1->torchvision==0.22.1) (3.0.2)\n",
      "Using cached torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl (7.5 MB)\n",
      "Using cached pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "Installing collected packages: pillow, torchvision\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [torchvision]\u001b[0m [torchvision]\n",
      "\u001b[1A\u001b[2KSuccessfully installed pillow-11.3.0 torchvision-0.22.1\n",
      "Collecting xformers==0.0.31\n",
      "  Using cached xformers-0.0.31-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: numpy in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from xformers==0.0.31) (2.3.2)\n",
      "Requirement already satisfied: torch==2.7.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from xformers==0.0.31) (2.7.1)\n",
      "Requirement already satisfied: filelock in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->xformers==0.0.31) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->xformers==0.0.31) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->xformers==0.0.31) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->xformers==0.0.31) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->xformers==0.0.31) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->xformers==0.0.31) (2025.7.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->xformers==0.0.31) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->xformers==0.0.31) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->xformers==0.0.31) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->xformers==0.0.31) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->xformers==0.0.31) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->xformers==0.0.31) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->xformers==0.0.31) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->xformers==0.0.31) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->xformers==0.0.31) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->xformers==0.0.31) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->xformers==0.0.31) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->xformers==0.0.31) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->xformers==0.0.31) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->xformers==0.0.31) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->xformers==0.0.31) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from triton==3.3.1->torch==2.7.1->xformers==0.0.31) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from sympy>=1.13.3->torch==2.7.1->xformers==0.0.31) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jinja2->torch==2.7.1->xformers==0.0.31) (3.0.2)\n",
      "Using cached xformers-0.0.31-cp39-abi3-manylinux_2_28_x86_64.whl (117.1 MB)\n",
      "Installing collected packages: xformers\n",
      "Successfully installed xformers-0.0.31\n",
      "Collecting unsloth-zoo\n",
      "  Using cached unsloth_zoo-2025.8.9-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: torch>=2.4.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth-zoo) (2.7.1)\n",
      "Collecting torchao (from unsloth-zoo)\n",
      "  Using cached torchao-0.12.0-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: triton in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth-zoo) (3.3.1)\n",
      "Requirement already satisfied: packaging>=24.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth-zoo) (25.0)\n",
      "Collecting tyro (from unsloth-zoo)\n",
      "  Using cached tyro-0.9.31-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3 (from unsloth-zoo)\n",
      "  Using cached transformers-4.56.0-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting datasets<4.0.0,>=3.4.1 (from unsloth-zoo)\n",
      "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sentencepiece>=0.2.0 (from unsloth-zoo)\n",
      "  Using cached sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Collecting tqdm (from unsloth-zoo)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: psutil in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth-zoo) (5.9.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth-zoo) (0.45.1)\n",
      "Requirement already satisfied: numpy in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth-zoo) (2.3.2)\n",
      "Collecting accelerate>=0.34.1 (from unsloth-zoo)\n",
      "  Using cached accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting trl!=0.15.0,!=0.19.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth-zoo)\n",
      "  Using cached trl-0.22.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting peft!=0.11.0,>=0.7.1 (from unsloth-zoo)\n",
      "  Using cached peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting protobuf (from unsloth-zoo)\n",
      "  Using cached protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting huggingface_hub>=0.34.0 (from unsloth-zoo)\n",
      "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting hf_transfer (from unsloth-zoo)\n",
      "  Using cached hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting cut_cross_entropy (from unsloth-zoo)\n",
      "  Using cached cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pillow in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth-zoo) (11.3.0)\n",
      "Collecting regex (from unsloth-zoo)\n",
      "  Using cached regex-2025.8.29-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting msgspec (from unsloth-zoo)\n",
      "  Using cached msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: typing_extensions in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth-zoo) (4.15.0)\n",
      "Requirement already satisfied: filelock in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth-zoo) (3.19.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets<4.0.0,>=3.4.1->unsloth-zoo)\n",
      "  Using cached pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets<4.0.0,>=3.4.1->unsloth-zoo)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets<4.0.0,>=3.4.1->unsloth-zoo)\n",
      "  Using cached pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth-zoo) (2.32.5)\n",
      "Collecting xxhash (from datasets<4.0.0,>=3.4.1->unsloth-zoo)\n",
      "  Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets<4.0.0,>=3.4.1->unsloth-zoo)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth-zoo)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth-zoo) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth-zoo)\n",
      "  Using cached aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from accelerate>=0.34.1->unsloth-zoo)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth-zoo)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth-zoo)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth-zoo) (24.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth-zoo)\n",
      "  Using cached frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth-zoo)\n",
      "  Using cached multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth-zoo)\n",
      "  Using cached propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth-zoo)\n",
      "  Using cached yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth-zoo) (3.7)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub>=0.34.0->unsloth-zoo)\n",
      "  Using cached hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth-zoo) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth-zoo) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth-zoo) (2025.8.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth-zoo) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth-zoo) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth-zoo) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth-zoo) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth-zoo) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth-zoo) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth-zoo) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth-zoo) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth-zoo) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth-zoo) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth-zoo) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth-zoo) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth-zoo) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth-zoo) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth-zoo) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth-zoo) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth-zoo) (1.11.1.6)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from triton->unsloth-zoo) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth-zoo) (1.3.0)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3->unsloth-zoo)\n",
      "  Using cached tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jinja2->torch>=2.4.0->unsloth-zoo) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth-zoo) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets<4.0.0,>=3.4.1->unsloth-zoo)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets<4.0.0,>=3.4.1->unsloth-zoo)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1->unsloth-zoo) (1.17.0)\n",
      "Collecting docstring-parser>=0.15 (from tyro->unsloth-zoo)\n",
      "  Using cached docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=11.1.0 (from tyro->unsloth-zoo)\n",
      "  Using cached rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth-zoo)\n",
      "  Using cached shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro->unsloth-zoo)\n",
      "  Using cached typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro->unsloth-zoo)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from rich>=11.1.0->tyro->unsloth-zoo) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth-zoo)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached unsloth_zoo-2025.8.9-py3-none-any.whl (196 kB)\n",
      "Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Using cached aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "Using cached multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "Using cached yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
      "Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "Using cached hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Using cached propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Using cached pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Using cached sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.56.0-py3-none-any.whl (11.6 MB)\n",
      "Using cached tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached regex-2025.8.29-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
      "Using cached trl-0.22.1-py3-none-any.whl (544 kB)\n",
      "Using cached cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Using cached hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
      "Using cached pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
      "Using cached torchao-0.12.0-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
      "Using cached tyro-0.9.31-py3-none-any.whl (131 kB)\n",
      "Using cached docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Using cached rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Using cached typeguard-4.4.4-py3-none-any.whl (34 kB)\n",
      "Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: torchao, pytz, xxhash, tzdata, typeguard, tqdm, shtab, sentencepiece, safetensors, regex, pyarrow, protobuf, propcache, multidict, msgspec, mdurl, hf-xet, hf_transfer, fsspec, frozenlist, docstring-parser, dill, aiohappyeyeballs, yarl, pandas, multiprocess, markdown-it-py, huggingface_hub, aiosignal, tokenizers, rich, aiohttp, tyro, transformers, cut_cross_entropy, accelerate, peft, datasets, trl, unsloth-zoo\n",
      "\u001b[2K  Attempting uninstall: fsspec[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/40\u001b[0m [hf_transfer]e]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.7.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/40\u001b[0m [hf_transfer]\n",
      "\u001b[2K    Uninstalling fsspec-2025.7.0:0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/40\u001b[0m [hf_transfer]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.7.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/40\u001b[0m [hf_transfer]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40/40\u001b[0m [unsloth-zoo]loth-zoo]]sets]e]s]ub]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.10.1 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 cut_cross_entropy-25.1.1 datasets-3.6.0 dill-0.3.8 docstring-parser-0.17.0 frozenlist-1.7.0 fsspec-2025.3.0 hf-xet-1.1.9 hf_transfer-0.1.9 huggingface_hub-0.34.4 markdown-it-py-4.0.0 mdurl-0.1.2 msgspec-0.19.0 multidict-6.6.4 multiprocess-0.70.16 pandas-2.3.2 peft-0.17.1 propcache-0.3.2 protobuf-6.32.0 pyarrow-21.0.0 pytz-2025.2 regex-2025.8.29 rich-14.1.0 safetensors-0.6.2 sentencepiece-0.2.1 shtab-1.7.2 tokenizers-0.22.0 torchao-0.12.0 tqdm-4.67.1 transformers-4.56.0 trl-0.22.1 typeguard-4.4.4 tyro-0.9.31 tzdata-2025.2 unsloth-zoo-2025.8.9 xxhash-3.5.0 yarl-1.20.1\n",
      "Collecting vllm==0.10.1.1\n",
      "  Using cached vllm-0.10.1.1-cp38-abi3-manylinux1_x86_64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: regex in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm==0.10.1.1) (2025.8.29)\n",
      "Collecting cachetools (from vllm==0.10.1.1)\n",
      "  Using cached cachetools-6.2.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: psutil in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm==0.10.1.1) (5.9.0)\n",
      "Requirement already satisfied: sentencepiece in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm==0.10.1.1) (0.2.1)\n",
      "Requirement already satisfied: numpy in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm==0.10.1.1) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm==0.10.1.1) (2.32.5)\n",
      "Requirement already satisfied: tqdm in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm==0.10.1.1) (4.67.1)\n",
      "Collecting blake3 (from vllm==0.10.1.1)\n",
      "  Using cached blake3-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting py-cpuinfo (from vllm==0.10.1.1)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: transformers>=4.55.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm==0.10.1.1) (4.56.0)\n",
      "Requirement already satisfied: tokenizers>=0.21.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm==0.10.1.1) (0.22.0)\n",
      "Requirement already satisfied: protobuf in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm==0.10.1.1) (6.32.0)\n",
      "Collecting fastapi>=0.115.0 (from fastapi[standard]>=0.115.0->vllm==0.10.1.1)\n",
      "  Using cached fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: aiohttp in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm==0.10.1.1) (3.12.15)\n",
      "Collecting openai>=1.99.1 (from vllm==0.10.1.1)\n",
      "  Using cached openai-1.102.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting pydantic>=2.10 (from vllm==0.10.1.1)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm==0.10.1.1) (0.21.1)\n",
      "Requirement already satisfied: pillow in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm==0.10.1.1) (11.3.0)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm==0.10.1.1)\n",
      "  Using cached prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tiktoken>=0.6.0 (from vllm==0.10.1.1)\n",
      "  Using cached tiktoken-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting lm-format-enforcer<0.11,>=0.10.11 (from vllm==0.10.1.1)\n",
      "  Using cached lm_format_enforcer-0.10.12-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting llguidance<0.8.0,>=0.7.11 (from vllm==0.10.1.1)\n",
      "  Using cached llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting outlines_core==0.2.10 (from vllm==0.10.1.1)\n",
      "  Using cached outlines_core-0.2.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting diskcache==5.6.3 (from vllm==0.10.1.1)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting lark==1.2.2 (from vllm==0.10.1.1)\n",
      "  Using cached lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting xgrammar==0.1.21 (from vllm==0.10.1.1)\n",
      "  Using cached xgrammar-0.1.21-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm==0.10.1.1) (4.15.0)\n",
      "Requirement already satisfied: filelock>=3.16.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm==0.10.1.1) (3.19.1)\n",
      "Collecting partial-json-parser (from vllm==0.10.1.1)\n",
      "  Using cached partial_json_parser-0.2.1.1.post6-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm==0.10.1.1) (26.2.0)\n",
      "Requirement already satisfied: msgspec in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm==0.10.1.1) (0.19.0)\n",
      "Collecting gguf>=0.13.0 (from vllm==0.10.1.1)\n",
      "  Using cached gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting mistral_common>=1.8.2 (from mistral_common[audio,image]>=1.8.2->vllm==0.10.1.1)\n",
      "  Using cached mistral_common-1.8.4-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting opencv-python-headless>=4.11.0 (from vllm==0.10.1.1)\n",
      "  Using cached opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: pyyaml in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm==0.10.1.1) (6.0.2)\n",
      "Collecting einops (from vllm==0.10.1.1)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting compressed-tensors==0.10.2 (from vllm==0.10.1.1)\n",
      "  Using cached compressed_tensors-0.10.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting depyf==0.19.0 (from vllm==0.10.1.1)\n",
      "  Using cached depyf-0.19.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting cloudpickle (from vllm==0.10.1.1)\n",
      "  Using cached cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting watchfiles (from vllm==0.10.1.1)\n",
      "  Using cached watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: python-json-logger in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm==0.10.1.1) (3.2.1)\n",
      "Collecting scipy (from vllm==0.10.1.1)\n",
      "  Using cached scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting ninja (from vllm==0.10.1.1)\n",
      "  Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting pybase64 (from vllm==0.10.1.1)\n",
      "  Using cached pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
      "Collecting cbor2 (from vllm==0.10.1.1)\n",
      "  Using cached cbor2-5.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting setproctitle (from vllm==0.10.1.1)\n",
      "  Using cached setproctitle-1.3.6-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting openai-harmony>=0.0.3 (from vllm==0.10.1.1)\n",
      "  Using cached openai_harmony-0.0.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
      "Collecting numba==0.61.2 (from vllm==0.10.1.1)\n",
      "  Using cached numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting ray>=2.48.0 (from ray[cgraph]>=2.48.0->vllm==0.10.1.1)\n",
      "  Using cached ray-2.49.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: torch==2.7.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm==0.10.1.1) (2.7.1)\n",
      "Requirement already satisfied: torchaudio==2.7.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm==0.10.1.1) (2.7.1)\n",
      "Requirement already satisfied: torchvision==0.22.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm==0.10.1.1) (0.22.1)\n",
      "Requirement already satisfied: xformers==0.0.31 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm==0.10.1.1) (0.0.31)\n",
      "Collecting astor (from depyf==0.19.0->vllm==0.10.1.1)\n",
      "  Using cached astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: dill in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from depyf==0.19.0->vllm==0.10.1.1) (0.3.8)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm==0.10.1.1)\n",
      "  Using cached llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Collecting numpy (from vllm==0.10.1.1)\n",
      "  Using cached numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm==0.10.1.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm==0.10.1.1) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm==0.10.1.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm==0.10.1.1) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm==0.10.1.1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm==0.10.1.1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm==0.10.1.1) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm==0.10.1.1) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm==0.10.1.1) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm==0.10.1.1) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm==0.10.1.1) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm==0.10.1.1) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm==0.10.1.1) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm==0.10.1.1) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm==0.10.1.1) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm==0.10.1.1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm==0.10.1.1) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm==0.10.1.1) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1->vllm==0.10.1.1) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from triton==3.3.1->torch==2.7.1->vllm==0.10.1.1) (78.1.1)\n",
      "Collecting interegular>=0.3.2 (from lm-format-enforcer<0.11,>=0.10.11->vllm==0.10.1.1)\n",
      "  Using cached interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from lm-format-enforcer<0.11,>=0.10.11->vllm==0.10.1.1) (25.0)\n",
      "Collecting starlette<0.48.0,>=0.40.0 (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm==0.10.1.1)\n",
      "  Using cached starlette-0.47.3-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.10->vllm==0.10.1.1)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.10->vllm==0.10.1.1)\n",
      "  Using cached pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.10->vllm==0.10.1.1)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from starlette<0.48.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm==0.10.1.1) (4.7.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm==0.10.1.1) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm==0.10.1.1) (1.3.0)\n",
      "Collecting fastapi-cli>=0.0.8 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.1.1)\n",
      "  Using cached fastapi_cli-0.0.10-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi[standard]>=0.115.0->vllm==0.10.1.1) (0.28.1)\n",
      "Collecting python-multipart>=0.0.18 (from fastapi[standard]>=0.115.0->vllm==0.10.1.1)\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm==0.10.1.1)\n",
      "  Using cached email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.1.1)\n",
      "  Using cached uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm==0.10.1.1)\n",
      "  Using cached dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting typer>=0.15.1 (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.1.1)\n",
      "  Using cached typer-0.17.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting rich-toolkit>=0.14.8 (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.1.1)\n",
      "  Using cached rich_toolkit-0.15.0-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting fastapi-cloud-cli>=0.1.1 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.1.1)\n",
      "  Using cached fastapi_cloud_cli-0.1.5-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting rignore>=0.5.1 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.1.1)\n",
      "  Using cached rignore-0.6.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting sentry-sdk>=2.20.0 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.1.1)\n",
      "  Using cached sentry_sdk-2.35.2-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: certifi in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm==0.10.1.1) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm==0.10.1.1) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm==0.10.1.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jinja2->torch==2.7.1->vllm==0.10.1.1) (3.0.2)\n",
      "Requirement already satisfied: jsonschema>=4.21.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.1.1) (4.25.0)\n",
      "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.1.1)\n",
      "  Using cached pydantic_extra_types-2.10.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.1.1) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.1.1) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.1.1) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.1.1) (0.22.3)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.99.1->vllm==0.10.1.1)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.99.1->vllm==0.10.1.1)\n",
      "  Using cached jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.1.1)\n",
      "  Using cached pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting click>=7.0 (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm==0.10.1.1)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm==0.10.1.1)\n",
      "  Using cached msgpack-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting cupy-cuda12x (from ray[cgraph]>=2.48.0->vllm==0.10.1.1)\n",
      "  Using cached cupy_cuda12x-13.6.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from requests>=2.26.0->vllm==0.10.1.1) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from requests>=2.26.0->vllm==0.10.1.1) (2.5.0)\n",
      "Requirement already satisfied: rich>=13.7.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.1.1) (14.1.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.1.1) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.1.1) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.1.1) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from sympy>=1.13.3->torch==2.7.1->vllm==0.10.1.1) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from tokenizers>=0.21.1->vllm==0.10.1.1) (0.34.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.21.1->vllm==0.10.1.1) (1.1.9)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from transformers>=4.55.0->vllm==0.10.1.1) (0.6.2)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.1.1)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.1.1)\n",
      "  Using cached httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.1.1)\n",
      "  Using cached python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.1.1)\n",
      "  Using cached uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.10.1.1)\n",
      "  Using cached websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp->vllm==0.10.1.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp->vllm==0.10.1.1) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp->vllm==0.10.1.1) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp->vllm==0.10.1.1) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp->vllm==0.10.1.1) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp->vllm==0.10.1.1) (1.20.1)\n",
      "Collecting fastrlock>=0.5 (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm==0.10.1.1)\n",
      "  Using cached fastrlock-0.8.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting soundfile>=0.12.1 (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.1.1)\n",
      "  Using cached soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.1.1) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.1.1) (2.21)\n",
      "Collecting soxr>=0.5.0 (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm==0.10.1.1)\n",
      "  Using cached soxr-0.5.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Using cached vllm-0.10.1.1-cp38-abi3-manylinux1_x86_64.whl (414.4 MB)\n",
      "Using cached compressed_tensors-0.10.2-py3-none-any.whl (169 kB)\n",
      "Using cached depyf-0.19.0-py3-none-any.whl (39 kB)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Using cached lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "Using cached numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
      "Using cached outlines_core-0.2.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "Using cached xgrammar-0.1.21-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
      "Using cached llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
      "Using cached llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "Using cached lm_format_enforcer-0.10.12-py3-none-any.whl (44 kB)\n",
      "Using cached numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "Using cached fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached starlette-0.47.3-py3-none-any.whl (72 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached email_validator-2.3.0-py3-none-any.whl (35 kB)\n",
      "Using cached dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "Using cached fastapi_cli-0.0.10-py3-none-any.whl (10 kB)\n",
      "Using cached fastapi_cloud_cli-0.1.5-py3-none-any.whl (18 kB)\n",
      "Using cached gguf-0.17.1-py3-none-any.whl (96 kB)\n",
      "Using cached interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Using cached mistral_common-1.8.4-py3-none-any.whl (6.5 MB)\n",
      "Using cached openai-1.102.0-py3-none-any.whl (812 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Using cached openai_harmony-0.0.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n",
      "Using cached prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
      "Using cached pydantic_extra_types-2.10.5-py3-none-any.whl (38 kB)\n",
      "Using cached pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Using cached ray-2.49.0-cp311-cp311-manylinux2014_x86_64.whl (70.1 MB)\n",
      "Using cached msgpack-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (429 kB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Using cached rich_toolkit-0.15.0-py3-none-any.whl (29 kB)\n",
      "Using cached rignore-0.6.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (950 kB)\n",
      "Using cached sentry_sdk-2.35.2-py2.py3-none-any.whl (363 kB)\n",
      "Using cached tiktoken-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached typer-0.17.3-py3-none-any.whl (46 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Using cached httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
      "Using cached python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Using cached uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "Using cached watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
      "Using cached websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
      "Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Using cached blake3-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (385 kB)\n",
      "Using cached cachetools-6.2.0-py3-none-any.whl (11 kB)\n",
      "Using cached cbor2-5.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (254 kB)\n",
      "Using cached cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Using cached cupy_cuda12x-13.6.0-cp311-cp311-manylinux2014_x86_64.whl (113.0 MB)\n",
      "Using cached fastrlock-0.8.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (54 kB)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Using cached soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "Using cached soxr-0.5.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (252 kB)\n",
      "Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
      "Using cached partial_json_parser-0.2.1.1.post6-py3-none-any.whl (10 kB)\n",
      "Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Using cached pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
      "Using cached scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.4 MB)\n",
      "Using cached setproctitle-1.3.6-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Installing collected packages: py-cpuinfo, fastrlock, blake3, websockets, uvloop, typing-inspection, shellingham, setproctitle, sentry-sdk, rignore, python-multipart, python-dotenv, pydantic-core, pycountry, pybase64, partial-json-parser, outlines_core, numpy, ninja, msgpack, llvmlite, llguidance, lark, jiter, interegular, httptools, einops, dnspython, distro, diskcache, cloudpickle, click, cbor2, cachetools, astor, annotated-types, watchfiles, uvicorn, tiktoken, starlette, soxr, soundfile, scipy, pydantic, opencv-python-headless, numba, gguf, email-validator, depyf, cupy-cuda12x, typer, rich-toolkit, pydantic-extra-types, prometheus-fastapi-instrumentator, openai-harmony, openai, lm-format-enforcer, fastapi, ray, fastapi-cloud-cli, fastapi-cli, xgrammar, mistral_common, compressed-tensors, vllm\n",
      "\u001b[2K  Attempting uninstall: numpy0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/65\u001b[0m [outlines_core]arser]\n",
      "\u001b[2K    Found existing installation: numpy 2.3.2━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/65\u001b[0m [outlines_core]\n",
      "\u001b[2K    Uninstalling numpy-2.3.2:[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/65\u001b[0m [numpy]ore]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.3.2━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/65\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65/65\u001b[0m [vllm]0m [vllm]0m [compressed-tensors]umentator]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 astor-0.8.1 blake3-1.0.5 cachetools-6.2.0 cbor2-5.7.0 click-8.2.1 cloudpickle-3.1.1 compressed-tensors-0.10.2 cupy-cuda12x-13.6.0 depyf-0.19.0 diskcache-5.6.3 distro-1.9.0 dnspython-2.7.0 einops-0.8.1 email-validator-2.3.0 fastapi-0.116.1 fastapi-cli-0.0.10 fastapi-cloud-cli-0.1.5 fastrlock-0.8.3 gguf-0.17.1 httptools-0.6.4 interegular-0.3.3 jiter-0.10.0 lark-1.2.2 llguidance-0.7.30 llvmlite-0.44.0 lm-format-enforcer-0.10.12 mistral_common-1.8.4 msgpack-1.1.1 ninja-1.13.0 numba-0.61.2 numpy-2.2.6 openai-1.102.0 openai-harmony-0.0.4 opencv-python-headless-4.12.0.88 outlines_core-0.2.10 partial-json-parser-0.2.1.1.post6 prometheus-fastapi-instrumentator-7.1.0 py-cpuinfo-9.0.0 pybase64-1.4.2 pycountry-24.6.1 pydantic-2.11.7 pydantic-core-2.33.2 pydantic-extra-types-2.10.5 python-dotenv-1.1.1 python-multipart-0.0.20 ray-2.49.0 rich-toolkit-0.15.0 rignore-0.6.4 scipy-1.16.1 sentry-sdk-2.35.2 setproctitle-1.3.6 shellingham-1.5.4 soundfile-0.13.1 soxr-0.5.0.post1 starlette-0.47.3 tiktoken-0.11.0 typer-0.17.3 typing-inspection-0.4.1 uvicorn-0.35.0 uvloop-0.21.0 vllm-0.10.1.1 watchfiles-1.1.0 websockets-15.0.1 xgrammar-0.1.21\n",
      "Found existing installation: torch 2.7.1\n",
      "Uninstalling torch-2.7.1:\n",
      "  Successfully uninstalled torch-2.7.1\n",
      "Collecting torch==2.7.1\n",
      "  Using cached torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: filelock in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch==2.7.1) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from triton==3.3.1->torch==2.7.1) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from sympy>=1.13.3->torch==2.7.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jinja2->torch==2.7.1) (3.0.2)\n",
      "Using cached torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.7.1\n",
      "No broken requirements found.\n",
      "Mon Sep  1 17:00:58 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.65.06              Driver Version: 581.08         CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2070 ...    On  |   00000000:06:00.0  On |                  N/A |\n",
      "|  0%   36C    P8              4W /  215W |     562MiB /   8192MiB |      6%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A             125      G   /Xwayland                             N/A      |\n",
      "|    0   N/A  N/A             235      C   /ollama                               N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Install all dependencies first\n",
    "!pip install torchaudio==2.7.1\n",
    "!pip install bitsandbytes==0.47.0\n",
    "!pip install torchvision==0.22.1\n",
    "!pip install xformers==0.0.31\n",
    "!pip install unsloth-zoo\n",
    "!pip install vllm==0.10.1.1\n",
    "\n",
    "# Then, install torch last\n",
    "!pip uninstall torch -y\n",
    "!pip install torch==2.7.1\n",
    "\n",
    "!pip check\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.7.1+cu126\n",
      "Triton version: 3.3.1\n",
      "Torchaudio version: 2.7.1+cu126\n",
      "Xformers version: 0.0.31\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "import torchaudio\n",
    "import torchvision\n",
    "import xformers\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Triton version: {triton.__version__}\")\n",
    "print(f\"Torchaudio version: {torchaudio.__version__}\")\n",
    "print(f\"Xformers version: {xformers.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1IgoPx8ODl4q"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    # If you're not in Colab, just use pip install or uv pip install\n",
    "    !pip install unsloth vllm\n",
    "else:\n",
    "    pass # For Colab / Kaggle, we need extra instructions hidden below \\/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cc18O9yeDl4q",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unsloth in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (2025.8.10)\n",
      "Requirement already satisfied: vllm in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (0.10.1.1)\n",
      "Requirement already satisfied: unsloth_zoo>=2025.8.9 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth) (2025.8.9)\n",
      "Requirement already satisfied: torch>=2.4.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth) (2.7.1)\n",
      "Requirement already satisfied: xformers>=0.0.27.post2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth) (0.0.31)\n",
      "Requirement already satisfied: bitsandbytes in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth) (0.47.0)\n",
      "Requirement already satisfied: triton>=3.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth) (3.3.1)\n",
      "Requirement already satisfied: packaging in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth) (25.0)\n",
      "Requirement already satisfied: tyro in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth) (0.9.31)\n",
      "Requirement already satisfied: transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth) (4.56.0)\n",
      "Requirement already satisfied: datasets<4.0.0,>=3.4.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth) (3.6.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth) (0.2.1)\n",
      "Requirement already satisfied: tqdm in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth) (4.67.1)\n",
      "Requirement already satisfied: psutil in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth) (5.9.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth) (0.45.1)\n",
      "Requirement already satisfied: numpy in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth) (2.2.6)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth) (1.10.1)\n",
      "Requirement already satisfied: trl!=0.15.0,!=0.19.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth) (0.22.1)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth) (0.17.1)\n",
      "Requirement already satisfied: protobuf in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth) (6.32.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.34.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth) (0.34.4)\n",
      "Requirement already satisfied: hf_transfer in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth) (0.1.9)\n",
      "Requirement already satisfied: diffusers in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth) (0.35.1)\n",
      "Requirement already satisfied: torchvision in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth) (0.22.1)\n",
      "Requirement already satisfied: filelock in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth) (2.32.5)\n",
      "Requirement already satisfied: xxhash in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (2025.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1->unsloth) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (3.12.15)\n",
      "Requirement already satisfied: regex in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (2025.8.29)\n",
      "Requirement already satisfied: cachetools in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (6.2.0)\n",
      "Requirement already satisfied: blake3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (1.0.5)\n",
      "Requirement already satisfied: py-cpuinfo in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (9.0.0)\n",
      "Requirement already satisfied: tokenizers>=0.21.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.22.0)\n",
      "Requirement already satisfied: fastapi>=0.115.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.116.1)\n",
      "Requirement already satisfied: openai>=1.99.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (1.102.0)\n",
      "Requirement already satisfied: pydantic>=2.10 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (2.11.7)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.21.1)\n",
      "Requirement already satisfied: pillow in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (11.3.0)\n",
      "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (7.1.0)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.11.0)\n",
      "Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.11 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.10.12)\n",
      "Requirement already satisfied: llguidance<0.8.0,>=0.7.11 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.7.30)\n",
      "Requirement already satisfied: outlines_core==0.2.10 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.2.10)\n",
      "Requirement already satisfied: diskcache==5.6.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (5.6.3)\n",
      "Requirement already satisfied: lark==1.2.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (1.2.2)\n",
      "Requirement already satisfied: xgrammar==0.1.21 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.1.21)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (4.15.0)\n",
      "Requirement already satisfied: partial-json-parser in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.2.1.1.post6)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (26.2.0)\n",
      "Requirement already satisfied: msgspec in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.19.0)\n",
      "Requirement already satisfied: gguf>=0.13.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.17.1)\n",
      "Requirement already satisfied: mistral_common>=1.8.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from mistral_common[audio,image]>=1.8.2->vllm) (1.8.4)\n",
      "Requirement already satisfied: opencv-python-headless>=4.11.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (4.12.0.88)\n",
      "Requirement already satisfied: einops in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.8.1)\n",
      "Requirement already satisfied: compressed-tensors==0.10.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.10.2)\n",
      "Requirement already satisfied: depyf==0.19.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.19.0)\n",
      "Requirement already satisfied: cloudpickle in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (3.1.1)\n",
      "Requirement already satisfied: watchfiles in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (1.1.0)\n",
      "Requirement already satisfied: python-json-logger in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (3.2.1)\n",
      "Requirement already satisfied: scipy in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (1.16.1)\n",
      "Requirement already satisfied: ninja in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (1.13.0)\n",
      "Requirement already satisfied: pybase64 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (1.4.2)\n",
      "Requirement already satisfied: cbor2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (5.7.0)\n",
      "Requirement already satisfied: setproctitle in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (1.3.6)\n",
      "Requirement already satisfied: openai-harmony>=0.0.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.0.4)\n",
      "Requirement already satisfied: numba==0.61.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (0.61.2)\n",
      "Requirement already satisfied: ray>=2.48.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from ray[cgraph]>=2.48.0->vllm) (2.49.0)\n",
      "Requirement already satisfied: torchaudio==2.7.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from vllm) (2.7.1)\n",
      "Requirement already satisfied: astor in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from depyf==0.19.0->vllm) (0.8.1)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from numba==0.61.2->vllm) (0.44.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (1.11.1.6)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from triton>=3.0.0->unsloth) (78.1.1)\n",
      "Requirement already satisfied: interegular>=0.3.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from lm-format-enforcer<0.11,>=0.10.11->vllm) (0.3.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from accelerate>=0.34.1->unsloth) (0.6.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (3.7)\n",
      "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.47.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pydantic>=2.10->vllm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pydantic>=2.10->vllm) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pydantic>=2.10->vllm) (0.4.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from starlette<0.48.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (4.7.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (1.3.0)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.8 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.0.10)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi[standard]>=0.115.0->vllm) (2.3.0)\n",
      "Requirement already satisfied: uvicorn>=0.12.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.35.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.7.0)\n",
      "Requirement already satisfied: typer>=0.15.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.17.3)\n",
      "Requirement already satisfied: rich-toolkit>=0.14.8 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.15.0)\n",
      "Requirement already satisfied: fastapi-cloud-cli>=0.1.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.5)\n",
      "Requirement already satisfied: rignore>=0.5.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.6.4)\n",
      "Requirement already satisfied: sentry-sdk>=2.20.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.35.2)\n",
      "Requirement already satisfied: certifi in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from huggingface_hub>=0.34.0->unsloth) (1.1.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
      "Requirement already satisfied: jsonschema>=4.21.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (4.25.0)\n",
      "Requirement already satisfied: pydantic-extra-types>=2.10.5 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2.10.5)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.22.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from openai>=1.99.1->vllm) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from openai>=1.99.1->vllm) (0.10.0)\n",
      "Requirement already satisfied: pycountry>=23 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (24.6.1)\n",
      "Requirement already satisfied: click>=7.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (8.2.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (1.1.1)\n",
      "Requirement already satisfied: cupy-cuda12x in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from ray[cgraph]>=2.48.0->vllm) (13.6.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (2.5.0)\n",
      "Requirement already satisfied: rich>=13.7.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.1.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
      "Requirement already satisfied: torchao in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth_zoo>=2025.8.9->unsloth) (0.12.0)\n",
      "Requirement already satisfied: cut_cross_entropy in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from unsloth_zoo>=2025.8.9->unsloth) (25.1.1)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.1.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm) (0.8.3)\n",
      "Requirement already satisfied: importlib_metadata in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from diffusers->unsloth) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.13.1)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2.21)\n",
      "Requirement already satisfied: soxr>=0.5.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.5.0.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1->unsloth) (1.17.0)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from tyro->unsloth) (0.17.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from tyro->unsloth) (1.7.2)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from tyro->unsloth) (4.4.4)\n",
      "\u001b[2mUsing Python 3.11.13 environment at: /home/golongson/miniconda3/envs/dl\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m18 packages\u001b[0m \u001b[2min 2.51s\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m2 packages\u001b[0m \u001b[2min 285ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 447ms\u001b[0m\u001b[0m                               \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.21.4\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.56.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.55.4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#@title Colab Extra Install { display-mode: \"form\" }\n",
    "#%%capture\n",
    "import os\n",
    "!pip install --upgrade -qqq uv\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    # If you're not in Colab, just use pip install!\n",
    "    !pip install unsloth vllm\n",
    "else:\n",
    "    try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
    "    except: get_numpy = \"numpy\"\n",
    "    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
    "    except: is_t4 = False\n",
    "    get_vllm, get_triton = (\"vllm==0.10.1\", \"triton==3.2.0\") if is_t4 else (\"vllm\", \"triton\")\n",
    "    !uv pip install -qqq --upgrade \\\n",
    "        unsloth {get_vllm} {get_numpy} torchvision bitsandbytes xformers\n",
    "    !uv pip install -qqq {get_triton}\n",
    "!uv pip install transformers==4.55.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fA26-ZvUDl4r"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVL2qQZHDl4r"
   },
   "source": [
    "Load up `unsloth/mistral-7b-instruct-v0.3-bnb-4bit`, and set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 8.0 GB\n",
      "=== Solution 1: Proper Unsloth setup for RTX 2070 Super ===\n",
      "==((====))==  Unsloth 2025.8.10: Fast Mistral patching. Transformers: 4.55.4. vLLM: 0.10.1.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 2070 SUPER. Num GPUs = 1. Max memory: 8.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "❌ First attempt failed: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Trying with even more conservative settings...\n",
      "==((====))==  Unsloth 2025.8.10: Fast Mistral patching. Transformers: 4.55.4. vLLM: 0.10.1.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 2070 SUPER. Num GPUs = 1. Max memory: 8.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "❌ Conservative settings also failed: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Switching to smaller model...\n",
      "==((====))==  Unsloth 2025.8.10: Fast Qwen2 patching. Transformers: 4.55.4. vLLM: 0.10.1.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 2070 SUPER. Num GPUs = 1. Max memory: 8.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b8034e7f454812b517fc282cfebf4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb1b287e6e3426989fa7b52c049102a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331495ee12c64cbd9f2cda4425c7ea42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89075d2fc5a64918b4696a9e2c44a1f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78755e682bb24f53b91854cc914a3370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be527ac6ad9045a6b2c5ddc5856fabf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b4dc17624e4c6d8e046c304167ed94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5525c2bb62401e8764c6b29a0e2310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Smaller 1.5B model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Unsloth 2025.8.10 patched 28 layers with 28 QKV layers, 28 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LoRA applied successfully with attention modules only!\n",
      "GPU Memory - Allocated: 5.04 GB, Reserved: 5.05 GB\n",
      "🎉 Setup complete! Model ready for training.\n",
      "\n",
      "==================================================\n",
      "Alternative Solution: Manual GPU/CPU split\n",
      "Uncomment below if you want to try manual device mapping:\n",
      "==================================================\n",
      "🔧 Troubleshooting tips:\n",
      "1. If still getting memory errors, reduce lora_rank further (to 4 or 8)\n",
      "2. Close all other applications using GPU memory\n",
      "3. Try the smaller Qwen2.5-1.5B model - it's quite capable!\n",
      "4. Consider using Google Colab with T4 (16GB) for larger models\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Clear GPU memory first\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n",
    "\n",
    "print(\"=== Solution 1: Proper Unsloth setup for RTX 2070 Super ===\")\n",
    "\n",
    "max_seq_length = 128\n",
    "lora_rank = 16  # Significantly reduced for 8GB GPU\n",
    "\n",
    "try:\n",
    "    # Attempt 1: Standard Unsloth approach with reduced settings\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "        max_seq_length = max_seq_length,\n",
    "        load_in_4bit = True,\n",
    "        fast_inference = False,  # Disable vLLM\n",
    "        max_lora_rank = lora_rank,\n",
    "        dtype = torch.float16,  # Use FP16 to save memory\n",
    "        # Remove device_map and offload params - Unsloth handles this\n",
    "    )\n",
    "    print(\"✅ Model loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ First attempt failed: {e}\")\n",
    "    print(\"Trying with even more conservative settings...\")\n",
    "    \n",
    "    try:\n",
    "        # Attempt 2: More conservative settings\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "            max_seq_length = 64,  # Reduced sequence length\n",
    "            load_in_4bit = True,\n",
    "            fast_inference = False,\n",
    "            max_lora_rank = 8,  # Very small rank\n",
    "            dtype = torch.float16,\n",
    "        )\n",
    "        max_seq_length = 64\n",
    "        lora_rank = 8\n",
    "        print(\"✅ Model loaded with conservative settings!\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"❌ Conservative settings also failed: {e2}\")\n",
    "        print(\"Switching to smaller model...\")\n",
    "        \n",
    "        # Attempt 3: Use smaller model that fits RTX 2070 Super\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = \"unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit\",  # 1.5B model\n",
    "            max_seq_length = 128,\n",
    "            load_in_4bit = True,\n",
    "            fast_inference = False,\n",
    "            max_lora_rank = 32,\n",
    "            dtype = torch.float16,\n",
    "        )\n",
    "        max_seq_length = 128\n",
    "        lora_rank = 32\n",
    "        print(\"✅ Smaller 1.5B model loaded successfully!\")\n",
    "\n",
    "# Apply LoRA - start with minimal target modules\n",
    "try:\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r = lora_rank,\n",
    "        target_modules = [\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention only\n",
    "        ],\n",
    "        lora_alpha = lora_rank,\n",
    "        use_gradient_checkpointing = \"unsloth\",\n",
    "        random_state = 3407,\n",
    "    )\n",
    "    print(\"✅ LoRA applied successfully with attention modules only!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"LoRA with attention failed: {e}\")\n",
    "    print(\"Trying with even fewer modules...\")\n",
    "    \n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r = max(lora_rank // 2, 4),  # Even smaller rank\n",
    "        target_modules = [\n",
    "            \"q_proj\", \"v_proj\",  # Only query and value projections\n",
    "        ],\n",
    "        lora_alpha = max(lora_rank // 2, 4),\n",
    "        use_gradient_checkpointing = \"unsloth\",\n",
    "        random_state = 3407,\n",
    "    )\n",
    "    print(\"✅ LoRA applied with minimal modules!\")\n",
    "\n",
    "# Check final memory usage\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    print(f\"GPU Memory - Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "\n",
    "print(\"🎉 Setup complete! Model ready for training.\")\n",
    "\n",
    "# Alternative: Manual device mapping approach (if needed)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Alternative Solution: Manual GPU/CPU split\")\n",
    "print(\"Uncomment below if you want to try manual device mapping:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "\"\"\"\n",
    "# This approach manually loads the base model with device mapping\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Manual device mapping for hybrid GPU/CPU loading\n",
    "device_map = {\n",
    "    \"model.embed_tokens\": 0,\n",
    "    \"model.layers.0\": 0,\n",
    "    \"model.layers.1\": 0,\n",
    "    \"model.layers.2\": 0,\n",
    "    \"model.layers.3\": 0,\n",
    "    \"model.layers.4\": 0,\n",
    "    \"model.layers.5\": 0,\n",
    "    \"model.layers.6\": 0,\n",
    "    \"model.layers.7\": 0,\n",
    "    \"model.layers.8\": 0,\n",
    "    \"model.layers.9\": 0,\n",
    "    \"model.layers.10\": 0,\n",
    "    \"model.layers.11\": 0,\n",
    "    \"model.layers.12\": 0,\n",
    "    \"model.layers.13\": 0,\n",
    "    \"model.layers.14\": 0,\n",
    "    \"model.layers.15\": 0,\n",
    "    # Rest on CPU\n",
    "    \"model.layers.16\": \"cpu\",\n",
    "    \"model.layers.17\": \"cpu\",\n",
    "    \"model.layers.18\": \"cpu\",\n",
    "    \"model.layers.19\": \"cpu\",\n",
    "    \"model.layers.20\": \"cpu\",\n",
    "    \"model.layers.21\": \"cpu\",\n",
    "    \"model.layers.22\": \"cpu\",\n",
    "    \"model.layers.23\": \"cpu\",\n",
    "    \"model.layers.24\": \"cpu\",\n",
    "    \"model.layers.25\": \"cpu\",\n",
    "    \"model.layers.26\": \"cpu\",\n",
    "    \"model.layers.27\": \"cpu\",\n",
    "    \"model.layers.28\": \"cpu\",\n",
    "    \"model.layers.29\": \"cpu\",\n",
    "    \"model.layers.30\": \"cpu\",\n",
    "    \"model.layers.31\": \"cpu\",\n",
    "    \"model.norm\": \"cpu\",\n",
    "    \"lm_head\": \"cpu\"\n",
    "}\n",
    "\n",
    "# Load with transformers first, then wrap with Unsloth\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    device_map=device_map,\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\")\n",
    "\n",
    "# Then apply Unsloth optimizations\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    base_model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(\"🔧 Troubleshooting tips:\")\n",
    "print(\"1. If still getting memory errors, reduce lora_rank further (to 4 or 8)\")\n",
    "print(\"2. Close all other applications using GPU memory\")\n",
    "print(\"3. Try the smaller Qwen2.5-1.5B model - it's quite capable!\")\n",
    "print(\"4. Consider using Google Colab with T4 (16GB) for larger models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuLbyhIODl4v"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "\n",
    "We directly leverage [@willccbb](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb) for data prep and all reward functions. You are free to create your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "7a333f18cc8144168db9c269cf6e8e6c",
      "093d5e8629c143b586954a7852f98b5c",
      "a5ea7a019dbf456aa3ab37c63e81edf9",
      "adf539342ac848e99b2d075d706773ab",
      "a06e34bd1a24438b8b93130c207c2606",
      "b7f45d2aa2d141e695c7a21bfe374933"
     ]
    },
    "id": "oCjU3qa6Dl4v",
    "outputId": "27f54e3b-2141-4329-ba7b-4c3b17c4921d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2a50c6b33e44f3aa0f283af02a6e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Load and prep dataset\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "# uncomment middle messages for 1-shot prompting\n",
    "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore\n",
    "    data = data.map(lambda x: { # type: ignore\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': x['question']}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    }) # type: ignore\n",
    "    return data # type: ignore\n",
    "\n",
    "dataset = get_gsm8k_questions()\n",
    "\n",
    "# Reward functions\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "def int_reward_func(completions, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def count_xml(text) -> float:\n",
    "    count = 0.0\n",
    "    if text.count(\"<reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
    "    return count\n",
    "\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJWcHJyYDl4w"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "\n",
    "Now set up GRPO Trainer and all configurations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2helO1j2Dl4x"
   },
   "source": [
    "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
    "\n",
    "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
    "\n",
    "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
    "|------|---------------|-----------|------------|-------------------|----------|\n",
    "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
    "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
    "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 128\n",
      "Max prompt length: 64\n",
      "Max completion length: 64\n",
      "Unsloth: The DAPO paper recommends `mask_truncated_completions = True`\n",
      "Unsloth: The DAPO paper recommends `epsilon_high = 0.28`\n",
      "Unsloth: The DAPO paper recommends setting `beta = 0.0` to remove the KL term\n",
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 4\n",
      "🚀 Starting GRPO training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 7,473 | Num Epochs = 1 | Total steps = 10\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 2 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 8,716,288 of 1,552,430,592 (0.56% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Question:\n",
      "A concert ticket costs $40. Mr. Benson bought 12 tickets and received a 5% discount for every ticket bought that exceeds 10. How much did Mr. Benson pay in all? \n",
      "Answer:\n",
      "476 \n",
      "Response:\n",
      "To calculate how much Mr. Benson paid for the concert tickets, we need to consider the cost per ticket and the discount applied for each ticket exceeding a certain threshold.\n",
      "\n",
      "Cost per ticket: $40\n",
      "\n",
      "Number of tickets: 12\n",
      "\n",
      "Discount for more than 10 tickets: 5%\n",
      "\n",
      "First, we \n",
      "Extracted:\n",
      "To calculate how much Mr. Benson paid for the concert tickets, we need to consider the cost per ticket and the discount applied for each ticket exceeding a certain threshold.\n",
      "\n",
      "Cost per ticket: $40\n",
      "\n",
      "Number of tickets: 12\n",
      "\n",
      "Discount for more than 10 tickets: 5%\n",
      "\n",
      "First, we\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 03:55, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completions / mean_length</th>\n",
       "      <th>completions / min_length</th>\n",
       "      <th>completions / max_length</th>\n",
       "      <th>completions / clipped_ratio</th>\n",
       "      <th>completions / mean_terminated_length</th>\n",
       "      <th>completions / min_terminated_length</th>\n",
       "      <th>completions / max_terminated_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>entropy</th>\n",
       "      <th>rewards / xmlcount_reward_func / mean</th>\n",
       "      <th>rewards / xmlcount_reward_func / std</th>\n",
       "      <th>rewards / soft_format_reward_func / mean</th>\n",
       "      <th>rewards / soft_format_reward_func / std</th>\n",
       "      <th>rewards / strict_format_reward_func / mean</th>\n",
       "      <th>rewards / strict_format_reward_func / std</th>\n",
       "      <th>rewards / int_reward_func / mean</th>\n",
       "      <th>rewards / int_reward_func / std</th>\n",
       "      <th>rewards / correctness_reward_func / mean</th>\n",
       "      <th>rewards / correctness_reward_func / std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.625000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>50.500000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.875000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>35.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.375000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>49.500000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Question:\n",
      "Janet pays $40/hour for 3 hours per week of clarinet lessons and $28/hour for 5 hours a week of piano lessons. How much more does she spend on piano lessons than clarinet lessons in a year? \n",
      "Answer:\n",
      "1040 \n",
      "Response:\n",
      "To find out how much more Janet spends on piano lessons than clarinet lessons in a year, we first need to calculate her weekly expenses for each, then multiply by the number of weeks in a year.\n",
      "\n",
      "**Clarinet Lessons:**\n",
      "- Hourly rate: $40\n",
      "- Weeks per year: 5 \n",
      "Extracted:\n",
      "To find out how much more Janet spends on piano lessons than clarinet lessons in a year, we first need to calculate her weekly expenses for each, then multiply by the number of weeks in a year.\n",
      "\n",
      "**Clarinet Lessons:**\n",
      "- Hourly rate: $40\n",
      "- Weeks per year: 5\n",
      "-------------------- Question:\n",
      "Over the past five years, on July 4th, the high temperature for Washington, DC has been: 90 degrees in 2020, 90 degrees in 2019, 90 degrees in 2018, 79 degrees in 2017 and 71 degrees in 2016. What is the average temperature for July 4th in Washington, DC over the past 5 years? \n",
      "Answer:\n",
      "84 \n",
      "Response:\n",
      "To find the average temperature for July 4th over the past 5 years, you need to add up all the temperatures and then divide by the number of years.\n",
      "\n",
      "81 degrees in 2019\n",
      "90 degrees in 2018\n",
      "79 degrees in 2017\n",
      " \n",
      "Extracted:\n",
      "To find the average temperature for July 4th over the past 5 years, you need to add up all the temperatures and then divide by the number of years.\n",
      "\n",
      "81 degrees in 2019\n",
      "90 degrees in 2018\n",
      "79 degrees in 2017\n",
      "-------------------- Question:\n",
      "Martin rings the small bell 4 times more than 1/3 as often as the big bell. If he rings both of them a combined total of 52 times, how many times does he ring the big bell? \n",
      "Answer:\n",
      "36 \n",
      "Response:\n",
      "To determine how many times Martin rings the big bell, let's define the variables and follow through the given relationship step by step.\n",
      "\n",
      "1. **Define Variables:**\n",
      "   Let \\( x \\) represent the number of times the big bell rings.\n",
      "\n",
      "2. **Define the Relationship:**\n",
      "   Martin rings the small bell  \n",
      "Extracted:\n",
      "To determine how many times Martin rings the big bell, let's define the variables and follow through the given relationship step by step.\n",
      "\n",
      "1. **Define Variables:**\n",
      "   Let \\( x \\) represent the number of times the big bell rings.\n",
      "\n",
      "2. **Define the Relationship:**\n",
      "   Martin rings the small bell\n",
      "-------------------- Question:\n",
      "Matt can make a batch of a dozen cookies using 2 pounds of flour.  He uses 4 bags of flour each weighing 5 pounds.  If Jim eats 15 cookies how many cookies are left? \n",
      "Answer:\n",
      "105 \n",
      "Response:\n",
      "To determine how many cookies remain after Jim eats some of them, we need to follow these steps:\n",
      "\n",
      "1. Calculate the total number of cookies Matt can make using 4 bags of flour.\n",
      "2. Determine the number of cookies left after Jim eats 15 cookies.\n",
      "\n",
      "### Step 1: Calculate the total number of \n",
      "Extracted:\n",
      "To determine how many cookies remain after Jim eats some of them, we need to follow these steps:\n",
      "\n",
      "1. Calculate the total number of cookies Matt can make using 4 bags of flour.\n",
      "2. Determine the number of cookies left after Jim eats 15 cookies.\n",
      "\n",
      "### Step 1: Calculate the total number of\n",
      "-------------------- Question:\n",
      "A jar of jellybeans has 14 blue jellybeans, 26 purple jellybeans and 40 orange jellybeans. If there are 200 jellybeans in the jar, how many are there of the red color? \n",
      "Answer:\n",
      "120 \n",
      "Response:\n",
      "There are 200 jellybeans in total, and 14 blue + 26 purple + 40 orange = 80 jellybeans that are not red.\n",
      "\n",
      "Therefore, 200 - 80 = 120 jellybeans are red color jellybeans.\n",
      "\n",
      "Therefore, there are \n",
      "Extracted:\n",
      "There are 200 jellybeans in total, and 14 blue + 26 purple + 40 orange = 80 jellybeans that are not red.\n",
      "\n",
      "Therefore, 200 - 80 = 120 jellybeans are red color jellybeans.\n",
      "\n",
      "Therefore, there are\n",
      "-------------------- Question:\n",
      "Janelle had 26 green marbles. Then she bought 6 bags of blue marbles. There were 10 marbles in each bag.  She created a gift of 6 green marbles and 8 blue marbles and gave it to a friend. How many marbles does Janelle have now? \n",
      "Answer:\n",
      "72 \n",
      "Response:\n",
      "To solve this problem, let's break it down step by step:\n",
      "\n",
      "1. **Calculate the total number of blue marbles Janelle initially has:**\n",
      "   - She buys 10 bags of blue marbles, and each bag contains 10 marbles.\n",
      "   \\[ 10 \\text{ \n",
      "Extracted:\n",
      "To solve this problem, let's break it down step by step:\n",
      "\n",
      "1. **Calculate the total number of blue marbles Janelle initially has:**\n",
      "   - She buys 10 bags of blue marbles, and each bag contains 10 marbles.\n",
      "   \\[ 10 \\text{\n",
      "-------------------- Question:\n",
      "An old pirate wants to leave his treasure on an island. He has 3500 gold coins. He wants to spread this equally across 5 chests. Additionally, he will put a total of 500 silver coins and twice as many bronze coins as silver, all distributed equally across the chests. How many coins in total will be in each chest? \n",
      "Answer:\n",
      "1000 \n",
      "Response:\n",
      "To determine the total number of coins in each chest, we need to follow these steps:\n",
      "\n",
      "1. Calculate the total number of coins.\n",
      "2. Distribute the gold, silver, and bronze coins equally among the chests.\n",
      "\n",
      "First, we calculate the total number of coins. There are 3500 gold coins, \n",
      "Extracted:\n",
      "To determine the total number of coins in each chest, we need to follow these steps:\n",
      "\n",
      "1. Calculate the total number of coins.\n",
      "2. Distribute the gold, silver, and bronze coins equally among the chests.\n",
      "\n",
      "First, we calculate the total number of coins. There are 3500 gold coins,\n",
      "-------------------- Question:\n",
      "Zach baked cookies for a bake sale. On Monday he baked 32 cookies. On Tuesday he ran out of sugar and could only bake half of the number of cookies he baked the day before. On Wednesday he baked three times the number of cookies he did on Tuesday, but his brother ate 4 of those cookies. How many cookies did he have at the end of the three days? \n",
      "Answer:\n",
      "92 \n",
      "Response:\n",
      "Let's call the number of cookies baked on Tuesday \"x\".\n",
      "\n",
      "On Monday, he baked 2x cookies.\n",
      "On Tuesday, he baked 3x cookies (since he baked three times the number of cookies he did on Tuesday).\n",
      "On Wednesday, he baked 4x cookies (three times the number on Tuesday, \n",
      "Extracted:\n",
      "Let's call the number of cookies baked on Tuesday \"x\".\n",
      "\n",
      "On Monday, he baked 2x cookies.\n",
      "On Tuesday, he baked 3x cookies (since he baked three times the number of cookies he did on Tuesday).\n",
      "On Wednesday, he baked 4x cookies (three times the number on Tuesday,\n",
      "-------------------- Question:\n",
      "Erin counts six trolls hiding by the path in the forest, 6 less than four times that number of trolls hiding under the bridge, and half as many trolls hiding in the plains as under the bridge. How many trolls did she count in total? \n",
      "Answer:\n",
      "33 \n",
      "Response:\n",
      "To find the total number of trolls Erin counted, let's break down the information provided:\n",
      "\n",
      "1. Erin counts six trolls hiding by the path in the forest.\n",
      "2. The number of trolls hiding under the bridge is four times the path count minus six (6 trolls).\n",
      "3. The number of trolls hiding in the plains \n",
      "Extracted:\n",
      "To find the total number of trolls Erin counted, let's break down the information provided:\n",
      "\n",
      "1. Erin counts six trolls hiding by the path in the forest.\n",
      "2. The number of trolls hiding under the bridge is four times the path count minus six (6 trolls).\n",
      "3. The number of trolls hiding in the plains\n",
      "\n",
      "==================================================\n",
      "Alternative Solution: Increase model sequence length\n",
      "==================================================\n",
      "\n",
      "If you need longer prompts (256 tokens), you should reload your model with a larger max_seq_length:\n",
      "\n",
      "# When loading your model initially, use:\n",
      "max_seq_length = 512  # Instead of 128\n",
      "\n",
      "model, tokenizer = FastLanguageModel.from_pretrained(\n",
      "    model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\", \n",
      "    max_seq_length = 512,  # Increased to accommodate longer sequences\n",
      "    load_in_4bit = True,\n",
      "    fast_inference = False,\n",
      "    # ... other parameters\n",
      ")\n",
      "\n",
      "# Then you can use:\n",
      "max_prompt_length = 256\n",
      "max_completion_length = 256  # 512 - 256 = 256\n",
      "\n",
      "WARNING: Longer sequences use more GPU memory. For RTX 2070 Super:\n",
      "- 128 tokens: ✅ Should work\n",
      "- 256 tokens: ⚠️  Might work with small batch size  \n",
      "- 512 tokens: ❌ Likely too much memory\n",
      "\n",
      "Test with max_seq_length = 256 first, then 512 if you have enough memory.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fix for GRPO training configuration\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# CRITICAL FIX: Adjust sequence lengths to be compatible\n",
    "# Your max_seq_length from model loading was 128, but max_prompt_length was 256\n",
    "# This creates negative max_completion_length = 128 - 256 = -128\n",
    "\n",
    "# Solution 1: Reduce prompt length to fit within model's max_seq_length\n",
    "max_prompt_length = 64  # Reduced from 256 to fit in 128 total\n",
    "max_completion_length = 64  # 128 - 64 = 64 tokens for completion\n",
    "\n",
    "print(f\"Max sequence length: {max_seq_length}\")\n",
    "print(f\"Max prompt length: {max_prompt_length}\")  \n",
    "print(f\"Max completion length: {max_completion_length}\")\n",
    "\n",
    "# Verify the calculation\n",
    "assert max_prompt_length + max_completion_length <= max_seq_length, \\\n",
    "    f\"Prompt ({max_prompt_length}) + Completion ({max_completion_length}) = {max_prompt_length + max_completion_length} exceeds max_seq_length ({max_seq_length})\"\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 2,  # Increased for better gradients with small batch\n",
    "    num_generations = 4,  # Reduced from 6 for RTX 2070 Super memory\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_completion_length,\n",
    "    max_steps = 10,\n",
    "    save_steps = 10,\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"none\",\n",
    "    output_dir = \"outputs\",\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        xmlcount_reward_func,\n",
    "        soft_format_reward_func,\n",
    "        strict_format_reward_func,\n",
    "        int_reward_func,\n",
    "        correctness_reward_func,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"🚀 Starting GRPO training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Alternative Solution: Increase model sequence length\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "If you need longer prompts (256 tokens), you should reload your model with a larger max_seq_length:\n",
    "\n",
    "# When loading your model initially, use:\n",
    "max_seq_length = 512  # Instead of 128\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\", \n",
    "    max_seq_length = 512,  # Increased to accommodate longer sequences\n",
    "    load_in_4bit = True,\n",
    "    fast_inference = False,\n",
    "    # ... other parameters\n",
    ")\n",
    "\n",
    "# Then you can use:\n",
    "max_prompt_length = 256\n",
    "max_completion_length = 256  # 512 - 256 = 256\n",
    "\n",
    "WARNING: Longer sequences use more GPU memory. For RTX 2070 Super:\n",
    "- 128 tokens: ✅ Should work\n",
    "- 256 tokens: ⚠️  Might work with small batch size  \n",
    "- 512 tokens: ❌ Likely too much memory\n",
    "\n",
    "Test with max_seq_length = 256 first, then 512 if you have enough memory.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FimflnG2Dl4y"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XeW54d7Dl4y"
   },
   "source": [
    "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Calculate pi.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "==================================================\n",
      "🚀 Method 1: Standard Unsloth inference\n",
      "Generated response:\n",
      " with many practical applications in science and engineering. Here are a few methods to calculate π:\n",
      "\n",
      "1. **Geometric Methods**:\n",
      "   - **Circle Area**: The area of a circle can be expressed as \\( \\pi r^2 \\), where \\( r \\) is the radius of the circle.\n",
      "     - Given that the formula for the area of a circle is \\( A = \\pi r^2 \\), if you know the area of the circle, you can solve for π using \\( \\pi = \\frac{A}{r^2} \\).\n",
      "\n",
      "2. **Series Expansions**:\n",
      "   - **Leibniz's Series**: This series involves the sum of terms that alternate between 1 and 1/(4k+3)/(4k+5):\n",
      "     \\[\n",
      "     \\arctan(x) = x - \\frac{x^3}{3} + \\frac{x^5}{5} - \\frac{x^7}{7} + \\cdots \n",
      "     \\]\n",
      "     If we set \\(x=1\\), this gives:\n",
      "     \\[\n",
      "     \\arctan(1) = 1 - \\frac{1}{3} + \\frac{1}{5} - \\frac{1}{7} + \\cdots \n",
      "     \\]\n",
      "     Since \\( \\arctan(1) = \\frac{\\pi}{4} \\), we have:\n",
      "     \\[\n",
      "     \\frac{\\pi}{4} = 1 - \\frac{1}{3} + \\frac{1}{5} - \\frac{1}{7} + \\cdots\n",
      "     \\]\n",
      "     Rearranging this equation will give us the approximate value of π.\n",
      "\n",
      "3. **Random Number Generation**:\n",
      "   - **Monte Carlo Simulation**: In computer simulations involving random numbers, it's possible to estimate π using Monte Carlo methods. For example, consider drawing a square inscribed in a circle: the area of the square is \\(4\\) and the area of the circle is \\(πR^2/4\\). By generating random points within the square, if you count how many fall inside the circle, your ratio should approximate π/4.\n",
      "\n",
      "4. **Approximation Using Known Values of π**:\n",
      "   - Approximate π as a decimal or fraction until enough digits are achieved for the purpose at hand. For instance, using π ≈ 3.14159265...\n",
      "\n",
      "Given these different approaches, choosing the most appropriate method depends on what level of precision is required, whether it’s theoretical approximations or numerical approximations based on geometric shapes and series expansions. Ultimately, π remains an unsolved problem since its exact value is unknown due to the infinite nature of its representation through continued fractions.\n",
      "\n",
      "==================================================\n",
      "🚀 Method 2: Using TextStreamer for real-time output\n",
      "Streaming response:\n",
      "As an AI language model, I don't have the capability to calculate Pi as it is an irrational number and cannot be represented exactly in decimal form without infinite precision. However, you can approximate Pi using different mathematical methods such as the Gregory-Leibniz series or more advanced algorithms like the Chudnovsky algorithm.\n",
      "\n",
      "Here's an example of how you could use the Chudnovsky algorithm to approximate Pi:\n",
      "\n",
      "```\n",
      "def chudnovsky_series(n):\n",
      "    numerator = 16807 * (2**38) * math.factorial(2*n) * math.factorial(4*n)\n",
      "    denominator = pow(16*(16**5), 1/5) * math.factorial(4*n) * math.factorial(2*n)\n",
      "\n",
      "    return int(6*arctan(1/denominator))\n",
      "\n",
      "pi = 926537/2625368\n",
      "print(chudnovsky_series(pi))\n",
      "``` \n",
      "\n",
      "This function uses the Chudnovsky algorithm to compute the numerical value of Pi up to n terms in the series, where n is typically a large integer. The result is then printed to the console.\n",
      "\n",
      "Keep in mind that this method does not guarantee exactness, but rather provides an approximation within a certain level of accuracy depending on the choice of `n` parameter. It may not give a very high degree of precision for larger values of n.\n",
      "\n",
      "==================================================\n",
      "🚀 Method 3: Batch inference for multiple prompts\n",
      "Batch responses:\n",
      "\n",
      "Prompt 1: Calculate pi.\n",
      "Response: ote that there's no known method to calculate Pi exactly (π) because it is an irrational number and its value can only be approximated arbitrarily closely but not precisely. Here are a few methods commonly used to approximate π:\n",
      "\n",
      "1. **Machin-like formulas:** These involve subtracting the arctangent of certain angles from 270 degrees. One such formula is:\n",
      "   \\[ \\pi = 2 \\times (\\arctan(5892 / 6963)) - 2 \\times (\\arctan(4679 / 5303)) \\]\n",
      "\n",
      "2. **Series expansions:** These include using infinite series like those involving e or logarithms.\n",
      "\n",
      "3. **Trigonometric functions:** By evaluating trigonometric ratios in specific angles.\n",
      "\n",
      "4. **Specialized algorithms:** Such as the Bailey–Borwein–Plouffe algorithm for calculating digits of π rapidly.\n",
      "\n",
      "Here’s an example approximation using Machin-like formula:\n",
      "\\[ \\pi \\approx 2 \\cdot (\\arctan(\\frac{5892}{6963}) - \\arctan(\\frac{4679}{5303})) \\]\n",
      "This gives us approximately the value of π around 3.141592...\n",
      "\n",
      "Remember that while these methods provide increasingly accurate values, they will never reach absolute precision due to the nature of π being irrational. The more terms one includes in these series or formulas, the closer one gets to an exact value but cannot achieve it. Therefore, any approximation is limited by computational resources, including time and memory usage.\n",
      "\n",
      "Prompt 2: What is the capital of France?\n",
      "Response: l context to make it easier to understand:\n",
      "\n",
      "1. Location: Paris is located in northern France and is known as the \"City of Light\" due to its historical importance in the arts and culture.\n",
      "2. History: It has been the capital city since 1793 when the French Revolution ended the monarchy and made the city the seat of government.\n",
      "3. Importance: Paris serves as the political, financial, cultural, educational, scientific, and transportation hub for most of France. It hosts major institutions such as the Sorbonne University and UNESCO headquarters.\n",
      "4. Culture: Paris is renowned for its art, fashion, gastronomy (French cuisine), history, and architecture. Key attractions include the Eiffel Tower, Notre Dame Cathedral, Montmartre, Louvre Museum, Palace of Versailles, and many other sites along the Seine River.\n",
      "5. Population: The population of Paris is approximately 2.2 million people living within its borders.\n",
      "\n",
      "Paris continues to be an influential city not only in France but also throughout Europe and beyond, contributing significantly to tourism and international relations through its rich heritage and vibrant modernity.\n",
      "\n",
      "Prompt 3: Explain machine learning in simple terms.\n",
      "Response: ere computers learn from data and patterns without explicit programming. Essentially, it's the process by which software allows machines to automatically learn from and make predictions based on new inputs.\n",
      "\n",
      "Here’s how it works:\n",
      "\n",
      "1. **Data Collection**: First, you collect relevant data related to what you want your model to predict or understand. For example, if you’re trying to predict house prices, you would gather historical sales data for houses.\n",
      "\n",
      "2. **Feature Selection**: Next, you select important features (variables) that are crucial for making accurate predictions. Think of these as the most significant pieces of information about each house in our price prediction example.\n",
      "\n",
      "3. **Training Phase**: The model learns from the collected data through the training phase. This involves creating mathematical equations that can accurately predict outcomes based on the selected features. Imagine this as teaching a child with examples they’ve seen before to do tasks they haven’t learned explicitly yet.\n",
      "\n",
      "4. **Model Evaluation**: After training, we evaluate how well our model performs using the unseen test data. If the model predicts correctly a lot, it means it’s doing a good job. \n",
      "\n",
      "5. **Prediction**: Finally, once trained and evaluated, we use the trained model to predict values for future input data. It’s like giving a child a set of instructions and then asking them to complete a task – but now imagine being able to perform the same actions over and over again perfectly every time.\n",
      "\n",
      "So, in essence, Machine Learning allows systems to learn from experience and adjust their behavior accordingly. It’s a powerful tool used across many applications, from image recognition to financial forecasting, making decisions more efficient and precise than humans alone could achieve due to its ability to learn and adapt from data.\n",
      "\n",
      "==================================================\n",
      "Alternative: If you want vLLM-style inference\n",
      "==================================================\n",
      "\n",
      "To use vLLM-style inference (model.fast_generate), you need to:\n",
      "\n",
      "1. Load your model WITH vLLM enabled:\n",
      "   fast_inference = True  # Enable this when loading the model\n",
      "\n",
      "2. Make sure you have enough GPU memory for vLLM compilation\n",
      "\n",
      "3. Then you can use:\n",
      "\n",
      "   from vllm import SamplingParams\n",
      "\n",
      "   sampling_params = SamplingParams(\n",
      "       temperature=0.8,\n",
      "       top_p=0.95,\n",
      "       max_tokens=1024,\n",
      "   )\n",
      "\n",
      "   output = model.fast_generate(\n",
      "       [text],\n",
      "       sampling_params=sampling_params,\n",
      "       lora_request=None,\n",
      "   )[0].outputs[0].text\n",
      "\n",
      "But since you disabled vLLM due to memory constraints, \n",
      "use the standard inference methods above instead.\n",
      "\n",
      "\n",
      "==================================================\n",
      "🎯 Easy-to-use helper function:\n",
      "==================================================\n",
      "Helper function response:\n",
      "alculations or computations of any kind. If you need to calculate Pi (π), you would need to use a calculator or some other computational tool. Alternatively, if you're interested in the value of π, it is approximately 3.141592653589793... and goes on infinitely without repeating.\n"
     ]
    }
   ],
   "source": [
    "# Fixed inference code for Unsloth (without vLLM)\n",
    "import torch\n",
    "\n",
    "# Prepare the input text\n",
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\": \"user\", \"content\": \"Calculate pi.\"},\n",
    "], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(\"Input text:\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Solution 1: Standard Unsloth inference (recommended)\n",
    "print(\"🚀 Method 1: Standard Unsloth inference\")\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Enable inference mode for better performance\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Generate with standard parameters\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.8,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "# Decode the response\n",
    "full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# Extract only the generated part (remove the input prompt)\n",
    "generated_text = full_output[len(text):]\n",
    "\n",
    "print(\"Generated response:\")\n",
    "print(generated_text)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🚀 Method 2: Using TextStreamer for real-time output\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "print(\"Streaming response:\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.8,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "        streamer=text_streamer,  # This will print tokens as they're generated\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🚀 Method 3: Batch inference for multiple prompts\")\n",
    "\n",
    "# Multiple prompts\n",
    "prompts = [\n",
    "    \"Calculate pi.\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain machine learning in simple terms.\"\n",
    "]\n",
    "\n",
    "# Format all prompts\n",
    "formatted_prompts = []\n",
    "for prompt in prompts:\n",
    "    formatted_text = tokenizer.apply_chat_template([\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ], tokenize=False, add_generation_prompt=True)\n",
    "    formatted_prompts.append(formatted_text)\n",
    "\n",
    "# Batch tokenization\n",
    "batch_inputs = tokenizer(formatted_prompts, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "\n",
    "# Generate for all prompts\n",
    "with torch.no_grad():\n",
    "    batch_outputs = model.generate(\n",
    "        **batch_inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.8,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "# Decode all responses\n",
    "print(\"Batch responses:\")\n",
    "for i, (prompt, output) in enumerate(zip(formatted_prompts, batch_outputs)):\n",
    "    full_response = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    generated_response = full_response[len(prompt):]\n",
    "    print(f\"\\nPrompt {i+1}: {prompts[i]}\")\n",
    "    print(f\"Response: {generated_response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Alternative: If you want vLLM-style inference\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "To use vLLM-style inference (model.fast_generate), you need to:\n",
    "\n",
    "1. Load your model WITH vLLM enabled:\n",
    "   fast_inference = True  # Enable this when loading the model\n",
    "   \n",
    "2. Make sure you have enough GPU memory for vLLM compilation\n",
    "\n",
    "3. Then you can use:\n",
    "   \n",
    "   from vllm import SamplingParams\n",
    "   \n",
    "   sampling_params = SamplingParams(\n",
    "       temperature=0.8,\n",
    "       top_p=0.95,\n",
    "       max_tokens=1024,\n",
    "   )\n",
    "   \n",
    "   output = model.fast_generate(\n",
    "       [text],\n",
    "       sampling_params=sampling_params,\n",
    "       lora_request=None,\n",
    "   )[0].outputs[0].text\n",
    "   \n",
    "But since you disabled vLLM due to memory constraints, \n",
    "use the standard inference methods above instead.\n",
    "\"\"\")\n",
    "\n",
    "# Helper function for easy inference\n",
    "def generate_response(prompt, max_tokens=1024, temperature=0.8, top_p=0.95):\n",
    "    \"\"\"Helper function for easy inference\"\"\"\n",
    "    # Format the prompt\n",
    "    formatted_text = tokenizer.apply_chat_template([\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ], tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    \n",
    "    # Extract generated text\n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return full_output[len(formatted_text):]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🎯 Easy-to-use helper function:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test the helper function\n",
    "response = generate_response(\"Calculate pi.\", max_tokens=512)\n",
    "print(\"Helper function response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42hOFF6aDl4y"
   },
   "outputs": [],
   "source": [
    "# model.save_lora(\"grpo_saved_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Method 1: Save LoRA adapters (recommended)\n",
      "==================================================\n",
      "❌ Error saving LoRA: 'Qwen2ForCausalLM' object has no attribute 'save_lora'\n",
      "Trying alternative methods...\n",
      "\n",
      "==================================================\n",
      "🎯 Method 2: Save with tokenizer\n",
      "==================================================\n",
      "❌ Error: 'Qwen2ForCausalLM' object has no attribute 'save_lora'\n",
      "\n",
      "==================================================\n",
      "🎯 Method 3: Save to Hugging Face format\n",
      "==================================================\n",
      "✅ Hugging Face format saved to: grpo_hf_format\n",
      "\n",
      "==================================================\n",
      "🎯 Method 4: Manual LoRA saving (if built-in methods fail)\n",
      "==================================================\n",
      "❌ Manual save failed: Object of type set is not JSON serializable\n",
      "\n",
      "==================================================\n",
      "🎯 Loading saved model back\n",
      "==================================================\n",
      "\n",
      "# To load your saved LoRA model later:\n",
      "\n",
      "from unsloth import FastLanguageModel\n",
      "\n",
      "# 1. Load base model\n",
      "model, tokenizer = FastLanguageModel.from_pretrained(\n",
      "    model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
      "    max_seq_length = 128,  # Same as training\n",
      "    load_in_4bit = True,\n",
      "    fast_inference = False,  # Same as training\n",
      ")\n",
      "\n",
      "# 2. Load your trained LoRA adapters\n",
      "model.load_lora(\"grpo_saved_lora\")  # Path to your saved LoRA\n",
      "\n",
      "# 3. Enable inference mode\n",
      "FastLanguageModel.for_inference(model)\n",
      "\n",
      "# Now you can use the model for inference with your trained adapters!\n",
      "\n",
      "\n",
      "==================================================\n",
      "🎯 File size comparison\n",
      "==================================================\n",
      "Saved model sizes:\n",
      "📁 grpo_saved_lora: 0.0 MB\n",
      "📁 grpo_model_full: 0.0 MB\n",
      "📁 grpo_hf_format: 48.4 MB\n",
      "📁 grpo_manual_save: 0.0 MB\n",
      "\n",
      "🎉 Model saving complete!\n",
      "\n",
      "Recommended approach:\n",
      "- Use Method 1 (model.save_lora) for smallest files\n",
      "- Save tokenizer separately if needed\n",
      "- LoRA adapters are typically only 10-50 MB vs full model 13+ GB\n",
      "\n",
      "🧹 GPU memory cleared. Current usage: 5.26 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Method 1: Save LoRA adapters only (recommended - smallest size)\n",
    "print(\"🎯 Method 1: Save LoRA adapters (recommended)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "save_dir = \"grpo_saved_lora\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # This saves only the LoRA adapters (very small file size)\n",
    "    model.save_lora(save_dir)\n",
    "    print(f\"✅ LoRA adapters saved successfully to: {save_dir}\")\n",
    "    print(f\"📁 Files saved:\")\n",
    "    for file in os.listdir(save_dir):\n",
    "        file_path = os.path.join(save_dir, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            print(f\"   - {file} ({size_mb:.1f} MB)\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error saving LoRA: {e}\")\n",
    "    print(\"Trying alternative methods...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🎯 Method 2: Save with tokenizer\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save LoRA + tokenizer together\n",
    "full_save_dir = \"grpo_model_full\"\n",
    "os.makedirs(full_save_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Save LoRA adapters\n",
    "    model.save_lora(full_save_dir)\n",
    "    \n",
    "    # Save tokenizer separately\n",
    "    tokenizer.save_pretrained(full_save_dir)\n",
    "    \n",
    "    print(f\"✅ Model and tokenizer saved to: {full_save_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🎯 Method 3: Save to Hugging Face format\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "hf_save_dir = \"grpo_hf_format\"\n",
    "os.makedirs(hf_save_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Save in standard Hugging Face format\n",
    "    model.save_pretrained(hf_save_dir)\n",
    "    tokenizer.save_pretrained(hf_save_dir)\n",
    "    \n",
    "    print(f\"✅ Hugging Face format saved to: {hf_save_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error with HF format: {e}\")\n",
    "    \n",
    "    # Fallback: Use Unsloth's push method for local save\n",
    "    try:\n",
    "        model.push_to_hub_merged(\n",
    "            save_directory=hf_save_dir,\n",
    "            tokenizer=tokenizer,\n",
    "            save_method=\"lora\",  # or \"merged_16bit\" for full model\n",
    "        )\n",
    "        print(f\"✅ Saved using Unsloth push method to: {hf_save_dir}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"❌ Push method also failed: {e2}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🎯 Method 4: Manual LoRA saving (if built-in methods fail)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "manual_save_dir = \"grpo_manual_save\"\n",
    "os.makedirs(manual_save_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Get the PEFT model state dict\n",
    "    if hasattr(model, 'peft_config'):\n",
    "        # Save PEFT config\n",
    "        import json\n",
    "        config_dict = {}\n",
    "        for key, config in model.peft_config.items():\n",
    "            config_dict[key] = config.to_dict()\n",
    "        \n",
    "        with open(os.path.join(manual_save_dir, \"adapter_config.json\"), \"w\") as f:\n",
    "            json.dump(config_dict, f, indent=2)\n",
    "        \n",
    "        # Save adapter weights\n",
    "        model.save_pretrained(manual_save_dir)\n",
    "        tokenizer.save_pretrained(manual_save_dir)\n",
    "        \n",
    "        print(f\"✅ Manual save successful to: {manual_save_dir}\")\n",
    "    else:\n",
    "        print(\"❌ Model doesn't appear to have PEFT adapters\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Manual save failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🎯 Loading saved model back\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Example of how to load the saved LoRA model\n",
    "load_example = \"\"\"\n",
    "# To load your saved LoRA model later:\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# 1. Load base model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    max_seq_length = 128,  # Same as training\n",
    "    load_in_4bit = True,\n",
    "    fast_inference = False,  # Same as training\n",
    ")\n",
    "\n",
    "# 2. Load your trained LoRA adapters\n",
    "model.load_lora(\"grpo_saved_lora\")  # Path to your saved LoRA\n",
    "\n",
    "# 3. Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Now you can use the model for inference with your trained adapters!\n",
    "\"\"\"\n",
    "\n",
    "print(load_example)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🎯 File size comparison\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def get_dir_size(directory):\n",
    "    \"\"\"Calculate directory size in MB\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        return 0\n",
    "    \n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            total_size += os.path.getsize(filepath)\n",
    "    return total_size / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "directories_to_check = [save_dir, full_save_dir, hf_save_dir, manual_save_dir]\n",
    "\n",
    "print(\"Saved model sizes:\")\n",
    "for directory in directories_to_check:\n",
    "    if os.path.exists(directory):\n",
    "        size = get_dir_size(directory)\n",
    "        print(f\"📁 {directory}: {size:.1f} MB\")\n",
    "\n",
    "print(\"\\n🎉 Model saving complete!\")\n",
    "print(\"\\nRecommended approach:\")\n",
    "print(\"- Use Method 1 (model.save_lora) for smallest files\")  \n",
    "print(\"- Save tokenizer separately if needed\")\n",
    "print(\"- LoRA adapters are typically only 10-50 MB vs full model 13+ GB\")\n",
    "\n",
    "# Cleanup GPU memory after saving\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"\\n🧹 GPU memory cleared. Current usage: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMHsspQsDl4y"
   },
   "source": [
    "Now we load the LoRA and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading saved LoRA model...\n",
      "==================================================\n",
      "==((====))==  Unsloth 2025.8.10: Fast Mistral patching. Transformers: 4.55.4. vLLM: 0.10.1.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 2070 SUPER. Num GPUs = 1. Max memory: 8.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "❌ Failed to load base model: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Trying smaller model...\n",
      "==((====))==  Unsloth 2025.8.10: Fast Qwen2 patching. Transformers: 4.55.4. vLLM: 0.10.1.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 2070 SUPER. Num GPUs = 1. Max memory: 8.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "✅ Smaller model loaded as fallback!\n",
      "\n",
      "🎯 Loading LoRA adapters...\n",
      "❌ Method 1 failed: 401 Client Error. (Request ID: Root=1-68b612cc-536952004681f2b358dd26d0;ea8ca81d-15b7-411a-95ec-7fa02b81cb0a)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/grpo_saved_lora/resolve/main/adapter_model.bin.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n",
      "Invalid username or password.\n",
      "Trying alternative loading method...\n",
      "❌ Method 2 also failed: Can't find 'adapter_config.json' at 'grpo_saved_lora'\n",
      "⚠️  Using base model without LoRA adapters for testing...\n",
      "🚀 Model ready for inference!\n",
      "\n",
      "==================================================\n",
      "🧪 Testing the loaded model\n",
      "==================================================\n",
      "\n",
      "📝 Test 1: Calculate pi\n",
      "------------------------------\n",
      "Input prompt: Calculate pi.\n",
      "Formatted text length: 172 characters\n",
      "Input tokens: 32\n",
      "🤖 Response: ongoing challenge in mathematics. The most common methods for calculating pi involve infinite series or numerical algorithms that use mathematical expressions based on complex numbers. However, I can show you how to understand the concept behind it using basic algebra.\n",
      "\n",
      "Pi is defined as the ratio of a circle's circumference (C) to its diameter (d). Mathematically, this is expressed as:\n",
      "\n",
      "\\[ \\pi = \\frac{C}{d} \\]\n",
      "\n",
      "where C ≈ 3.14159... and d ≈ 2π\n",
      "\n",
      "To find the value of π, mathematicians have used various techniques like:\n",
      "\n",
      "1. Infinite Series: Some formulas, such as Leibniz formula or Gregory-Leibniz series, allow us to approximate π through sums of fractions.\n",
      "\n",
      "However, the most well-known method is by Archimedes:\n",
      "\n",
      "```python\n",
      "def approx_pi(n):\n",
      "    # Calculate the sum of the first n terms of Archimedes' sequence.\n",
      "    return 6 * sum(1 / ((3 - i) * sqrt((i + 0.5)**2 - (sqrt(i**2 + 0.25) - sqrt(i**2 - 0.75))**2))\n",
      "\n",
      "approx_pi(1000)\n",
      "```\n",
      "\n",
      "2. Numerical Algorithms: Various iterative algorithms converge upon π with increasing accuracy over time.\n",
      "\n",
      "It's important to note that while we cannot \"calculate\" the value of π in any traditional sense because it's an irrational number, we can compute increasingly precise approximations to it.\n",
      "\n",
      "In conclusion, while exact computation is challenging due to pi being transcendental and non-repeating, there are efficient ways to calculate π up to millions of decimal places using these and other approximation methods.\n",
      "Response length: 1455 characters\n",
      "\n",
      "📝 Test 2: Math reasoning\n",
      "------------------------------\n",
      "Input prompt: What is 15 * 23 + 7?\n",
      "Formatted text length: 179 characters\n",
      "Input tokens: 42\n",
      "🤖 Response: hese steps:\n",
      "\n",
      "1. **Multiply**: Perform the multiplication first.\n",
      "   - \\( 15 \\times 23 = 345 \\)\n",
      "\n",
      "2. **Add**: Then, add the result to 7.\n",
      "   - \\( 345 + 7 = 352 \\)\n",
      "\n",
      "So, the final answer is \\( 352 \\).\n",
      "Response length: 194 characters\n",
      "\n",
      "📝 Test 3: Complex reasoning\n",
      "------------------------------\n",
      "Input prompt: Explain the process of photosynthesis in simple terms.\n",
      "Formatted text length: 213 characters\n",
      "Input tokens: 40\n",
      "🤖 Response: ht to convert carbon dioxide from the air into glucose (a type of sugar) and oxygen. This process takes place mainly during the day when the sun provides light energy, although it also continues under very low light conditions at night. Here's how it works:\n",
      "\n",
      "1. **Light Absorption**: Chlorophyll, a green pigment found in plant cells, absorbs light energy.\n",
      "\n",
      "2. **Water Splitting**: Water molecules split into hydrogen and oxygen atoms. Oxygen gas escapes as waste products because it cannot be stored inside the plant cell for later use.\n",
      "\n",
      "3. **Energy Conversion**: The absorbed light energy is used to combine carbon dioxide (from the air) with water (also from the soil or atmosphere) to produce glucose and release oxygen.\n",
      "\n",
      "4. **Glucose Production**: Glucose is then stored in the plant as starch, ready for its immediate use or as needed.\n",
      "\n",
      "5. **Oxygen Release**: As a side product of this process, the excess oxygen produced diffuses out through tiny pores on the leaves called stomata.\n",
      "\n",
      "In summary, photosynthesis allows plants to create their own food using sunlight. It is crucial not only for growing plants but also for maintaining life on Earth through the continuous cycling of carbon and oxygen.\n",
      "Response length: 1206 characters\n",
      "\n",
      "📝 Test 4: Batch inference\n",
      "------------------------------\n",
      "\n",
      "Batch test 1: What is the capital of Japan?\n",
      "Input prompt: What is the capital of Japan?\n",
      "Formatted text length: 188 characters\n",
      "Input tokens: 36\n",
      "🤖 Response: 's largest city and its most populous urban area. It was also the former imperial capital until 1968 when that function was transferred to Sapporo.\n",
      "Some interesting facts about Tokyo:\n",
      "- It covers an area of approximately 2,170 square kilometers (835 square miles).\n",
      "- The population in Tokyo proper as of June 2021 was around 14 million people.\n",
      "- The city serves as Japan's economic hub.\n",
      "- Tokyo has many world-renowned landmarks like Shibuya Crossing, Sensoji Temple, and Asakusa.\n",
      "While some smaller cities may serve as major administrative or cultural centers, Tokyo remains at the top of the list for being both a thriving metropolitan area and the official capital of Japan.\n",
      "Response length: 677 characters\n",
      "\n",
      "Batch test 2: How does machine learning work?\n",
      "Input prompt: How does machine learning work?\n",
      "Formatted text length: 190 characters\n",
      "Input tokens: 35\n",
      "🤖 Response: that involves developing algorithms and statistical models to enable computers to learn from data without being explicitly programmed to do so. Here's how it works in general terms:\n",
      "\n",
      "1. **Data Collection**: The first step is collecting large amounts of relevant data about the task or problem you want the system to solve.\n",
      "\n",
      "2. **Feature Engineering**: Data scientists or engineers analyze the collected data to identify the most important features (variables) that will be used for making predictions or decisions. This process often involves creating new feature variables by combining existing ones.\n",
      "\n",
      "3. **Model Selection**: Depending on the type of machine learning algorithm, different approaches can be chosen. Supervised learning uses labeled data to train models, while unsupervised learning finds patterns in unlabeled data.\n",
      "\n",
      "4. **Training**: Once the model is selected, training begins. During this phase, the algorithm learns from the historical data. Through iterative optimization, the parameters of the learned function adjust based on the specific criteria defined as cost functions.\n",
      "\n",
      "5. **Testing and Validation**: After training, the model is tested using unseen test data to check its performance. The model’s accuracy and efficiency are evaluated during this stage.\n",
      "\n",
      "6. **Prediction or Decision Making**: Finally, once validated, the trained model makes predictions\n",
      "Response length: 1383 characters\n",
      "\n",
      "Batch test 3: Write a short poem about stars.\n",
      "Input prompt: Write a short poem about stars.\n",
      "Formatted text length: 190 characters\n",
      "Input tokens: 36\n",
      "🤖 Response: rifts through the sky,\n",
      "A thousand twinkling orbs shine bright,\n",
      "Stars that guide us on our journey right.\n",
      "\n",
      "They twinkle like diamonds so pure,\n",
      "In their light we find our true home,\n",
      "In each other's eyes, they meet and glow,\n",
      "In the darkness, they're our constant hold.\n",
      "\n",
      "The world may fade in the morning light,\n",
      "But our memories stay with us to sing,\n",
      "Of days when the stars did not hide,\n",
      "And our hearts were light as the stars' fire.\n",
      "Response length: 429 characters\n",
      "\n",
      "==================================================\n",
      "🎉 Model testing complete!\n",
      "==================================================\n",
      "GPU Memory - Allocated: 6.23 GB, Reserved: 6.40 GB\n",
      "\n",
      "🔍 Model Analysis:\n",
      "- Model type: PeftModelForCausalLM\n",
      "- Has PEFT adapters: True\n",
      "- Active adapters: ['default']\n",
      "- Device: cuda:0\n",
      "- Tokenizer vocab size: 151665\n",
      "\n",
      "💡 Notes:\n",
      "- If responses look different from base model, LoRA training worked!\n",
      "- If responses are similar to base model, LoRA might not have loaded\n",
      "- Check for any special formatting patterns from your GRPO reward functions\n",
      "- Compare responses to base model (without LoRA) to see the difference\n",
      "\n",
      "📄 Test results saved to 'model_test_results.txt'\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Clear GPU memory first\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"🔄 Loading saved LoRA model...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Step 1: Load the base model (same configuration as training)\n",
    "max_seq_length = 128  # Same as used during training\n",
    "lora_rank = 16  # Same as used during training\n",
    "\n",
    "try:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",  # Same base model\n",
    "        max_seq_length = max_seq_length,\n",
    "        load_in_4bit = True,\n",
    "        fast_inference = False,  # Same as training\n",
    "        max_lora_rank = lora_rank,\n",
    "        dtype = torch.float16,\n",
    "    )\n",
    "    print(\"✅ Base model loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to load base model: {e}\")\n",
    "    print(\"Trying smaller model...\")\n",
    "    \n",
    "    # Fallback to smaller model if the 7B doesn't fit\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit\",\n",
    "        max_seq_length = max_seq_length,\n",
    "        load_in_4bit = True,\n",
    "        fast_inference = False,\n",
    "        max_lora_rank = 32,\n",
    "        dtype = torch.float16,\n",
    "    )\n",
    "    print(\"✅ Smaller model loaded as fallback!\")\n",
    "\n",
    "# Step 2: Load your trained LoRA adapters\n",
    "print(\"\\n🎯 Loading LoRA adapters...\")\n",
    "\n",
    "try:\n",
    "    # Method 1: Direct LoRA loading (if the model supports it)\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r = lora_rank,\n",
    "        target_modules = [\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        ],\n",
    "        lora_alpha = lora_rank,\n",
    "        use_gradient_checkpointing = \"unsloth\",\n",
    "        random_state = 3407,\n",
    "    )\n",
    "    \n",
    "    # Load the saved LoRA weights\n",
    "    model.load_adapter(\"grpo_saved_lora\", adapter_name=\"default\")\n",
    "    print(\"✅ LoRA adapters loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Method 1 failed: {e}\")\n",
    "    print(\"Trying alternative loading method...\")\n",
    "    \n",
    "    try:\n",
    "        # Method 2: Load using from_pretrained with adapter\n",
    "        from peft import PeftModel\n",
    "        model = PeftModel.from_pretrained(model, \"grpo_saved_lora\")\n",
    "        print(\"✅ LoRA loaded using PEFT!\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"❌ Method 2 also failed: {e2}\")\n",
    "        print(\"⚠️  Using base model without LoRA adapters for testing...\")\n",
    "\n",
    "# Step 3: Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "print(\"🚀 Model ready for inference!\")\n",
    "\n",
    "# Step 4: Set up system prompt and test\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant. Provide clear, accurate, and concise responses.\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🧪 Testing the loaded model\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def test_model(prompt, system_prompt=SYSTEM_PROMPT, max_tokens=512):\n",
    "    \"\"\"Test function for the loaded model\"\"\"\n",
    "    \n",
    "    # Format the conversation\n",
    "    if system_prompt:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    else:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Input prompt: {prompt}\")\n",
    "    print(f\"Formatted text length: {len(text)} characters\")\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    print(f\"Input tokens: {inputs['input_ids'].shape[1]}\")\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.8,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    \n",
    "    # Decode the full output\n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the generated part\n",
    "    generated_response = full_output[len(text):].strip()\n",
    "    \n",
    "    print(f\"🤖 Response: {generated_response}\")\n",
    "    print(f\"Response length: {len(generated_response)} characters\")\n",
    "    \n",
    "    return generated_response\n",
    "\n",
    "# Test 1: Original test case\n",
    "print(\"\\n📝 Test 1: Calculate pi\")\n",
    "print(\"-\" * 30)\n",
    "response1 = test_model(\"Calculate pi.\")\n",
    "\n",
    "# Test 2: Different prompt to see if training worked\n",
    "print(\"\\n📝 Test 2: Math reasoning\")\n",
    "print(\"-\" * 30)\n",
    "response2 = test_model(\"What is 15 * 23 + 7?\")\n",
    "\n",
    "# Test 3: Check if model follows any special formatting from GRPO training\n",
    "print(\"\\n📝 Test 3: Complex reasoning\")\n",
    "print(\"-\" * 30)\n",
    "response3 = test_model(\"Explain the process of photosynthesis in simple terms.\")\n",
    "\n",
    "# Test 4: Batch inference\n",
    "print(\"\\n📝 Test 4: Batch inference\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "test_prompts = [\n",
    "    \"What is the capital of Japan?\",\n",
    "    \"How does machine learning work?\",\n",
    "    \"Write a short poem about stars.\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nBatch test {i}: {prompt}\")\n",
    "    response = test_model(prompt, max_tokens=256)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🎉 Model testing complete!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Memory usage info\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    print(f\"GPU Memory - Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "\n",
    "print(\"\\n🔍 Model Analysis:\")\n",
    "print(f\"- Model type: {type(model).__name__}\")\n",
    "print(f\"- Has PEFT adapters: {hasattr(model, 'peft_config')}\")\n",
    "if hasattr(model, 'peft_config'):\n",
    "    print(f\"- Active adapters: {list(model.peft_config.keys())}\")\n",
    "print(f\"- Device: {next(model.parameters()).device}\")\n",
    "print(f\"- Tokenizer vocab size: {len(tokenizer)}\")\n",
    "\n",
    "print(\"\\n💡 Notes:\")\n",
    "print(\"- If responses look different from base model, LoRA training worked!\")\n",
    "print(\"- If responses are similar to base model, LoRA might not have loaded\")\n",
    "print(\"- Check for any special formatting patterns from your GRPO reward functions\")\n",
    "print(\"- Compare responses to base model (without LoRA) to see the difference\")\n",
    "\n",
    "# Optional: Save a test log\n",
    "test_log = f\"\"\"\n",
    "Model Test Results\n",
    "==================\n",
    "Model: Mistral-7B with LoRA adapters\n",
    "Test Date: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\n",
    "\n",
    "Test 1 - Calculate pi: {response1[:100]}...\n",
    "Test 2 - Math: {response2[:100]}...\n",
    "Test 3 - Science: {response3[:100]}...\n",
    "\n",
    "Memory Usage: {allocated:.2f} GB GPU\n",
    "\"\"\"\n",
    "\n",
    "with open(\"model_test_results.txt\", \"w\") as f:\n",
    "    f.write(test_log)\n",
    "\n",
    "print(\"\\n📄 Test results saved to 'model_test_results.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "print(\"🚀 Saving Model for vLLM (Float16)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Clear GPU memory before saving\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check available space (merged models are large!)\n",
    "def check_disk_space():\n",
    "    import shutil\n",
    "    total, used, free = shutil.disk_usage(\".\")\n",
    "    free_gb = free / (1024**3)\n",
    "    print(f\"💾 Available disk space: {free_gb:.1f} GB\")\n",
    "    if free_gb < 15:  # Mistral-7B merged is ~13GB\n",
    "        print(\"⚠️  WARNING: Low disk space! Merged model needs ~13GB\")\n",
    "    return free_gb\n",
    "\n",
    "free_space = check_disk_space()\n",
    "\n",
    "print(\"\\n🎯 Option 1: Save merged model in float16 (RECOMMENDED for vLLM)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Merge LoRA into base model and save in float16 - BEST for vLLM\n",
    "if True:  # Changed from False to True\n",
    "    try:\n",
    "        save_path = \"model_merged_16bit\"\n",
    "        print(f\"Saving merged float16 model to: {save_path}\")\n",
    "        print(\"This will take a few minutes...\")\n",
    "        \n",
    "        model.save_pretrained_merged(\n",
    "            save_path, \n",
    "            tokenizer, \n",
    "            save_method=\"merged_16bit\",  # Float16 format - perfect for vLLM\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Float16 merged model saved successfully!\")\n",
    "        \n",
    "        # Check saved model size\n",
    "        def get_dir_size(directory):\n",
    "            total_size = 0\n",
    "            for dirpath, dirnames, filenames in os.walk(directory):\n",
    "                for filename in filenames:\n",
    "                    filepath = os.path.join(dirpath, filename)\n",
    "                    if os.path.isfile(filepath):\n",
    "                        total_size += os.path.getsize(filepath)\n",
    "            return total_size / (1024**3)  # GB\n",
    "        \n",
    "        size_gb = get_dir_size(save_path)\n",
    "        print(f\"📁 Model size: {size_gb:.1f} GB\")\n",
    "        \n",
    "        # List saved files\n",
    "        print(\"📄 Saved files:\")\n",
    "        for file in os.listdir(save_path):\n",
    "            file_path = os.path.join(save_path, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "                print(f\"   - {file} ({size_mb:.1f} MB)\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving merged float16 model: {e}\")\n",
    "        print(\"Possible causes:\")\n",
    "        print(\"- Insufficient disk space\")\n",
    "        print(\"- Insufficient GPU memory\") \n",
    "        print(\"- Model not properly trained\")\n",
    "\n",
    "print(\"\\n🎯 Option 2: Push merged model to Hugging Face Hub\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Push to Hugging Face Hub in float16\n",
    "if False:  # Set to True and add your token to enable\n",
    "    try:\n",
    "        hf_repo_name = \"your-username/your-model-name\"  # Change this!\n",
    "        hf_token = \"your_hf_token_here\"  # Add your HF token!\n",
    "        \n",
    "        print(f\"Pushing to Hugging Face: {hf_repo_name}\")\n",
    "        \n",
    "        model.push_to_hub_merged(\n",
    "            hf_repo_name, \n",
    "            tokenizer, \n",
    "            save_method=\"merged_16bit\", \n",
    "            token=hf_token\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Model pushed to Hugging Face Hub!\")\n",
    "        print(f\"🌐 Available at: https://huggingface.co/{hf_repo_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error pushing to hub: {e}\")\n",
    "\n",
    "print(\"\\n🎯 Option 3: Save merged model in 4bit (smaller but less compatible)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Merge to 4bit (smaller file, but may not work with all vLLM versions)\n",
    "if False:  # Set to True if you want 4bit version too\n",
    "    try:\n",
    "        save_path_4bit = \"model_merged_4bit\"\n",
    "        print(f\"Saving merged 4bit model to: {save_path_4bit}\")\n",
    "        \n",
    "        model.save_pretrained_merged(\n",
    "            save_path_4bit, \n",
    "            tokenizer, \n",
    "            save_method=\"merged_4bit\",\n",
    "        )\n",
    "        \n",
    "        print(\"✅ 4bit merged model saved!\")\n",
    "        size_gb = get_dir_size(save_path_4bit)\n",
    "        print(f\"📁 4bit model size: {size_gb:.1f} GB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving 4bit model: {e}\")\n",
    "\n",
    "print(\"\\n🎯 Option 4: Save just LoRA adapters (smallest, for loading later)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Just save LoRA adapters (what you had before)\n",
    "if True:  # This is your existing LoRA save\n",
    "    try:\n",
    "        lora_path = \"model_lora_only\"\n",
    "        \n",
    "        model.save_pretrained(lora_path)\n",
    "        tokenizer.save_pretrained(lora_path)\n",
    "        \n",
    "        print(\"✅ LoRA adapters saved!\")\n",
    "        size_mb = get_dir_size(lora_path) * 1024  # Convert to MB\n",
    "        print(f\"📁 LoRA size: {size_mb:.1f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving LoRA: {e}\")\n",
    "\n",
    "print(\"\\n🎯 Option 5: Push LoRA to Hugging Face Hub\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Push just LoRA adapters to hub\n",
    "if False:  # Set to True and configure to enable\n",
    "    try:\n",
    "        hf_lora_repo = \"your-username/your-lora-adapters\"  # Change this!\n",
    "        hf_token = \"your_hf_token_here\"  # Add your HF token!\n",
    "        \n",
    "        model.push_to_hub(hf_lora_repo, token=hf_token)\n",
    "        tokenizer.push_to_hub(hf_lora_repo, token=hf_token)\n",
    "        \n",
    "        print(\"✅ LoRA adapters pushed to Hugging Face Hub!\")\n",
    "        print(f\"🌐 Available at: https://huggingface.co/{hf_lora_repo}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error pushing LoRA to hub: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🎯 How to use your saved models\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "vllm_usage_guide = '''\n",
    "# For vLLM inference with your float16 merged model:\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Load your merged model with vLLM\n",
    "llm = LLM(\n",
    "    model=\"./model_merged_16bit\",  # Path to your saved model\n",
    "    gpu_memory_utilization=0.8,\n",
    "    max_model_len=2048,  # Adjust based on your needs\n",
    "    dtype=\"float16\"  # Explicitly use float16\n",
    ")\n",
    "\n",
    "# Set up sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_tokens=512,\n",
    ")\n",
    "\n",
    "# Format your prompt\n",
    "prompt = tokenizer.apply_chat_template([\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Calculate pi.\"}\n",
    "], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Generate\n",
    "outputs = llm.generate([prompt], sampling_params)\n",
    "response = outputs[0].outputs[0].text\n",
    "\n",
    "print(response)\n",
    "'''\n",
    "\n",
    "print(vllm_usage_guide)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"📋 Summary of what was saved:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "saved_models = []\n",
    "\n",
    "if os.path.exists(\"model_merged_16bit\"):\n",
    "    size = get_dir_size(\"model_merged_16bit\")\n",
    "    saved_models.append(f\"✅ Float16 merged model: model_merged_16bit ({size:.1f} GB) - BEST for vLLM\")\n",
    "\n",
    "if os.path.exists(\"model_merged_4bit\"):\n",
    "    size = get_dir_size(\"model_merged_4bit\") \n",
    "    saved_models.append(f\"✅ 4bit merged model: model_merged_4bit ({size:.1f} GB)\")\n",
    "\n",
    "if os.path.exists(\"model_lora_only\"):\n",
    "    size = get_dir_size(\"model_lora_only\") * 1024  # MB\n",
    "    saved_models.append(f\"✅ LoRA adapters only: model_lora_only ({size:.1f} MB)\")\n",
    "\n",
    "if saved_models:\n",
    "    for model_info in saved_models:\n",
    "        print(model_info)\n",
    "else:\n",
    "    print(\"❌ No models were saved successfully\")\n",
    "\n",
    "print(f\"\\n💾 Total disk space used: ~{sum([get_dir_size(d) for d in ['model_merged_16bit', 'model_merged_4bit'] if os.path.exists(d)]):.1f} GB\")\n",
    "\n",
    "print(\"\\n🎯 Recommendations:\")\n",
    "print(\"- Use 'model_merged_16bit' for vLLM inference (best compatibility)\")  \n",
    "print(\"- Keep 'model_lora_only' as backup (small size)\")\n",
    "print(\"- Test vLLM loading before deleting any files\")\n",
    "print(\"- Float16 merged model will work with vLLM out of the box!\")\n",
    "\n",
    "# Clear GPU memory after saving\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"\\n🧹 GPU memory cleared. Available: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0)) / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCv4vXHd61i7"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "print(\"🦙 GGUF / llama.cpp Model Conversion\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clear GPU memory before conversion\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check available disk space (GGUF files can be large)\n",
    "def check_disk_space():\n",
    "    import shutil\n",
    "    total, used, free = shutil.disk_usage(\".\")\n",
    "    free_gb = free / (1024**3)\n",
    "    print(f\"💾 Available disk space: {free_gb:.1f} GB\")\n",
    "    \n",
    "    # Estimate space needed for different formats\n",
    "    print(\"📊 Estimated space requirements:\")\n",
    "    print(\"   - Q8_0 (8-bit): ~7-8 GB\")\n",
    "    print(\"   - F16 (16-bit): ~13-14 GB\") \n",
    "    print(\"   - Q4_K_M (4-bit): ~4-5 GB\")\n",
    "    print(\"   - All formats: ~25-30 GB total\")\n",
    "    \n",
    "    if free_gb < 30:\n",
    "        print(\"⚠️  WARNING: Consider saving formats one at a time if space is limited\")\n",
    "    \n",
    "    return free_gb\n",
    "\n",
    "free_space = check_disk_space()\n",
    "\n",
    "def get_file_size(filepath):\n",
    "    \"\"\"Get file size in GB\"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        size_bytes = os.path.getsize(filepath)\n",
    "        return size_bytes / (1024**3)\n",
    "    return 0\n",
    "\n",
    "def save_gguf_with_timing(save_func, format_name, estimated_time=\"5-10 minutes\"):\n",
    "    \"\"\"Helper to save GGUF with timing and error handling\"\"\"\n",
    "    print(f\"\\n🔄 Converting to {format_name}...\")\n",
    "    print(f\"⏱️  Estimated time: {estimated_time}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        save_func()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✅ {format_name} conversion completed in {elapsed/60:.1f} minutes!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"❌ {format_name} conversion failed after {elapsed/60:.1f} minutes\")\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎯 GGUF Conversion Options\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Conversion results tracking\n",
    "conversion_results = {}\n",
    "\n",
    "# Option 1: Save to 8bit Q8_0 (High quality, medium size)\n",
    "print(\"\\n📦 Option 1: Q8_0 (8-bit quantization)\")\n",
    "print(\"-\" * 40)\n",
    "print(\"✨ Features: High quality, good for most use cases\")\n",
    "print(\"📁 Size: ~7-8 GB\")\n",
    "print(\"🎯 Best for: General purpose, balanced quality/size\")\n",
    "\n",
    "if True:  # Changed from False to True\n",
    "    def save_q8_0():\n",
    "        model.save_pretrained_gguf(\n",
    "            \"model_q8_0\", \n",
    "            tokenizer,\n",
    "            # quantization_method defaults to \"q8_0\"\n",
    "        )\n",
    "    \n",
    "    success = save_gguf_with_timing(save_q8_0, \"Q8_0\", \"5-8 minutes\")\n",
    "    conversion_results[\"Q8_0\"] = success\n",
    "    \n",
    "    if success:\n",
    "        size = get_file_size(\"model_q8_0/model-unsloth.Q8_0.gguf\")\n",
    "        print(f\"📁 Q8_0 file size: {size:.1f} GB\")\n",
    "\n",
    "# Option 2: Save to 16bit GGUF (Highest quality, largest size)  \n",
    "print(\"\\n📦 Option 2: F16 (16-bit full precision)\")\n",
    "print(\"-\" * 40)\n",
    "print(\"✨ Features: Highest quality, no quantization loss\")\n",
    "print(\"📁 Size: ~13-14 GB\")\n",
    "print(\"🎯 Best for: Maximum quality, research, reference\")\n",
    "\n",
    "if True:  # Changed from False to True\n",
    "    def save_f16():\n",
    "        model.save_pretrained_gguf(\n",
    "            \"model_f16\", \n",
    "            tokenizer, \n",
    "            quantization_method=\"f16\"\n",
    "        )\n",
    "    \n",
    "    success = save_gguf_with_timing(save_f16, \"F16\", \"8-12 minutes\")\n",
    "    conversion_results[\"F16\"] = success\n",
    "    \n",
    "    if success:\n",
    "        size = get_file_size(\"model_f16/model-unsloth.F16.gguf\")\n",
    "        print(f\"📁 F16 file size: {size:.1f} GB\")\n",
    "\n",
    "# Option 3: Save to q4_k_m GGUF (Good quality, smallest size)\n",
    "print(\"\\n📦 Option 3: Q4_K_M (4-bit quantization)\")\n",
    "print(\"-\" * 40)\n",
    "print(\"✨ Features: Good quality, smallest size, fastest inference\")\n",
    "print(\"📁 Size: ~4-5 GB\") \n",
    "print(\"🎯 Best for: Resource-constrained devices, mobile deployment\")\n",
    "\n",
    "if True:  # Changed from False to True\n",
    "    def save_q4_k_m():\n",
    "        model.save_pretrained_gguf(\n",
    "            \"model_q4_k_m\", \n",
    "            tokenizer, \n",
    "            quantization_method=\"q4_k_m\"\n",
    "        )\n",
    "    \n",
    "    success = save_gguf_with_timing(save_q4_k_m, \"Q4_K_M\", \"3-6 minutes\")\n",
    "    conversion_results[\"Q4_K_M\"] = success\n",
    "    \n",
    "    if success:\n",
    "        size = get_file_size(\"model_q4_k_m/model-unsloth.Q4_K_M.gguf\")\n",
    "        print(f\"📁 Q4_K_M file size: {size:.1f} GB\")\n",
    "\n",
    "# Option 4: Additional quantization formats (advanced)\n",
    "print(\"\\n📦 Option 4: Additional GGUF formats\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "additional_formats = {\n",
    "    \"q5_k_m\": {\"desc\": \"5-bit, balanced quality/size\", \"size\": \"~5-6 GB\"},\n",
    "    \"q6_k\": {\"desc\": \"6-bit, high quality\", \"size\": \"~6-7 GB\"}, \n",
    "    \"q4_0\": {\"desc\": \"4-bit, fastest inference\", \"size\": \"~4 GB\"},\n",
    "    \"q5_0\": {\"desc\": \"5-bit, good balance\", \"size\": \"~5 GB\"},\n",
    "}\n",
    "\n",
    "save_additional = False  # Set to True to enable additional formats\n",
    "\n",
    "if save_additional:\n",
    "    for fmt, info in additional_formats.items():\n",
    "        print(f\"\\n🔄 Converting to {fmt.upper()} ({info['desc']}, {info['size']})\")\n",
    "        \n",
    "        def save_additional_format():\n",
    "            model.save_pretrained_gguf(\n",
    "                f\"model_{fmt}\", \n",
    "                tokenizer, \n",
    "                quantization_method=fmt\n",
    "            )\n",
    "        \n",
    "        success = save_gguf_with_timing(save_additional_format, fmt.upper(), \"3-8 minutes\")\n",
    "        conversion_results[fmt.upper()] = success\n",
    "\n",
    "# Option 5: Push to Hugging Face Hub\n",
    "print(\"\\n🌐 Hugging Face Hub Upload\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "upload_to_hub = False  # Set to True and configure to enable uploads\n",
    "\n",
    "if upload_to_hub:\n",
    "    hf_token = \"your_hf_token_here\"  # Replace with your token\n",
    "    hf_repo_base = \"your-username/your-model-name\"  # Replace with your repo\n",
    "    \n",
    "    # Upload Q8_0 to hub\n",
    "    if conversion_results.get(\"Q8_0\", False):\n",
    "        try:\n",
    "            print(\"🚀 Uploading Q8_0 to Hugging Face Hub...\")\n",
    "            model.push_to_hub_gguf(\n",
    "                f\"{hf_repo_base}-Q8-GGUF\", \n",
    "                tokenizer, \n",
    "                token=hf_token\n",
    "            )\n",
    "            print(f\"✅ Q8_0 uploaded to: https://huggingface.co/{hf_repo_base}-Q8-GGUF\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Q8_0 upload failed: {e}\")\n",
    "    \n",
    "    # Upload F16 to hub\n",
    "    if conversion_results.get(\"F16\", False):\n",
    "        try:\n",
    "            print(\"🚀 Uploading F16 to Hugging Face Hub...\")\n",
    "            model.push_to_hub_gguf(\n",
    "                f\"{hf_repo_base}-F16-GGUF\", \n",
    "                tokenizer, \n",
    "                quantization_method=\"f16\", \n",
    "                token=hf_token\n",
    "            )\n",
    "            print(f\"✅ F16 uploaded to: https://huggingface.co/{hf_repo_base}-F16-GGUF\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ F16 upload failed: {e}\")\n",
    "    \n",
    "    # Upload Q4_K_M to hub  \n",
    "    if conversion_results.get(\"Q4_K_M\", False):\n",
    "        try:\n",
    "            print(\"🚀 Uploading Q4_K_M to Hugging Face Hub...\")\n",
    "            model.push_to_hub_gguf(\n",
    "                f\"{hf_repo_base}-Q4-GGUF\", \n",
    "                tokenizer, \n",
    "                quantization_method=\"q4_k_m\", \n",
    "                token=hf_token\n",
    "            )\n",
    "            print(f\"✅ Q4_K_M uploaded to: https://huggingface.co/{hf_repo_base}-Q4-GGUF\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Q4_K_M upload failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📋 CONVERSION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_size = 0\n",
    "successful_conversions = []\n",
    "\n",
    "for format_name, success in conversion_results.items():\n",
    "    if success:\n",
    "        # Find the corresponding directory and file\n",
    "        format_dirs = {\n",
    "            \"Q8_0\": \"model_q8_0\",\n",
    "            \"F16\": \"model_f16\", \n",
    "            \"Q4_K_M\": \"model_q4_k_m\"\n",
    "        }\n",
    "        \n",
    "        if format_name in format_dirs:\n",
    "            dir_path = format_dirs[format_name]\n",
    "            if os.path.exists(dir_path):\n",
    "                # Calculate directory size\n",
    "                dir_size = 0\n",
    "                for dirpath, dirnames, filenames in os.walk(dir_path):\n",
    "                    for filename in filenames:\n",
    "                        filepath = os.path.join(dirpath, filename)\n",
    "                        dir_size += os.path.getsize(filepath)\n",
    "                \n",
    "                dir_size_gb = dir_size / (1024**3)\n",
    "                total_size += dir_size_gb\n",
    "                \n",
    "                print(f\"✅ {format_name}: {dir_path}/ ({dir_size_gb:.1f} GB)\")\n",
    "                successful_conversions.append(format_name)\n",
    "            else:\n",
    "                print(f\"⚠️  {format_name}: Conversion reported success but directory not found\")\n",
    "    else:\n",
    "        print(f\"❌ {format_name}: Conversion failed\")\n",
    "\n",
    "print(f\"\\n📊 Total disk space used: {total_size:.1f} GB\")\n",
    "print(f\"🎯 Successful conversions: {len(successful_conversions)}/{len(conversion_results)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🦙 HOW TO USE WITH LLAMA.CPP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "llama_cpp_guide = '''\n",
    "# Install llama.cpp (if not already installed)\n",
    "git clone https://github.com/ggerganov/llama.cpp.git\n",
    "cd llama.cpp\n",
    "make\n",
    "\n",
    "# Run inference with your GGUF models:\n",
    "\n",
    "# Q4_K_M (fastest, smallest)\n",
    "./main -m ./model_q4_k_m/model-unsloth.Q4_K_M.gguf -n 512 -p \"Calculate pi.\"\n",
    "\n",
    "# Q8_0 (balanced)\n",
    "./main -m ./model_q8_0/model-unsloth.Q8_0.gguf -n 512 -p \"Calculate pi.\"\n",
    "\n",
    "# F16 (highest quality)\n",
    "./main -m ./model_f16/model-unsloth.F16.gguf -n 512 -p \"Calculate pi.\"\n",
    "\n",
    "# Interactive chat mode\n",
    "./main -m ./model_q4_k_m/model-unsloth.Q4_K_M.gguf -n 512 -i\n",
    "\n",
    "# With custom system prompt\n",
    "./main -m ./model_q4_k_m/model-unsloth.Q4_K_M.gguf -n 512 \\\\\n",
    "  --system-prompt-file system_prompt.txt -p \"Your question here\"\n",
    "\n",
    "# Python binding (llama-cpp-python)\n",
    "pip install llama-cpp-python\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(model_path=\"./model_q4_k_m/model-unsloth.Q4_K_M.gguf\")\n",
    "output = llm(\"Calculate pi.\", max_tokens=512)\n",
    "print(output['choices'][0]['text'])\n",
    "'''\n",
    "\n",
    "print(llama_cpp_guide)\n",
    "\n",
    "print(\"\\n🎯 FORMAT RECOMMENDATIONS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"🥇 Q4_K_M: Best for most users (good quality, fast, small)\")\n",
    "print(\"🥈 Q8_0: Best for quality-conscious users (high quality, medium size)\")  \n",
    "print(\"🥉 F16: Best for researchers (maximum quality, large size)\")\n",
    "\n",
    "print(f\"\\n🧹 Cleaning up...\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "remaining_space = check_disk_space()\n",
    "print(f\"💾 Remaining disk space: {remaining_space:.1f} GB\")\n",
    "\n",
    "print(\"\\n🎉 GGUF conversion complete!\")\n",
    "print(\"Your models are ready for llama.cpp inference! 🦙\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "11f2e88894554db6ac79b9eb48cfb03e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15801ee329284588b135df9a42503fe8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "168739a5f97e45cdafd21bcc910c4747": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac4fe3bcc7a84a57965654e6d3280535",
      "placeholder": "​",
      "style": "IPY_MODEL_aeaa5cd10af64255808914388b69640a",
      "value": " 111/111 [00:00&lt;00:00, 6.28kB/s]"
     }
    },
    "17434aad34354304bfebe580666e3462": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1846b281f52a4bf5bd9806e489e1c1b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7f6869b83324fae80ff8ba2cabe44a8",
      "max": 4138270819,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c330633ebd8f471ba11d447677c30461",
      "value": 4138270819
     }
    },
    "187f0a75f86b443bb495ec9942ed5818": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1beca53176884284884ed11cdc98e8f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c5fa2cfdd9e4c0cbd65084bd921e4fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2fecd04c3b14b58994632241a53b3a9",
      "max": 111,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b5e3be4431b844859ea858439f8fa704",
      "value": 111
     }
    },
    "1d2a2594395f49fc804426f88a5482ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "235805f996234b19979e945dd50232b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_665c857c300c4824bffe8de0f756150e",
       "IPY_MODEL_6c2f42471d8748fbaf210c8e5eb631d5",
       "IPY_MODEL_c5bf032c18114d8d979462a014a86904"
      ],
      "layout": "IPY_MODEL_f006e1a7dcd4457883317dc2a7bae2ee"
     }
    },
    "239f3de9262746e4a1420a324a9797be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "25817548b317469a8c9388d862abd7e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25f5c4ca3bf84282b24467b592711f4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5e993952883f45f5996e7a66075a8d0e",
       "IPY_MODEL_1c5fa2cfdd9e4c0cbd65084bd921e4fb",
       "IPY_MODEL_168739a5f97e45cdafd21bcc910c4747"
      ],
      "layout": "IPY_MODEL_1beca53176884284884ed11cdc98e8f3"
     }
    },
    "268c2d57c569442fad9e4453aaebd10d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ffc226846234c6ca3cdf82fb8657102",
      "placeholder": "​",
      "style": "IPY_MODEL_d08b585b518c45aa9042093516815301",
      "value": "tokenizer.json: 100%"
     }
    },
    "2a3e59cf65a94dc9acfb75d748b49c0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2c9c53c4dbe142ddb89b2975909a1bfa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33f9c4e0007346a3b1273f1466716e2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3a1a83a2ea6a467db57eacbfb4425b23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f964a6e5c00411886f8b8c4722616fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_187f0a75f86b443bb495ec9942ed5818",
      "placeholder": "​",
      "style": "IPY_MODEL_15801ee329284588b135df9a42503fe8",
      "value": " 137k/137k [00:00&lt;00:00, 6.79MB/s]"
     }
    },
    "3feb363cb1244ad4b5429dd9eed1f25f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8dbc19ec9b274c7cbb158d6c35d70aca",
      "max": 560,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_239f3de9262746e4a1420a324a9797be",
      "value": 560
     }
    },
    "41a652eae0ed4dea825d4344b5c099d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c7ec87406ad4e64b807a76974d13075",
      "max": 1961691,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cb32c31485154fa384a0c9a08b66dce0",
      "value": 1961691
     }
    },
    "47cda6240eaa4fc19d54ad7edeb99601": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bcee8f82d16048baa59d7b276d052839",
      "max": 137227,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2a3e59cf65a94dc9acfb75d748b49c0b",
      "value": 137227
     }
    },
    "48af25136e2240f5ab3d06ea380e78ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c7ec87406ad4e64b807a76974d13075": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "586597214350493686a45b291a8c61ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5bb31c10a7a647b089d6e9d66695a0aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f43e398d0b4431991b2522f87a7f8b6",
      "placeholder": "​",
      "style": "IPY_MODEL_fc1daa9a3449457da954666c442c3bc3",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "5d17e8c65e164d8d9da5f9ebba5f3c36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_faeeef94d9764ff9ac3d7132ff4ad4cb",
      "placeholder": "​",
      "style": "IPY_MODEL_e2b0eed32b584775849bd7df29f9f6ad",
      "value": "model.safetensors: 100%"
     }
    },
    "5e993952883f45f5996e7a66075a8d0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e94299a2c3eb45e789aa3ad9f2ff3474",
      "placeholder": "​",
      "style": "IPY_MODEL_33f9c4e0007346a3b1273f1466716e2c",
      "value": "generation_config.json: 100%"
     }
    },
    "5f9999b1cc53442e8641624ed3620eb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "665c857c300c4824bffe8de0f756150e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2dc398344e74ed5b4820e60ff313698",
      "placeholder": "​",
      "style": "IPY_MODEL_7248e157eff645328de943752f9c3607",
      "value": "tokenizer.model: 100%"
     }
    },
    "6c2f42471d8748fbaf210c8e5eb631d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d2a2594395f49fc804426f88a5482ce",
      "max": 587404,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5f9999b1cc53442e8641624ed3620eb6",
      "value": 587404
     }
    },
    "6ec55250004c4c89885b1bbe98009604": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a7cbf77562c471fab87ff2a8a976e41",
      "placeholder": "​",
      "style": "IPY_MODEL_ed19617454ad4bc2ac608b25ffbfb729",
      "value": "config.json: 100%"
     }
    },
    "7248e157eff645328de943752f9c3607": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "73faadabada94be18eb229aeb88a667d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a7cbf77562c471fab87ff2a8a976e41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f43e398d0b4431991b2522f87a7f8b6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "807541d716a04917994d0da3cde130a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87e2e864d69f4710b0e625f2c91dc481": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb9bf6910bd2408189c0c99c896e527c",
      "placeholder": "​",
      "style": "IPY_MODEL_f8a17b835d964b6fae28db8c2394b77c",
      "value": " 4.14G/4.14G [03:10&lt;00:00, 22.3MB/s]"
     }
    },
    "8dbc19ec9b274c7cbb158d6c35d70aca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90372c83e1554e98ac476e35630bec76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9611e68a858a435aa1eb4a5bb315d788": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "96c4b90f4965477d80ed2f5af8340887": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_268c2d57c569442fad9e4453aaebd10d",
       "IPY_MODEL_41a652eae0ed4dea825d4344b5c099d5",
       "IPY_MODEL_ce20f204ab3e4687920157111aee2d1a"
      ],
      "layout": "IPY_MODEL_11f2e88894554db6ac79b9eb48cfb03e"
     }
    },
    "9a478c0676c84afb8605188010b48c6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ff0a1ae7050049e9ac2ababd1c8a30a7",
       "IPY_MODEL_3feb363cb1244ad4b5429dd9eed1f25f",
       "IPY_MODEL_edcbb5a711174fdc93b5347d08ac892c"
      ],
      "layout": "IPY_MODEL_807541d716a04917994d0da3cde130a5"
     }
    },
    "9a8bfb49004e44e18c7c12b38f9f1f12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9ffc226846234c6ca3cdf82fb8657102": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a38dc87bc9424a88b35a1ec195b80087": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_48af25136e2240f5ab3d06ea380e78ba",
      "placeholder": "​",
      "style": "IPY_MODEL_bf81564c9ed14f00ad40011ff9bad495",
      "value": " 1.16k/1.16k [00:00&lt;00:00, 23.8kB/s]"
     }
    },
    "ac4fe3bcc7a84a57965654e6d3280535": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aeaa5cd10af64255808914388b69640a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b1c4e829f4674c33ab5dbf7ffc50051a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25817548b317469a8c9388d862abd7e1",
      "max": 1157,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9a8bfb49004e44e18c7c12b38f9f1f12",
      "value": 1157
     }
    },
    "b2fecd04c3b14b58994632241a53b3a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5e3be4431b844859ea858439f8fa704": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b7f6869b83324fae80ff8ba2cabe44a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bcee8f82d16048baa59d7b276d052839": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf81564c9ed14f00ad40011ff9bad495": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c1ffbc6650ee4903b59cd67c0735ef47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6ec55250004c4c89885b1bbe98009604",
       "IPY_MODEL_b1c4e829f4674c33ab5dbf7ffc50051a",
       "IPY_MODEL_a38dc87bc9424a88b35a1ec195b80087"
      ],
      "layout": "IPY_MODEL_ce018ad4472a48fa92009eaabaafae9e"
     }
    },
    "c330633ebd8f471ba11d447677c30461": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c5bf032c18114d8d979462a014a86904": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73faadabada94be18eb229aeb88a667d",
      "placeholder": "​",
      "style": "IPY_MODEL_90372c83e1554e98ac476e35630bec76",
      "value": " 587k/587k [00:00&lt;00:00, 32.4MB/s]"
     }
    },
    "cb32c31485154fa384a0c9a08b66dce0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cb9bf6910bd2408189c0c99c896e527c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce018ad4472a48fa92009eaabaafae9e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce20f204ab3e4687920157111aee2d1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c9c53c4dbe142ddb89b2975909a1bfa",
      "placeholder": "​",
      "style": "IPY_MODEL_586597214350493686a45b291a8c61ad",
      "value": " 1.96M/1.96M [00:00&lt;00:00, 2.82MB/s]"
     }
    },
    "d08b585b518c45aa9042093516815301": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d62fe3f7610947f782b2c1983cf89bde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dcd3165aa2014012b36c21649a8a7a85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e2b0eed32b584775849bd7df29f9f6ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e94299a2c3eb45e789aa3ad9f2ff3474": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed19617454ad4bc2ac608b25ffbfb729": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "edcbb5a711174fdc93b5347d08ac892c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_17434aad34354304bfebe580666e3462",
      "placeholder": "​",
      "style": "IPY_MODEL_9611e68a858a435aa1eb4a5bb315d788",
      "value": " 560/560 [00:00&lt;00:00, 40.6kB/s]"
     }
    },
    "f006e1a7dcd4457883317dc2a7bae2ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f16f76163d4440bcb6f54986e93851fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5d17e8c65e164d8d9da5f9ebba5f3c36",
       "IPY_MODEL_1846b281f52a4bf5bd9806e489e1c1b8",
       "IPY_MODEL_87e2e864d69f4710b0e625f2c91dc481"
      ],
      "layout": "IPY_MODEL_fa976c16cdef4da3b30e433ff6e51a55"
     }
    },
    "f2dc398344e74ed5b4820e60ff313698": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f60d3f6cc36a4680aefa788d3379d389": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5bb31c10a7a647b089d6e9d66695a0aa",
       "IPY_MODEL_47cda6240eaa4fc19d54ad7edeb99601",
       "IPY_MODEL_3f964a6e5c00411886f8b8c4722616fe"
      ],
      "layout": "IPY_MODEL_3a1a83a2ea6a467db57eacbfb4425b23"
     }
    },
    "f8a17b835d964b6fae28db8c2394b77c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa976c16cdef4da3b30e433ff6e51a55": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "faeeef94d9764ff9ac3d7132ff4ad4cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc1daa9a3449457da954666c442c3bc3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ff0a1ae7050049e9ac2ababd1c8a30a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d62fe3f7610947f782b2c1983cf89bde",
      "placeholder": "​",
      "style": "IPY_MODEL_dcd3165aa2014012b36c21649a8a7a85",
      "value": "special_tokens_map.json: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
